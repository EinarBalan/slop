{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207820f",
   "metadata": {},
   "source": [
    "### Soft prompt tuning on LLaMA 8B Instruct (self-contained)\n",
    "\n",
    "This notebook shows a minimal, end-to-end example of soft prompt tuning (Prompt Tuning) using PEFT on a LLaMA-family instruction-tuned model.\n",
    "\n",
    "It will:\n",
    "- Install dependencies in-notebook\n",
    "- Load an 8B instruct checkpoint via `transformers`\n",
    "- Configure PEFT Prompt Tuning (train only virtual tokens)\n",
    "- Train briefly on a tiny inline dataset\n",
    "- Run inference using the tuned soft prompts\n",
    "\n",
    "Notes:\n",
    "- You may need access approval for gated models. If so, set `MODEL_ID` to a model you can pull.\n",
    "- 8B models require significant RAM/VRAM; adjust to a smaller chat model if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU transformers peft accelerate datasets trl einops sentencepiece bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config and tiny inline dataset\n",
    "from typing import List, Dict\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Change to a model you can access\n",
    "OUTPUT_DIR = \"./softprompt-llama8b\"\n",
    "PROMPT_TOKENS = 32\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "LEARNING_RATE = 5e-3\n",
    "NUM_TRAIN_STEPS = 30  # keep short for demo\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "# Tiny toy dataset of (instruction, output)\n",
    "examples: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"instruction\": \"Write a short, funny haiku about databases.\",\n",
    "        \"output\": \"Tables join in love\\nIndex hearts beat in queries\\nACID dreams commit\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain soft prompt tuning in one sentence.\",\n",
    "        \"output\": \"Soft prompt tuning trains only virtual tokens that steer the frozen model.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"List three benefits of unit tests.\",\n",
    "        \"output\": \"Confidence in changes; documentation of behavior; faster debugging.\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d090ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model (8B instruct)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"Loaded:\", MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PEFT Prompt Tuning\n",
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=PROMPT_TOKENS,\n",
    "    prompt_tuning_init_text=\"You are a helpful, concise assistant.\",\n",
    "    tokenizer_name_or_path=MODEL_ID,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess examples for supervised fine-tuning format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to chat-style prompt => completion text\n",
    "def to_chat(example):\n",
    "    system = \"You are a helpful assistant.\"\n",
    "    user = example[\"instruction\"]\n",
    "    assistant = example[\"output\"]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "        {\"role\": \"assistant\", \"content\": assistant},\n",
    "    ]\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    else:\n",
    "        # Fallback simple format\n",
    "        text = f\"<s>[SYSTEM]\\n{system}\\n[/SYSTEM]\\n[USER]\\n{user}\\n[/USER]\\n[ASSISTANT]\\n{assistant}</s>\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = Dataset.from_list(examples).map(to_chat)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "train_ds = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "train_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8564581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup and brief training\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=MICRO_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: {\n",
    "        k: torch.tensor([b[k] for b in batch]) for k in batch[0].keys()\n",
    "    },\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_update_steps = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=max(1, num_update_steps // 10), num_training_steps=num_update_steps\n",
    ")\n",
    "\n",
    "model.train()\n",
    "model = model.to(next(model.parameters()).device)\n",
    "\n",
    "step = 0\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(1):\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        if step % 10 == 0:\n",
    "            print(f\"step {step} loss {loss.item()*GRAD_ACCUM_STEPS:.4f}\")\n",
    "        step += 1\n",
    "        if step >= NUM_TRAIN_STEPS:\n",
    "            break\n",
    "    if step >= NUM_TRAIN_STEPS:\n",
    "        break\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved prompt adapter to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa429bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with tuned soft prompts\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Reload base + adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "base.eval()\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short, funny haiku about databases.\"},\n",
    "]\n",
    "if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "else:\n",
    "    prompt = \"<s>[SYSTEM]\\nYou are a helpful assistant.\\n[/SYSTEM]\\n[USER]\\nWrite a short, funny haiku about databases.\\n[/USER]\\n[ASSISTANT]\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base.device)\n",
    "with torch.no_grad():\n",
    "    _ = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        streamer=streamer,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e230bce",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
