{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207820f",
   "metadata": {},
   "source": [
    "### Soft prompt tuning on LLaMA 8B Instruct (self-contained)\n",
    "\n",
    "This notebook shows a minimal, end-to-end example of soft prompt tuning (Prompt Tuning) using PEFT on a LLaMA-family instruction-tuned model.\n",
    "\n",
    "It will:\n",
    "- Install dependencies in-notebook\n",
    "- Load an 8B instruct checkpoint via `transformers`\n",
    "- Configure PEFT Prompt Tuning (train only virtual tokens)\n",
    "- Train briefly on a tiny inline dataset\n",
    "- Run inference using the tuned soft prompts\n",
    "\n",
    "Notes:\n",
    "- You may need access approval for gated models. If so, set `MODEL_ID` to a model you can pull.\n",
    "- 8B models require significant RAM/VRAM; adjust to a smaller chat model if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "140c0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU transformers==4.55.2 peft accelerate datasets trl einops sentencepiece bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818467a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac42244e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc349d1898064911a2d9feba10c1b558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a6cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config and tiny inline dataset\n",
    "from typing import List, Dict\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"  # Change to a model you can access\n",
    "OUTPUT_DIR = \"./softprompt-llama8b\"\n",
    "PROMPT_TOKENS = 32\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "LEARNING_RATE = 5e-3\n",
    "NUM_TRAIN_STEPS = 30  # keep short for demo\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "# Tiny toy dataset of (instruction, output)\n",
    "examples: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"instruction\": \"Write a short, funny haiku about databases.\",\n",
    "        \"output\": \"Tables join in love\\nIndex hearts beat in queries\\nACID dreams commit\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain soft prompt tuning in one sentence.\",\n",
    "        \"output\": \"Soft prompt tuning trains only virtual tokens that steer the frozen model.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"List three benefits of unit tests.\",\n",
    "        \"output\": \"Confidence in changes; documentation of behavior; faster debugging.\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d090ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dc5a5a4fba4fafaafec4382d565831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Qwen/Qwen3-4B-Instruct-2507\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model (8B instruct)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"Loaded:\", MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ecd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 81,920 || all params: 4,022,550,016 || trainable%: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Configure PEFT Prompt Tuning\n",
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=PROMPT_TOKENS,\n",
    "    prompt_tuning_init_text=\"You are a helpful, concise assistant.\",\n",
    "    tokenizer_name_or_path=MODEL_ID,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9acb179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8dde29cb7e4cc39b9331c0e401a12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e663d86286420482ca698aed4e8341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess examples for supervised fine-tuning format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to chat-style prompt => completion text\n",
    "def to_chat(example):\n",
    "    system = \"You are a helpful assistant.\"\n",
    "    user = example[\"instruction\"]\n",
    "    assistant = example[\"output\"]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "        {\"role\": \"assistant\", \"content\": assistant},\n",
    "    ]\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    else:\n",
    "        # Fallback simple format\n",
    "        text = f\"<s>[SYSTEM]\\n{system}\\n[/SYSTEM]\\n[USER]\\n{user}\\n[/USER]\\n[ASSISTANT]\\n{assistant}</s>\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = Dataset.from_list(examples).map(to_chat)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "train_ds = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "train_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8564581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 22.9054\n",
      "Saved prompt adapter to: ./softprompt-llama8b\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup and brief training\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=MICRO_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: {\n",
    "        k: torch.tensor([b[k] for b in batch]) for k in batch[0].keys()\n",
    "    },\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_update_steps = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=max(1, num_update_steps // 10), num_training_steps=num_update_steps\n",
    ")\n",
    "\n",
    "model.train()\n",
    "model = model.to(next(model.parameters()).device)\n",
    "\n",
    "step = 0\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(1):\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        if step % 10 == 0:\n",
    "            print(f\"step {step} loss {loss.item()*GRAD_ACCUM_STEPS:.4f}\")\n",
    "        step += 1\n",
    "        if step >= NUM_TRAIN_STEPS:\n",
    "            break\n",
    "    if step >= NUM_TRAIN_STEPS:\n",
    "        break\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved prompt adapter to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aa429bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df31f264d8d64b829579d5a918101562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Write a short, funny haiku about databases.\n",
      "assistant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/slop/lib/python3.12/site-packages/peft/peft_model.py:2066: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables full of data,  \n",
      "Joins cause chaos, indexes sighâ€”  \n",
      "SQL says, \"Just wait!\" ðŸ˜…\n"
     ]
    }
   ],
   "source": [
    "# Inference with tuned soft prompts\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Reload base + adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "base.eval()\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short, funny haiku about databases.\"},\n",
    "]\n",
    "if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "else:\n",
    "    prompt = \"<s>[SYSTEM]\\nYou are a helpful assistant.\\n[/SYSTEM]\\n[USER]\\nWrite a short, funny haiku about databases.\\n[/USER]\\n[ASSISTANT]\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base.device)\n",
    "with torch.no_grad():\n",
    "    _ = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        streamer=streamer,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e230bce",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
