{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207820f",
   "metadata": {},
   "source": [
    "### Output-only style tuning with soft prompts (self-contained)\n",
    "\n",
    "This notebook fine-tunes style by training ONLY on assistant outputs (no instructions), using PEFT Prompt Tuning on an instruct model.\n",
    "\n",
    "It will:\n",
    "- Install dependencies in-notebook\n",
    "- Load a chat checkpoint via `transformers`\n",
    "- Configure Prompt Tuning (learn virtual tokens only)\n",
    "- Train on an outputs-only dataset to steer style\n",
    "- Run inference on a normal user prompt\n",
    "\n",
    "Notes:\n",
    "- Adjust `MODEL_ID` to a model you can pull.\n",
    "- Large models need significant VRAM; pick a smaller one if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be2b63",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140c0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers==4.55.2 peft accelerate datasets trl einops sentencepiece bitsandbytes jinja2>=3.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff47167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting triton==3.4\n",
      "  Downloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kernels\n",
      "  Downloading kernels-0.9.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4) (59.6.0)\n",
      "Collecting pyyaml>=6\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub<1.0,>=0.26.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from kernels) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from kernels) (21.3)\n",
      "Collecting tomli>=2.0\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub<1.0,>=0.26.0->kernels) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub<1.0,>=0.26.0->kernels) (4.10.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.26.0->kernels) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.26.0->kernels) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.26.0->kernels) (1.1.9)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface_hub<1.0,>=0.26.0->kernels) (2024.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface_hub<1.0,>=0.26.0->kernels) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub<1.0,>=0.26.0->kernels) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub<1.0,>=0.26.0->kernels) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.26.0->kernels) (3.4.3)\n",
      "Installing collected packages: triton, tomli, pyyaml, kernels\n",
      "Successfully installed kernels-0.9.0 pyyaml-6.0.2 tomli-2.2.1 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# for oss \n",
    "%pip install triton==3.4 kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baf2cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\", # 0 # very slow, pretty much same quality as 8b on a100\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",  # 1 # good quality, pretty fast\n",
    "    \"openai/gpt-oss-20b\",                   # 2 # good quality, decently quick, but have to deal with thinking \n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",          # 3 # tends to generate the same post over and over (without tuning)\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\",     # 4 # pretty slow > 1 min per post on a100, good quality\n",
    "    \"google/gemma-3-4b-it\",                 # 5 # multimodal, can't get working\n",
    "    \"google/gemma-3-27b-it\",                # 6 # multimodal, can't get working\n",
    "    \"mistralai/Mistral-7B-v0.1\",            # 7 WAITING FOR ACCESS\n",
    "    \"microsoft/phi-4\",                      # 8 14B, pretty slow, low-decent quality, generates same post over and over\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" #9 very fast, have to deal with thinking, decent quality\n",
    "    ]\n",
    "\n",
    "MODEL_ID = models[1] \n",
    "OUTPUT_DIR = \"./softprompt-style-outputs\"\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 0.2\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "PROMPT = \"Please generate one reddit post. Don't include any extraneous characters like asterisks or other symbols. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5fa89",
   "metadata": {},
   "source": [
    "## Load data \n",
    "(make sure to run sampleposts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5a6cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config and instruction/output dataset\n",
    "from typing import List\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load sampled Reddit posts from JSON created by sample-posts.py\n",
    "# Each item is a dict with keys: title, subreddit, self_text\n",
    "with open(\"./data/humorposts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reddit_posts: List[dict] = json.load(f)\n",
    "\n",
    "# Build dataset in the format: {\"instruction\": PROMPT, \"output\": post}\n",
    "examples: List[dict] = []\n",
    "for p in reddit_posts:\n",
    "    title = p.get(\"title\", \"\")\n",
    "    self_text = p.get(\"self_text\", \"\")\n",
    "    image_url = p.get(\"image_url\", \"\")\n",
    "    \n",
    "    if not self_text or image_url: continue\n",
    "    \n",
    "    subreddit = p.get(\"subreddit\", \"\")\n",
    "    subreddit = re.sub(r\"\\s*(/)?r/\", \"r/\", subreddit)\n",
    "    post = f\"title: {title}\\nself_text: {self_text}\\nsubreddit: {subreddit}\"\n",
    "    examples.append({\"instruction\": PROMPT, \"output\": post})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6ac99",
   "metadata": {},
   "source": [
    "# Load model\n",
    "\n",
    "Make sure to set HUGGING_FACE_HUB_TOKEN environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19dc1945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import dotenv, os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "login(token=os.getenv(\"HUGGING_FACE_HUB_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d090ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model (8B instruct)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"Loaded:\", MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d41bc2",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f27a82",
   "metadata": {},
   "source": [
    "### Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f8444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a generated Reddit post:\n",
      "\n",
      "title: I just had the weirdest dream\n",
      "self_text: I was walking through a familiar neighborhood, but everything was slightly off. The houses were the same, but the colors were all wrong. I saw my childhood best friend standing in front of one of the houses, but she was wearing a bright pink wig and a superhero cape. I tried to talk to her, but she just ignored me and walked away. Then I woke up. Has anyone else ever had a dream that was so vivid and strange?\n",
      "subreddit: r/dreams<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# prompt = \"Please generate one reddit post (and nothing else). Make sure to stick to the format below exactly. Don't include any extraneous characters like asterisks or other symbols. \\n\\n title: {title} \\n self_text: {self_text} \\n subreddit: {subreddit} \\n Here's an example of the format: \\n\\ntitle: This is the title of the post! \\nself_text: Here's where the content of the post goes. \\nsubreddit: This is the subreddit, or the name of the community the post belongs to.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": PROMPT},\n",
    "]\n",
    "\n",
    "streamer = TextStreamer(tokenizer, \n",
    "                        skip_special_tokens=False,\n",
    "                        skip_prompt=True)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "\tmax_new_tokens=MAX_SEQ_LEN,\n",
    "\ttemperature=0.9,\n",
    "\ttop_p=0.8,\n",
    "\tstreamer=streamer,\n",
    ")\n",
    "# print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e0408",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1ecd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262,144 || all params: 8,030,523,392 || trainable%: 0.0033\n"
     ]
    }
   ],
   "source": [
    "# Configure PEFT Prompt Tuning\n",
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=PROMPT_TOKENS,\n",
    "    prompt_tuning_init_text=\"Generate a reddit post.\",\n",
    "    tokenizer_name_or_path=MODEL_ID,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9acb179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 967.83 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess instruction/output dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Build HF dataset from examples [{\"instruction\", \"output\"}]\n",
    "dataset = Dataset.from_list(examples)\n",
    "\n",
    "# Tokenize instruction with chat template, and supervise only the output tokens\n",
    "def tokenize_io(sample):\n",
    "    # Build chat prompt prefix for the user instruction\n",
    "    messages = [{\"role\": \"user\", \"content\": sample[\"instruction\"]}]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"]\n",
    "    output_ids = tokenizer(sample[\"output\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = prompt_ids + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "    labels = ([-100] * len(prompt_ids)) + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Truncate from the left if too long, keeping alignment between inputs and labels\n",
    "    if len(input_ids) > MAX_SEQ_LEN:\n",
    "        input_ids = input_ids[-MAX_SEQ_LEN:]\n",
    "        labels = labels[-MAX_SEQ_LEN:]\n",
    "        attention_mask = attention_mask[-MAX_SEQ_LEN:]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "train_ds = dataset.map(tokenize_io, remove_columns=dataset.column_names)\n",
    "train_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8564581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 2.7644\n",
      "step 10 loss 2.9495\n",
      "step 20 loss 2.9693\n",
      "step 30 loss 3.2349\n",
      "step 40 loss 4.3521\n",
      "step 50 loss 2.4671\n",
      "step 60 loss 2.9861\n",
      "step 70 loss 2.6146\n",
      "step 80 loss 2.4659\n",
      "step 90 loss 2.8505\n",
      "step 100 loss 3.2453\n",
      "step 110 loss 3.5126\n",
      "step 120 loss 2.6644\n",
      "step 130 loss 2.4040\n",
      "step 140 loss 3.2621\n",
      "step 150 loss 2.5959\n",
      "step 160 loss 2.8041\n",
      "step 170 loss 2.6731\n",
      "step 180 loss 2.0506\n",
      "step 190 loss 2.7093\n",
      "step 200 loss 1.0189\n",
      "step 210 loss 2.9810\n",
      "step 220 loss 2.2182\n",
      "step 230 loss 2.4045\n",
      "step 240 loss 2.8120\n",
      "step 250 loss 2.6533\n",
      "step 260 loss 2.7973\n",
      "step 270 loss 2.4042\n",
      "step 280 loss 3.0514\n",
      "step 290 loss 2.6159\n",
      "step 300 loss 2.6006\n",
      "step 310 loss 1.8526\n",
      "step 320 loss 1.9546\n",
      "step 330 loss 2.5402\n",
      "step 340 loss 3.4069\n",
      "step 350 loss 2.8354\n",
      "step 360 loss 3.9945\n",
      "step 370 loss 2.3689\n",
      "step 380 loss 2.1852\n",
      "step 390 loss 1.9901\n",
      "step 400 loss 2.8739\n",
      "step 410 loss 2.3417\n",
      "step 420 loss 2.7267\n",
      "step 430 loss 2.9212\n",
      "step 440 loss 3.2932\n",
      "step 450 loss 1.9399\n",
      "step 460 loss 3.4223\n",
      "step 470 loss 3.7334\n",
      "step 480 loss 2.4692\n",
      "step 490 loss 2.3530\n",
      "step 500 loss 2.8044\n",
      "step 510 loss 2.6627\n",
      "step 520 loss 2.0503\n",
      "step 530 loss 2.7890\n",
      "step 540 loss 2.9961\n",
      "step 550 loss 2.4632\n",
      "step 560 loss 2.8756\n",
      "step 570 loss 2.3584\n",
      "step 580 loss 3.1670\n",
      "step 590 loss 2.1629\n",
      "step 600 loss 2.6197\n",
      "step 610 loss 2.3123\n",
      "step 620 loss 1.5171\n",
      "step 630 loss 2.3559\n",
      "step 640 loss 2.6944\n",
      "step 650 loss 2.5341\n",
      "step 660 loss 2.4219\n",
      "step 670 loss 2.0366\n",
      "step 680 loss 2.1144\n",
      "step 690 loss 3.1136\n",
      "step 700 loss 2.8318\n",
      "step 710 loss 3.0001\n",
      "step 720 loss 3.7590\n",
      "step 730 loss 2.3191\n",
      "step 740 loss 2.2816\n",
      "step 750 loss 2.6354\n",
      "step 760 loss 2.5833\n",
      "step 770 loss 3.1004\n",
      "step 780 loss 2.2890\n",
      "step 790 loss 1.5857\n",
      "step 800 loss 3.2088\n",
      "step 810 loss 3.0203\n",
      "step 820 loss 2.6147\n",
      "step 830 loss 1.4766\n",
      "step 840 loss 2.3630\n",
      "step 850 loss 3.4562\n",
      "step 860 loss 1.7707\n",
      "step 870 loss 3.2575\n",
      "step 880 loss 2.6553\n",
      "step 890 loss 3.2525\n",
      "step 900 loss 2.8678\n",
      "step 910 loss 2.7421\n",
      "step 920 loss 1.8654\n",
      "step 930 loss 2.3282\n",
      "step 940 loss 2.6520\n",
      "step 950 loss 1.9021\n",
      "step 960 loss 3.0231\n",
      "step 970 loss 1.9622\n",
      "step 980 loss 2.0182\n",
      "step 990 loss 1.8797\n",
      "Saved prompt adapter to: ./softprompt-style-outputs\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup and brief training\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "def collate_fn(features):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    batch_size = len(features)\n",
    "    seq_lens = [len(f[\"input_ids\"]) for f in features]\n",
    "    max_len = max(seq_lens)\n",
    "\n",
    "    input_ids = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "    labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "\n",
    "    for i, f in enumerate(features):\n",
    "        ids = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
    "        attn = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
    "        labs = torch.tensor(f[\"labels\"], dtype=torch.long)\n",
    "        L = ids.size(0)\n",
    "        input_ids[i, :L] = ids\n",
    "        attention_mask[i, :L] = attn\n",
    "        labels[i, :L] = labs\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=MICRO_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# Total optimizer steps we intend to take\n",
    "total_optim_steps = NUM_TRAIN_STEPS\n",
    "num_warmup_steps = max(1, int(0.1 * total_optim_steps))\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_optim_steps,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "model = model.to(next(model.parameters()).device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "optim_step = 0\n",
    "accumulated = 0\n",
    "running_loss = 0.0\n",
    "for epoch in range(10):  # repeat over dataset until reaching desired steps\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        (loss / GRAD_ACCUM_STEPS).backward()\n",
    "        running_loss += loss.item()\n",
    "        accumulated += 1\n",
    "        if accumulated % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if optim_step % 10 == 0:\n",
    "                print(f\"step {optim_step} loss {running_loss / GRAD_ACCUM_STEPS:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            optim_step += 1\n",
    "            if optim_step >= total_optim_steps:\n",
    "                break\n",
    "    if optim_step >= total_optim_steps:\n",
    "        break\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved prompt adapter to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5aa429bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(64, 4096)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(128256, 4096)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import TextStreamer, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Reload base + adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "base.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7dd30cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: I'm not sure if I'm a good or bad person\n",
      "self_text: I'm a 23 year old man who has never been in a relationship, I have a decent job but I'm not very ambitious, I'm not very outgoing and I don't really like parties or big social events. I'm not really interested in politics or current events. I like to play video games and watch TV. I'm a bit of a loner but I don't really feel lonely. I'm pretty content with my life. \n",
      "\n",
      "Is that good or bad? \n",
      "subreddit: copypasta\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, \n",
    "                        skip_special_tokens=True,\n",
    "                        skip_prompt=True\n",
    "                        )\n",
    "\n",
    "# Build chat-formatted inputs via the model's chat template\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": PROMPT},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(base.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_SEQ_LEN,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        streamer=streamer,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e230bce",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
