{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207820f",
   "metadata": {},
   "source": [
    "### Output-only style tuning with soft prompts (self-contained)\n",
    "\n",
    "This notebook fine-tunes style by training ONLY on assistant outputs (no instructions), using PEFT Prompt Tuning on an instruct model.\n",
    "\n",
    "It will:\n",
    "- Install dependencies in-notebook\n",
    "- Load a chat checkpoint via `transformers`\n",
    "- Configure Prompt Tuning (learn virtual tokens only)\n",
    "- Train on an outputs-only dataset to steer style\n",
    "- Run inference on a normal user prompt\n",
    "\n",
    "Notes:\n",
    "- Adjust `MODEL_ID` to a model you can pull.\n",
    "- Large models need significant VRAM; pick a smaller one if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "140c0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers==4.55.2 peft accelerate datasets trl einops sentencepiece bitsandbytes jinja2>=3.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5a6cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config and outputs-only dataset\n",
    "from typing import List\n",
    "import json\n",
    "import re\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Change to a model you can access\n",
    "OUTPUT_DIR = \"./softprompt-style-outputs\"\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 5e-3\n",
    "NUM_TRAIN_STEPS = 800  # longer for style steering\n",
    "MAX_SEQ_LEN = 256\n",
    "\n",
    "# Load sampled Reddit posts from JSON created by sample-posts.py\n",
    "# Each item is a dict with keys: title, subreddit, self_text\n",
    "with open(\"./data/post-sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reddit_posts: List[dict] = json.load(f)\n",
    "\n",
    "# Basic cleaners to remove UI boilerplate and artifacts that leak into generations\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"^\\s*View\\s+More\\s+Posts\\s*$\",\n",
    "    r\"^\\s*View\\s+Post\\s*$\",\n",
    "    r\"^\\s*Help\\??\\s*$\",\n",
    "    r\"^\\s*Edit:\\s*.*$\",\n",
    "]\n",
    "boilerplate_regexes = [re.compile(p, flags=re.IGNORECASE) for p in BOILERPLATE_PATTERNS]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\r\\n?\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Remove boilerplate lines\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\")]\n",
    "    kept = []\n",
    "    for ln in lines:\n",
    "        if any(rx.match(ln) for rx in boilerplate_regexes):\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "    return \"\\n\".join(kept).strip()\n",
    "\n",
    "# Convert to outputs-only strings in a strict template order\n",
    "style_outputs: List[str] = []\n",
    "for p in reddit_posts:\n",
    "    title = clean_text(p.get(\"title\", \"\"))\n",
    "    self_text = clean_text(p.get(\"self_text\", \"\"))\n",
    "    subreddit = clean_text(p.get(\"subreddit\", \"\"))\n",
    "    # Remove repeated subreddit tokens like 'r/AskReddit' lines duplicated\n",
    "    subreddit = re.sub(r\"\\s*(/)?r/\", \"r/\", subreddit)\n",
    "    style_outputs.append(\n",
    "        f\"title: {title}\\nself_text: {self_text}\\nsubreddit: {subreddit}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d090ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 59.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer and model (8B instruct)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"Loaded:\", MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1ecd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262,144 || all params: 8,030,523,392 || trainable%: 0.0033\n"
     ]
    }
   ],
   "source": [
    "# Configure PEFT Prompt Tuning\n",
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=PROMPT_TOKENS,\n",
    "    prompt_tuning_init_text=\"You are a helpful, concise assistant.\",\n",
    "    tokenizer_name_or_path=MODEL_ID,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9acb179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 4067.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess outputs-only dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create text stream directly from outputs\n",
    "outputs_ds = Dataset.from_dict({\"text\": [o.strip() for o in style_outputs]})\n",
    "\n",
    "# Tokenize without padding; we'll pad dynamically in the collate_fn\n",
    "def tokenize(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    out[\"labels\"] = [ids[:] for ids in out[\"input_ids\"]]\n",
    "    return out\n",
    "\n",
    "train_ds = outputs_ds.map(tokenize, batched=True, remove_columns=outputs_ds.column_names)\n",
    "train_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8564581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 4.3706\n",
      "step 10 loss 3.5588\n",
      "step 20 loss 3.2212\n",
      "step 30 loss 3.1885\n",
      "step 40 loss 3.1434\n",
      "step 50 loss 2.8586\n",
      "step 60 loss 3.0076\n",
      "step 70 loss 2.5332\n",
      "step 80 loss 2.3638\n",
      "step 90 loss 2.3937\n",
      "step 100 loss 2.7967\n",
      "step 110 loss 2.3713\n",
      "step 120 loss 2.5810\n",
      "step 130 loss 3.4381\n",
      "step 140 loss 2.9435\n",
      "step 150 loss 2.2477\n",
      "step 160 loss 2.1865\n",
      "step 170 loss 2.3452\n",
      "step 180 loss 2.5205\n",
      "step 190 loss 4.3069\n",
      "step 200 loss 2.8646\n",
      "step 210 loss 2.1046\n",
      "step 220 loss 3.4014\n",
      "step 230 loss 1.8779\n",
      "step 240 loss 2.5006\n",
      "step 250 loss 3.3016\n",
      "step 260 loss 2.3269\n",
      "step 270 loss 1.5209\n",
      "step 280 loss 2.0935\n",
      "step 290 loss 2.4100\n",
      "step 300 loss 1.4401\n",
      "step 310 loss 2.2326\n",
      "step 320 loss 2.3852\n",
      "step 330 loss 2.7482\n",
      "step 340 loss 2.7135\n",
      "step 350 loss 2.6208\n",
      "step 360 loss 2.4218\n",
      "step 370 loss 2.8657\n",
      "step 380 loss 2.8885\n",
      "step 390 loss 3.1669\n",
      "step 400 loss 2.2478\n",
      "step 410 loss 2.7647\n",
      "step 420 loss 2.6662\n",
      "step 430 loss 2.4746\n",
      "step 440 loss 2.3423\n",
      "step 450 loss 3.3020\n",
      "step 460 loss 2.7758\n",
      "step 470 loss 2.4833\n",
      "step 480 loss 2.2442\n",
      "step 490 loss 3.4733\n",
      "step 500 loss 2.3305\n",
      "step 510 loss 2.4044\n",
      "step 520 loss 1.3975\n",
      "step 530 loss 2.0879\n",
      "step 540 loss 2.4465\n",
      "step 550 loss 2.6720\n",
      "step 560 loss 2.3831\n",
      "step 570 loss 2.6611\n",
      "step 580 loss 2.3725\n",
      "step 590 loss 2.5005\n",
      "step 600 loss 1.7478\n",
      "step 610 loss 2.9659\n",
      "step 620 loss 3.3401\n",
      "step 630 loss 2.5142\n",
      "step 640 loss 2.2200\n",
      "step 650 loss 3.7026\n",
      "step 660 loss 2.7360\n",
      "step 670 loss 2.7367\n",
      "step 680 loss 2.4809\n",
      "step 690 loss 2.7882\n",
      "step 700 loss 3.6500\n",
      "step 710 loss 2.9239\n",
      "step 720 loss 2.4334\n",
      "step 730 loss 2.6531\n",
      "step 740 loss 2.6200\n",
      "step 750 loss 2.2482\n",
      "step 760 loss 2.7380\n",
      "step 770 loss 2.2421\n",
      "step 780 loss 2.7314\n",
      "step 790 loss 2.7594\n",
      "Saved prompt adapter to: ./softprompt-style-outputs\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup and brief training\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "def collate_fn(features):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    batch_size = len(features)\n",
    "    seq_lens = [len(f[\"input_ids\"]) for f in features]\n",
    "    max_len = max(seq_lens)\n",
    "\n",
    "    input_ids = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "    labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "\n",
    "    for i, f in enumerate(features):\n",
    "        ids = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
    "        attn = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
    "        labs = torch.tensor(f[\"labels\"], dtype=torch.long)\n",
    "        L = ids.size(0)\n",
    "        input_ids[i, :L] = ids\n",
    "        attention_mask[i, :L] = attn\n",
    "        labels[i, :L] = labs\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=MICRO_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# Total optimizer steps we intend to take\n",
    "total_optim_steps = NUM_TRAIN_STEPS\n",
    "num_warmup_steps = max(1, int(0.1 * total_optim_steps))\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_optim_steps,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "model = model.to(next(model.parameters()).device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "optim_step = 0\n",
    "accumulated = 0\n",
    "running_loss = 0.0\n",
    "for epoch in range(10):  # repeat over dataset until reaching desired steps\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        (loss / GRAD_ACCUM_STEPS).backward()\n",
    "        running_loss += loss.item()\n",
    "        accumulated += 1\n",
    "        if accumulated % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if optim_step % 10 == 0:\n",
    "                print(f\"step {optim_step} loss {running_loss / GRAD_ACCUM_STEPS:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            optim_step += 1\n",
    "            if optim_step >= total_optim_steps:\n",
    "                break\n",
    "    if optim_step >= total_optim_steps:\n",
    "        break\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved prompt adapter to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5aa429bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please generate one SINGLE reddo enjoy. It's imperative that you only generate one and that you stick to the format below exactly. Don't include any extraneous characters like asterisks or other symbols. \n",
      "\n",
      " title: {title} \n",
      " self_text: {self_text} \n",
      " subreddit: {subreddit} \n",
      " Here's an example of the format: \n",
      "\n",
      "title: This is the title of the post! \n",
      "self_text: Here's where the content of the post goes. \n",
      "subreddit: This is the subreddit, or the name of the community the post belongs to. \n",
      "}\n",
      "submitted to r/RandomJokes. \n",
      "View Reddit\n",
      "\\end{self-post}\n",
      "view more View less more bath & body works 3 wick candles 10.00 bath & body works 3 wick candles 10.00 View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self-post}\n",
      "view more View less more View Reddit\n",
      "Pinned post\n",
      "\\end{self\n"
     ]
    }
   ],
   "source": [
    "# Inference with tuned soft prompts (no chat template)\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Reload base + adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "base.eval()\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "# Empty prompt so the soft prompt dominates formatting\n",
    "prompt = \"Please generate one reddit post. Make sure to stick to the format below exactly. Don't include any extraneous characters like asterisks or other symbols. \\n\\n title: {title} \\n self_text: {self_text} \\n subreddit: {subreddit} \\n Here's an example of the format: \\n\\ntitle: This is the title of the post! \\nself_text: Here's where the content of the post goes. \\nsubreddit: This is the subreddit, or the name of the community the post belongs to.\"\n",
    "\n",
    "# Ensure at least one token as input to avoid empty input_ids\n",
    "if prompt.strip() == \"\":\n",
    "    token_id = (\n",
    "        tokenizer.bos_token_id\n",
    "        if tokenizer.bos_token_id is not None\n",
    "        else (tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id)\n",
    "    )\n",
    "    if token_id is not None:\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor([[token_id]], dtype=torch.long, device=base.device),\n",
    "            \"attention_mask\": torch.ones((1, 1), dtype=torch.long, device=base.device),\n",
    "        }\n",
    "    else:\n",
    "        inputs = tokenizer(\" \", return_tensors=\"pt\").to(base.device)\n",
    "else:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        streamer=streamer,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e230bce",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfda43fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: This is the title of the post! self_text: Here's where the content of the post goes. subreddit: This is the subreddit, or the name of the community the post belongs to. \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div> \n",
      "</div> \n",
      "</div> \n",
      "</body> \n",
      "</html> \n",
      "</p> \n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# Inference with stop on second title to ensure exactly one post\n",
    "from peft import PeftModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "# Reload base + adapter (safe to re-run)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "base.eval()\n",
    "\n",
    "class StopOnSecondTitle(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, prompt_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_len = prompt_len\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        gen_ids = input_ids[0, self.prompt_len:]\n",
    "        if gen_ids.numel() == 0:\n",
    "            return False\n",
    "        text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        return text.count(\"title:\") >= 2\n",
    "\n",
    "prompt = (\n",
    "    \"Please generate a reddit post that the user is likely to enjoy. \"\n",
    "    \"It's imperative that you only generate one and that you stick to the format below exactly. \"\n",
    "    \" title: {title} \\n self_text: {self_text} \\n subreddit: {subreddit} \\n Here's an example of the format: \\n\\n\"\n",
    "    \"title: This is the title of the post! \\n\"\n",
    "    \"self_text: Here's where the content of the post goes. \\n\"\n",
    "    \"subreddit: This is the subreddit, or the name of the community the post belongs to.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base.device)\n",
    "stopper = StoppingCriteriaList([StopOnSecondTitle(tokenizer, inputs[\"input_ids\"].shape[1])])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        stopping_criteria=stopper,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "# Decode only generated tokens (exclude the prompt)\n",
    "gen_tokens = out.sequences[0, inputs[\"input_ids\"].shape[1]:]\n",
    "text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Trim to the first post only\n",
    "first_idx = text.find(\"title:\")\n",
    "if first_idx != -1:\n",
    "    text = text[first_idx:]\n",
    "    second_idx = text.find(\"\\ntitle:\", 1)\n",
    "    if second_idx != -1:\n",
    "        text = text[:second_idx]\n",
    "\n",
    "print(text.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "178a3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: generate a single reddit post (no prompt echo)\n",
    "from typing import Optional\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnSecondTitle(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, prompt_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_len = prompt_len\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        gen_ids = input_ids[0, self.prompt_len:]\n",
    "        if gen_ids.numel() == 0:\n",
    "            return False\n",
    "        text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        return text.count(\"title:\") >= 2\n",
    "\n",
    "GEN_PROMPT = (\n",
    "    \"Please generate a reddit post that the user is likely to enjoy. \"\n",
    "    \"It's imperative that you only generate one and that you stick to the format below exactly. \"\n",
    "    \" title: {title} \\n self_text: {self_text} \\n subreddit: {subreddit} \\n Here's an example of the format: \\n\\n\"\n",
    "    \"title: This is the title of the post! \\n\"\n",
    "    \"self_text: Here's where the content of the post goes. \\n\"\n",
    "    \"subreddit: This is the subreddit, or the name of the community the post belongs to.\"\n",
    ")\n",
    "\n",
    "def generate_reddit_post(temperature: float = 0.7, top_p: float = 0.9, max_new_tokens: int = 256) -> str:\n",
    "    inputs = tokenizer(GEN_PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "    stopper = StoppingCriteriaList([StopOnSecondTitle(tokenizer, inputs[\"input_ids\"].shape[1])])\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            stopping_criteria=stopper,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    gen_tokens = out.sequences[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "    first_idx = text.find(\"title:\")\n",
    "    if first_idx != -1:\n",
    "        text = text[first_idx:]\n",
    "        second_idx = text.find(\"\\ntitle:\", 1)\n",
    "        if second_idx != -1:\n",
    "            text = text[:second_idx - len(\"title:\")]\n",
    "    return text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae31a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm asking for this because I'm a bot, and I have no idea what you want to see. \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div> \\n</div>\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reddit_post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5edd0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51993/1816834068.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate 10 posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_reddit_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n---\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_51993/1816834068.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate 10 posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_reddit_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n---\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_51993/51516370.py\u001b[0m in \u001b[0;36mgenerate_reddit_post\u001b[0;34m(temperature, top_p, max_new_tokens)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mstopper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStoppingCriteriaList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStopOnSecondTitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         out = model.generate(\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1973\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2630\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3628\u001b[0m             \u001b[0;31m# pre-process distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3629\u001b[0;31m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3631\u001b[0m             \u001b[0;31m# Store scores, attentions and hidden_states when required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;31m# Remove all tokens with a probability less than the last token of the top-k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mindices_to_remove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mscores_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_to_remove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores_processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate 10 posts\n",
    "posts = [generate_reddit_post() for _ in range(10)]\n",
    "print(\"\\n\\n---\\n\\n\".join(posts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# If you set the env var already:\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b411b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 57.00 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 77.75 GiB is allocated by PyTorch, and 739.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51993/2060557560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bfloat16\"\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# H100-friendly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    601\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5067\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5068\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5069\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5070\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5071\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5531\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5532\u001b[0;31m                 \u001b[0m_error_msgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shard_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5533\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error_msgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m    976\u001b[0m             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 )\n\u001b[1;32m    842\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 57.00 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 77.75 GiB is allocated by PyTorch, and 739.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"bfloat16\",          # H100-friendly\n",
    "    device_map={\"\": 0},              # force GPU 0\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please generate a reddit post that the user is likely to enjoy. Make sure to stick to the format below exactly. Don't include any extraneous characters like asterisks or other symbols. \n",
      "\n",
      "title: {title} \n",
      "self_text: {self_text} \n",
      "subreddit: {subreddit} \n",
      "Here's an example of the format: \n",
      "\n",
      "title: This is the title of the post! \n",
      "self_text: Here's where the content of the post goes. \n",
      "subreddit: This is the subreddit, or the name of the community the post belongs to. \n",
      "\n",
      "Here's a post that I think the user would enjoy: \n",
      "\n",
      "title: I just spent 10 hours building a treehouse and I'm not sure if it's worth it \n",
      "self_text: I've been working on this treehouse for weeks, and I finally finished it today. It's a lot bigger than I expected it to be, but I'm not sure if it's worth all the effort. The ladder is a little rickety, and the roof is a little leaky, but it's still really cool. I was thinking about adding some stairs instead of a ladder, but I'm not sure if that would make it too big. Has anyone else ever built a treehouse? Do you have any advice for me? \n",
      "subreddit: r/DIYprojects \n",
      "\n",
      "Let me know if you'd like me to generate a different post! \n",
      "\n",
      "Edit: I can generate a post that is more tailored to the user's interests. If you could provide me with some information about the user's interests, that would be helpful. For example, are they interested in a particular hobby or activity? Do they have any pets? Do they have a favorite TV show or movie? Any information you can provide will help me generate a post that the user is likely\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Please generate a reddit post that the user is likely to enjoy. \"\n",
    "    \"Make sure to stick to the format below exactly. Don't include any extraneous characters like asterisks or other symbols. \\n\\n\"\n",
    "    \"title: {title} \\n\"\n",
    "    \"self_text: {self_text} \\n\"\n",
    "    \"subreddit: {subreddit} \\n\"\n",
    "    \"Here's an example of the format: \\n\\n\"\n",
    "    \"title: This is the title of the post! \\n\"\n",
    "    \"self_text: Here's where the content of the post goes. \\n\"\n",
    "    \"subreddit: This is the subreddit, or the name of the community the post belongs to.\"\n",
    ")\n",
    "\n",
    "out = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,              # smaller = faster\n",
    ")\n",
    "print(out[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
