{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8a0981",
   "metadata": {},
   "source": [
    "# Ablation Study Results\n",
    "\n",
    "Axes\n",
    "- model (gemma-3-4b-it, gemma-3-27b-it, gpt-5)\n",
    "- manual vs judge\n",
    "- experiment (base, subreddit, summary, liked posts list, fine tuned, soft prompt (100), soft prompt(500))\n",
    "- dataset (circlejerk, jokes + puns, gaming, animals, sports, etc.)\n",
    "\n",
    "Dataset selection:\n",
    "- small\n",
    "    - okbuddy\n",
    "    - boomerhumor\n",
    "    - animals\n",
    "    - creative\n",
    "    - food\n",
    "    - religion\n",
    "- medium\n",
    "    - finance\n",
    "    - school\n",
    "    - pop\n",
    "- varied but focused: \n",
    "    - nerdy\n",
    "    - personal\n",
    "    - ucla\n",
    "    - tech\n",
    "    - school\n",
    "    - ucla\n",
    "- ultra specific: \n",
    "    - minecraft\n",
    "    - nba\n",
    "- format specific:\n",
    "    - copypasta,\n",
    "    - no stupid questions\n",
    "    - am i the asshole\n",
    "- 3 test (unalike)\n",
    "    - pop\n",
    "    - religion\n",
    "    - tech\n",
    "- 3 test (alike)\n",
    "    - tech\n",
    "    - nerdy\n",
    "    - finance\n",
    "- college student\n",
    "    - ucla\n",
    "    - nerdy\n",
    "    - okbuddy\n",
    "    - copypasta\n",
    "    - pop\n",
    "    - food\n",
    "    - animals\n",
    "- new mother\n",
    "    - pregnancy\n",
    "    - parenting\n",
    "    - baby\n",
    "    - food\n",
    "    - am i the asshole\n",
    "    - pop\n",
    "    - boomerhumor\n",
    "- creative gen alpha\n",
    "    - minecraft\n",
    "    - creative\n",
    "    - food\n",
    "    - school\n",
    "    - nba\n",
    "\n",
    "### gemma-3-4b-it\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n",
    "\n",
    "### gemma-3-27b-it\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n",
    "\n",
    "### gpt-5\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2e1e5",
   "metadata": {},
   "source": [
    "## Notes during testing\n",
    "- very good alignment on tech (92)\n",
    "- very good alignment on personal (180)\n",
    "- very good alignment on nerdy (204)\n",
    "- very good alignmenton school (93)\n",
    "- bad alignment on interesting (35)\n",
    "- bad alignment on finance (57)\n",
    "- decent alignment on interesting + finance (on the finance side) (92)\n",
    "- for hyper targeted like minecraft, even 10 examples is good enough for alignment, but post topic variety goes down. ~100 is best to balance quality and training time\n",
    "- 50/50 minecraft + ucla works REALLY well\n",
    "\n",
    "it seems like ~100 samples is good enough for gemma 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae67d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers peft accelerate datasets trl einops sentencepiece bitsandbytes jinja2>=3.1.0 dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf9bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"gpt-5\"\n",
    "]\n",
    "\n",
    "# Load dataset categories from JSON file\n",
    "DATASETS1 = [\n",
    "    {\n",
    "    \"minecraft\": 1,  \n",
    "    },\n",
    "    {\n",
    "    \"ucla\": 1,  \n",
    "    },\n",
    "    {\n",
    "    \"nostupidquestions\": 1,  \n",
    "    },\n",
    "    {\n",
    "    \"copypasta\": 1,  \n",
    "    },\n",
    "]\n",
    "\n",
    "DATASETS2 = [\n",
    "    {\n",
    "    \"nerdy\": 1,  \n",
    "    },\n",
    "    {\n",
    "    \"personal\": 1,  \n",
    "    },\n",
    "    { # unalike\n",
    "    \"pop\": 1,  \n",
    "    \"religion\": 1,\n",
    "    \"tech\": 1\n",
    "    },\n",
    "    { # alike\n",
    "        \"tech\": 1,\n",
    "        \"nerdy\": 1,\n",
    "        \"finance\": 1,\n",
    "    },\n",
    "    { # format specific\n",
    "        \"copypasta\": 1,\n",
    "        \"nostupidquestions\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "    },\n",
    "    { # college student\n",
    "        \"ucla\": 1,\n",
    "        \"nerdy\": 1,\n",
    "        \"okbuddy\": 1,\n",
    "        \"copypasta\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"food\": 1,\n",
    "        \"animals\": 1,\n",
    "    },\n",
    "    { # new mother\n",
    "        \"pregnancy\": 1,\n",
    "        \"parenting\": 1,\n",
    "        \"baby\": 1,\n",
    "        \"food\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"boomerhumor\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "TRAIN_SIZES = [\n",
    "    10, 20, 50, 100, 250, 500, 1000, 2000\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    \"self defined\",\n",
    "    \"summary\",\n",
    "    \"like history\",\n",
    "    \"fine tune\",\n",
    "    \"soft prompt\",\n",
    "]\n",
    "\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 0.2\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"models\"\n",
    "GENERATED_OUTPUT_DIR = \"generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dabf1f",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f1bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a generator to lazily load dataset from json file based on dataset argument\n",
    "\n",
    "\"\"\"\n",
    "choice for sample ablation:\n",
    "    - make sure to increase max number of training steps as you go\n",
    "    - do some testing beforehand for 1000, 5000 to find good number of steps\n",
    "    - 10, 20, 50, 100, 250, 500, 1000, 5000\n",
    "    - minecraft\n",
    "    - ucla\n",
    "    - nostupidquestions   \n",
    "    - copypasta\n",
    "    - 4 * 8 = 32 soft prompts\n",
    "    \n",
    "choice (for 100 samples, or optimal from above):\n",
    "    - nerdy\n",
    "    - personal \n",
    "    - alike 3\n",
    "    - unalike 3\n",
    "    - format specific\n",
    "    - college student \n",
    "    - new mother \n",
    "    - creative gen alpha \n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import re\n",
    "import random, math\n",
    "\n",
    "def load_datasets_proportional_objects(datasets_dict: Dict[str, float], total_posts: int) -> List[dict]:\n",
    "    examples: List[dict] = []\n",
    "    \n",
    "    # Get total of all values in datasets_dict\n",
    "    total_proportion = sum(datasets_dict.values())\n",
    "    for dataset_name, proportion in datasets_dict.items():\n",
    "        # Calculate number of posts for this dataset\n",
    "        factor = proportion / total_proportion\n",
    "        target_count = math.ceil(total_posts * factor)\n",
    "        print(f\"Loading {target_count} posts from {dataset_name} dataset ({factor*100:.1f}%)\")\n",
    "        \n",
    "        # Load sampled Reddit posts from JSON created by sample-posts.py\n",
    "        # Each item is a dict with keys: title, subreddit, self_text\n",
    "        try:\n",
    "            with open(f\"../../datasets/{dataset_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                reddit_posts: List[dict] = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find dataset file for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter valid posts (must have self_text and no image_url)\n",
    "        valid_posts = []\n",
    "        for p in reddit_posts:\n",
    "            title = p.get(\"title\", \"\")\n",
    "            self_text = p.get(\"self_text\", \"\")\n",
    "            image_url = p.get(\"image_url\", \"\")\n",
    "            \n",
    "            if self_text and not image_url:\n",
    "                valid_posts.append(p)\n",
    "        \n",
    "        print(f\"Found {len(valid_posts)} valid posts in {dataset_name}\")\n",
    "        \n",
    "        # Sample the target number of posts\n",
    "        if len(valid_posts) >= target_count:\n",
    "            # Randomly sample target_count posts\n",
    "            sampled_posts = random.sample(valid_posts, target_count)\n",
    "        else:\n",
    "            # Use all available posts if we don't have enough\n",
    "            print(f\"Warning: Only {len(valid_posts)} posts available, using all\")\n",
    "            sampled_posts = valid_posts\n",
    "        \n",
    "        examples.extend(sampled_posts)\n",
    "    \n",
    "    # Shuffle the final dataset to mix posts from different datasets\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    print(f\"Loaded dataset {datasets_dict} with {total_posts} posts\")\n",
    "    return examples\n",
    "\n",
    "def load_datasets_proportional(datasets_dict: Dict[str, float], total_posts: int, prompt: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load datasets with proportional sampling.\n",
    "    \n",
    "    Args:\n",
    "        datasets_dict: Dictionary mapping dataset names to their proportions, e.g. {\"minecraft\": 1, \"ucla\": 1} will load half minecraft, half ucla\n",
    "        total_posts: Total number of posts desired across all datasets\n",
    "    \n",
    "    Returns:\n",
    "        List of examples in the format: {\"instruction\": PROMPT, \"output\": post}\n",
    "    \"\"\"\n",
    "    \n",
    "    examples: List[dict] = []\n",
    "    \n",
    "    # Get total of all values in datasets_dict\n",
    "    total_proportion = sum(datasets_dict.values())\n",
    "    for dataset_name, proportion in datasets_dict.items():\n",
    "        # Calculate number of posts for this dataset\n",
    "        factor = proportion / total_proportion\n",
    "        target_count = math.ceil(total_posts * factor)\n",
    "        print(f\"Loading {target_count} posts from {dataset_name} dataset ({factor*100:.1f}%)\")\n",
    "        \n",
    "        # Load sampled Reddit posts from JSON created by sample-posts.py\n",
    "        # Each item is a dict with keys: title, subreddit, self_text\n",
    "        try:\n",
    "            with open(f\"../../datasets/{dataset_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                reddit_posts: List[dict] = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find dataset file for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter valid posts (must have self_text and no image_url)\n",
    "        valid_posts = []\n",
    "        for p in reddit_posts:\n",
    "            title = p.get(\"title\", \"\")\n",
    "            self_text = p.get(\"self_text\", \"\")\n",
    "            image_url = p.get(\"image_url\", \"\")\n",
    "            \n",
    "            if self_text and not image_url:\n",
    "                subreddit = p.get(\"subreddit\", \"\")\n",
    "                subreddit = re.sub(r\"\\s*(/)?r/\", \"r/\", subreddit)\n",
    "                post = f\"title: {title}\\nself_text: {self_text}\\nsubreddit: {subreddit}\"\n",
    "                valid_posts.append({\"instruction\": prompt, \"output\": post})\n",
    "        \n",
    "        print(f\"Found {len(valid_posts)} valid posts in {dataset_name}\")\n",
    "        \n",
    "        # Sample the target number of posts\n",
    "        if len(valid_posts) >= target_count:\n",
    "            # Randomly sample target_count posts\n",
    "            sampled_posts = random.sample(valid_posts, target_count)\n",
    "        else:\n",
    "            # Use all available posts if we don't have enough\n",
    "            print(f\"Warning: Only {len(valid_posts)} posts available, using all\")\n",
    "            sampled_posts = valid_posts\n",
    "        \n",
    "        examples.extend(sampled_posts)\n",
    "    \n",
    "    # Shuffle the final dataset to mix posts from different datasets\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    print(f\"Loaded dataset {datasets_dict} with {total_posts} posts\")\n",
    "    return examples\n",
    "\n",
    "# Example usage - modify these values as needed\n",
    "# datasets_dict = {\n",
    "#     \"ucla\": 0.5,  # 100% minecraft posts\n",
    "#     \"minecraft\": 0.5,  \n",
    "# }\n",
    "# total_posts = 100  # Total number of posts desired\n",
    "\n",
    "# examples = load_datasets_proportional(datasets_dict, total_posts, \"prompt\")\n",
    "\n",
    "# print(f\"Total number of examples loaded: {len(examples)}\")\n",
    "# if examples:\n",
    "#     print(\"Sample example:\")\n",
    "#     print(examples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b9c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess instruction/output dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_dataset(examples, tokenizer):\n",
    "    # Build HF dataset from examples [{\"instruction\", \"output\"}]\n",
    "    dataset = Dataset.from_list(examples)\n",
    "\n",
    "    # Tokenize instruction with chat template, and supervise only the output tokens\n",
    "    def tokenize_io(sample):\n",
    "        # Build chat prompt prefix for the user instruction\n",
    "        messages = [{\"role\": \"user\", \"content\": sample[\"instruction\"]}]\n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        prompt_ids = tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"]\n",
    "        output_ids = tokenizer(sample[\"output\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "\n",
    "        input_ids = prompt_ids + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "        labels = ([-100] * len(prompt_ids)) + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Truncate from the left if too long, keeping alignment between inputs and labels\n",
    "        if len(input_ids) > MAX_SEQ_LEN:\n",
    "            input_ids = input_ids[-MAX_SEQ_LEN:]\n",
    "            labels = labels[-MAX_SEQ_LEN:]\n",
    "            attention_mask = attention_mask[-MAX_SEQ_LEN:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    train_ds = dataset.map(tokenize_io, remove_columns=dataset.column_names)\n",
    "    print(\"Preprocessed dataset...\")\n",
    "    return train_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a47b18",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef940c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login as huggingface_login\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import dotenv, os\n",
    "from peft import PeftModel, PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
    "\n",
    "def login():\n",
    "    dotenv.load_dotenv()\n",
    "    huggingface_login(token=os.getenv(\"HUGGING_FACE_HUB_TOKEN\"))\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {model_name} model and tokenizer\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def init_peft_model(model, model_name: str):\n",
    "    config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=PROMPT_TOKENS,\n",
    "        prompt_tuning_init_text=\"Generate a reddit post.\",\n",
    "        tokenizer_name_or_path=model_name,\n",
    "    )\n",
    "    return get_peft_model(model, config)\n",
    "\n",
    "def apply_peft_adapter(base_model, adapter_name: str):\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_name)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad76af",
   "metadata": {},
   "source": [
    "## Soft Prompt Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fb50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through models, datasets, and dataset sizes to create soft prompt adapters for each \n",
    "# 4 * 8 = 32 soft prompts\n",
    "# 2 * 8 = 16 adapters\n",
    "# take note of training time, adapter size\n",
    "\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train_soft_prompt(model, tokenizer, train_ds, train_steps, output_dir):\n",
    "    def collate_fn(features):\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        batch_size = len(features)\n",
    "        seq_lens = [len(f[\"input_ids\"]) for f in features]\n",
    "        max_len = max(seq_lens)\n",
    "\n",
    "        input_ids = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "        labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "\n",
    "        for i, f in enumerate(features):\n",
    "            ids = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
    "            attn = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
    "            labs = torch.tensor(f[\"labels\"], dtype=torch.long)\n",
    "            L = ids.size(0)\n",
    "            input_ids[i, :L] = ids\n",
    "            attention_mask[i, :L] = attn\n",
    "            labels[i, :L] = labs\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=MICRO_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    # Total optimizer steps we intend to take\n",
    "    total_optim_steps = train_steps\n",
    "    num_warmup_steps = max(1, int(0.1 * total_optim_steps))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=total_optim_steps,\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model_device = device\n",
    "\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optim_step = 0\n",
    "    accumulated = 0\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(10):  # repeat over dataset until reaching desired steps\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(model_device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            (loss / GRAD_ACCUM_STEPS).backward()\n",
    "            running_loss += loss.item()\n",
    "            accumulated += 1\n",
    "            if accumulated % GRAD_ACCUM_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                if optim_step % 10 == 0:\n",
    "                    print(f\"step {optim_step} loss {running_loss / GRAD_ACCUM_STEPS:.4f}\")\n",
    "                running_loss = 0.0\n",
    "                optim_step += 1\n",
    "                if optim_step >= total_optim_steps:\n",
    "                    break\n",
    "        if optim_step >= total_optim_steps:\n",
    "            break\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    print(\"Saved prompt adapter to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d588b",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through models, datasets \n",
    "# 2 * 8 = 16 LORA fine tuned models\n",
    "# take note of training time, model size\n",
    "\n",
    "def train_fine_tune_lora(model, tokenizer, train_ds, train_steps, output_dir):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66d8d1",
   "metadata": {},
   "source": [
    "## Generate posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d20ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 posts per cell \n",
    "# look into inference parallelism\n",
    "# iterate through models, experiments, datasets\n",
    "# write each generated post to json file indicating model, experiment, dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "def generate_post(prompt, model, tokenizer):\n",
    "    if isinstance(model, OpenAI):\n",
    "        response = model.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            input=prompt\n",
    "        )\n",
    "        return response.output_text\n",
    "    \n",
    "    # if not openai, use normal pipeline\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_SEQ_LEN,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    # Extract the generated text and parse the post format\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    \n",
    "    # Initialize the post object\n",
    "    post_obj = {\n",
    "        \"title\": \"\",\n",
    "        \"self_text\": \"\",\n",
    "        \"subreddit\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Split by lines and parse each field\n",
    "    lines = generated_text.strip().split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"title: \"):\n",
    "            post_obj[\"title\"] = line[7:]  # Remove \"title: \" prefix\n",
    "        elif line.startswith(\"self_text: \"):\n",
    "            post_obj[\"self_text\"] = line[11:]  # Remove \"self_text: \" prefix\n",
    "        elif line.startswith(\"subreddit: \"):\n",
    "            post_obj[\"subreddit\"] = line[11:]  # Remove \"subreddit: \" prefix\n",
    "    \n",
    "    return post_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41960fb",
   "metadata": {},
   "source": [
    "## Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ddc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge whether each generated post adheres to dataset category, heuristic based on word content and llm judge\n",
    "\n",
    "def judge_post_gpt5(post, dataset):\n",
    "    JUDGE_PROMPTS = {\n",
    "        \"nerdy\": \"Please judge whether the following post is nerdy. \\n\\npost: {post}\\n\\n\",\n",
    "        \"personal\": \"Please judge whether the following post is personal. \\n\\npost: {post}\\n\\n\",\n",
    "        \"alike 3\": \"Please judge whether the following post is similar to the 3 posts below. \\n\\npost: {post}\\n\\n\",\n",
    "        \"unalike 3\": \"Please judge whether the following post is not similar to the 3 posts below. \\n\\npost: {post}\\n\\n\",\n",
    "        \"format specific\": \"Please judge whether the following post is formatted correctly. \\n\\npost: {post}\\n\\n\",\n",
    "        \"college student\": \"Please judge whether the following post is college student. \\n\\npost: {post}\\n\\n\",\n",
    "        \"new mother\": \"Please judge whether the following post is new mother. \\n\\npost: {post}\\n\\n\",\n",
    "        \"creative gen alpha\": \"Please judge whether the following post is creative gen alpha. \\n\\npost: {post}\\n\\n\",\n",
    "    }\n",
    "    \n",
    "\n",
    "def judge_post_heuristic(post, dataset):\n",
    "    KEYWORDS = {\n",
    "        \"nerdy\": [],\n",
    "        \"personal\": [],\n",
    "        \"alike 3\": [],\n",
    "        \"unalike 3\": [],\n",
    "        \"format specific\": [],\n",
    "        \"college student\": [],\n",
    "        \"new mother\": [],\n",
    "        \"creative gen alpha\": [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad1c6b",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d853c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON output helpers\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "def ensure_parent_dir(path_str: str) -> Path:\n",
    "    path = Path(path_str)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def collect_posts(n: int, prompt: str, model, tokenizer) -> list:\n",
    "    posts = []\n",
    "    for i in tqdm(range(n), desc=\"Generating posts\"):\n",
    "        post = generate_post(prompt, model, tokenizer)\n",
    "        # print(f\"generated post {i}\")\n",
    "        posts.append(post)\n",
    "    return posts\n",
    "\n",
    "def write_posts_json(path_str: str, posts: list) -> None:\n",
    "    path = ensure_parent_dir(path_str)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdddbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"gpt-5\"\n",
    "]\n",
    "\n",
    "\n",
    "# Load dataset categories from JSON file\n",
    "DATASET1_NAMES = [\n",
    "    \"minecraft\",\n",
    "    \"ucla\",\n",
    "    \"nostupidquestions\",\n",
    "    \"copypasta\",\n",
    "    \"varietypack\",\n",
    "]\n",
    "DATASETS1 = [\n",
    "    {\n",
    "        \"minecraft\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"ucla\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"nostupidquestions\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"copypasta\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"nerdy\": 1,  \n",
    "        \"personal\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "        \"tech\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"animals\": 1, \n",
    "        \"boomerhumor\": 1,\n",
    "        \"copypasta\": 1,\n",
    "        \"creative\": 1,\n",
    "        \"food\": 1,\n",
    "        \"nba\": 1,\n",
    "        \"religion\": 1,\n",
    "        \"school\": 1,\n",
    "        \"ucla\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "DATASET2_NAMES = [\n",
    "    \"nerdy\",\n",
    "    \"personal\",\n",
    "    \"unalike\",\n",
    "    \"alike\",\n",
    "    \"formatspecific\",\n",
    "    \"college\",\n",
    "    \"newmother\",\n",
    "]\n",
    "DATASETS2 = [\n",
    "    {\n",
    "        \"nerdy\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"personal\": 1,  \n",
    "    },\n",
    "    { # unalike\n",
    "        \"pop\": 1,  \n",
    "        \"religion\": 1,\n",
    "        \"tech\": 1\n",
    "    },\n",
    "    { # alike\n",
    "        \"tech\": 1,\n",
    "        \"nerdy\": 1,\n",
    "        \"finance\": 1,\n",
    "    },\n",
    "    { # format specific\n",
    "        \"copypasta\": 1,\n",
    "        \"nostupidquestions\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "    },\n",
    "    { # college student\n",
    "        \"ucla\": 1,\n",
    "        \"nerdy\": 1,\n",
    "        \"okbuddy\": 1,\n",
    "        # \"copypasta\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"food\": 1,\n",
    "        \"animals\": 1,\n",
    "    },\n",
    "    { # new mother\n",
    "        \"pregnancy\": 1,\n",
    "        \"parenting\": 1,\n",
    "        \"baby\": 1,\n",
    "        \"food\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"boomerhumor\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "TRAIN_SIZES = [\n",
    "    10, 20, 50, 100, 250, 500, 1000,2000\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    \"self defined\",\n",
    "    \"summary\",\n",
    "    \"like history\",\n",
    "    # \"fine tune\",\n",
    "    \"soft prompt\",\n",
    "]\n",
    "\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 0.2\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"models\"\n",
    "GENERATED_OUTPUT_DIR = \"generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6006704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPTS = {\n",
    "    \"self defined\": {\n",
    "        # user defines their own interests. e.g. a bio of interests\n",
    "        \"nerdy\": \"Please generate one reddit post. Here are some topics of interest: gaming, pcs, tech, mmos, rpgs, anime. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"personal\": \"Please generate one reddit post. Here are some topics of interest: relationship advice, social issues, depression, anxiety, pregnancy, parenting, dreams. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\", \n",
    "        \"alike\": \"Please generate one reddit post. Here are some topics of interest: gaming, pcs, tech, mmos, rpgs, anime, pc building, investment advice, stocks, personal finance. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"unalike\": \"Please generate one reddit post. Here are some topics of interest: popular tv shows, movies, religion and spirituality, building pcs, consumer technology. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"formatspecific\": \"Please generate one reddit post. Here are some topics of interest: silly questions, relatioship advice (am I the asshole?), humorous, often nonsensical stories designed to be copy pasted as a joke. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"college\": \"Please generate one reddit post. Here are some topics of interest: ucla, college life, relationships, gen z humor, popular tv shows, movies, gaming, pcs, tech, mmos, rpgs, anime, cooking, food, cats, dogs, pets. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"newmother\": \"Please generate one reddit post. Here are some topics of interest: pregnancy, parenting, babies, cooking, food, personal relationships, popular tv shows, movies, jokes, puns. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "    },\n",
    "    \"fine tune\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "    \"soft prompt\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def __load_posts(dataset):\n",
    "    posts = load_datasets_proportional_objects(dataset, 100)\n",
    "    posts_str = \"\"\n",
    "    for post in posts:\n",
    "        posts_str += f\"title: {post['title']}\\nself_text: {post['self_text']}\\nsubreddit: {post['subreddit']}\\n\\n\"\n",
    "    return posts_str\n",
    "\n",
    "def generate_summarized_prompt(client, dataset):   \n",
    "    # read in 100 posts from json dataset\n",
    "    posts_str = __load_posts(dataset)\n",
    "        \n",
    "    summary_prompt = f\"\"\"Based on the following posts that a user has liked, generate a summary of the user's interests.\n",
    "    {posts_str}\n",
    "    \"\"\"\n",
    "    \n",
    "    # summarize posts\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        input=summary_prompt\n",
    "    )\n",
    "    \n",
    "    summary = response.output_text\n",
    "    \n",
    "    prompt = f\"Please generate one reddit post. Here is a summary of the user's interests:{summary}\\n\"\n",
    "    \n",
    "    prompt += \"Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def generate_like_history_prompt(client, dataset):\n",
    "    posts_str = __load_posts(dataset)\n",
    "    \n",
    "    prompt = f\"Please generate one reddit post similar to the following posts:\\n{posts_str}\\n\"\n",
    "    \n",
    "    prompt += \"Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f9a65",
   "metadata": {},
   "source": [
    "### testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a9f9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # single test\n",
    "\n",
    "# model_name = MODELS[0]\n",
    "# dataset_name = DATASET1_NAMES[0]\n",
    "# dataset_dict = DATASETS1[0]\n",
    "# train_size = TRAIN_SIZES[3]\n",
    "# print(model_name, dataset_name, train_size)\n",
    "\n",
    "# # load data + model\n",
    "# login()\n",
    "# model, tokenizer = load_model(model_name)\n",
    "# examples = load_datasets_proportional(dataset_dict, train_size, PROMPTS[\"soft prompt\"]) \n",
    "# train_ds = preprocess_dataset(examples, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ef0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/soft_prompt_{model_name}_{dataset_name}_{train_size}\" \n",
    "\n",
    "# # initialize peft model\n",
    "# peft_model = init_peft_model(model, model_name)\n",
    "\n",
    "# # train soft prompt\n",
    "# train_soft_prompt(peft_model, tokenizer, train_ds, NUM_TRAIN_STEPS, soft_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4e54cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate posts\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/soft_prompt_{model_name}_{dataset_name}_{train_size}\" \n",
    "\n",
    "# # load peft model\n",
    "# peft_model = apply_peft_adapter(model, soft_output_dir)\n",
    "\n",
    "# out_path = Path(f\"{GENERATED_OUTPUT_DIR}/train_size_ablation/{model_name}_{dataset_name}_{train_size}.json\")\n",
    "# out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# posts = collect_posts(5, PROMPTS[\"soft prompt\"], peft_model, tokenizer)\n",
    "# write_posts_json(out_path, posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (1.77.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/einar/miniconda3/envs/slop/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe6a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 17 posts from ucla dataset (16.7%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loading 17 posts from nerdy dataset (16.7%)\n",
      "Found 271 valid posts in nerdy\n",
      "Loading 17 posts from okbuddy dataset (16.7%)\n",
      "Found 14 valid posts in okbuddy\n",
      "Warning: Only 14 posts available, using all\n",
      "Loading 17 posts from pop dataset (16.7%)\n",
      "Found 47 valid posts in pop\n",
      "Loading 17 posts from food dataset (16.7%)\n",
      "Found 10 valid posts in food\n",
      "Warning: Only 10 posts available, using all\n",
      "Loading 17 posts from animals dataset (16.7%)\n",
      "Found 43 valid posts in animals\n",
      "Loaded dataset {'ucla': 1, 'nerdy': 1, 'okbuddy': 1, 'pop': 1, 'food': 1, 'animals': 1} with 100 posts\n",
      "Please generate one reddit post. Here is a summary of the user's interests:Hereâ€™s a concise profile of the userâ€™s interests based on the liked posts:\n",
      "\n",
      "- UCLA student life: course planning and professor selection (Chem, Stats, Math 33A, MIMG/EEB), research/internships, tutoring/mental health resources, campus logistics (Rooter buses, LAX rides, makerspace), and general UCLA community updates.\n",
      "- Home cooking and food nerding: comfort classics (mashed potatoes, gratins), technique troubleshooting (stainless steel, brown butter, balancing sweetness), food safety, kitchen tools, and special-diet cooking (Crohnâ€™s-friendly, glutenâ€‘free).\n",
      "- Pets and animal care: strong interest in dogs (hypoallergenic breeds, weight loss, bedding, training/potty issues, adolescent behavior), cats (DIY cat setups, vocalization and feeding behavior, litter/marking problems), and freshwater aquariums (corydoras behavior, cichlid aggression, shrimp health).\n",
      "- Gaming enthusiast: broad, multi-platform tastes (Skyrim, Fallout, RDR2, Tarkov, Hearthstone, Valorant, Overwatch, Rocket League, TF2, Fortnite), into modding/troubleshooting, game systems/preservation, and debates about digital vs physical ownership.\n",
      "- Fantasy/sciâ€‘fi and genre TV/books: deep-dive lore and theorycrafting for ASOIAF/House of the Dragon and Harry Potter; Handmaidâ€™s Tale worldbuilding; horror (Hammer), plus meta takes on Breaking Bad and Chucky; also enjoys Gilmore Girls.\n",
      "- Reality TV chatter: Love Is Blind relationship analysis and cast dynamics.\n",
      "- Internet humor and meme culture: frequent copypasta, TwoSentenceHorror, punny jokes, teenagers-sub style posts, and general meme absurdity.\n",
      "- Etiquette and everyday questions: pet etiquette (lawns), community norms, and small life logistics.\n",
      "\n",
      "Overall: a UCLA-based, pet-loving home cook and aquarium keeper with broad gaming tastes, a soft spot for fantasy/horror and reality TV discourse, and an active interest in campus resources and internet humor.Use this format. \n",
      "\n",
      "title: {title}\n",
      " self_text: {self_text}\n",
      " subreddit: {subreddit}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test summary prompt\n",
    "# from openai import OpenAI\n",
    "# import os\n",
    "# import dotenv\n",
    "\n",
    "# dotenv.load_dotenv()\n",
    "\n",
    "# client = OpenAI()\n",
    "# prompt = generate_summarized_prompt(client, DATASETS2[5])\n",
    "# print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7eb6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 17 posts from ucla dataset (16.7%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loading 17 posts from nerdy dataset (16.7%)\n",
      "Found 271 valid posts in nerdy\n",
      "Loading 17 posts from okbuddy dataset (16.7%)\n",
      "Found 14 valid posts in okbuddy\n",
      "Warning: Only 14 posts available, using all\n",
      "Loading 17 posts from pop dataset (16.7%)\n",
      "Found 47 valid posts in pop\n",
      "Loading 17 posts from food dataset (16.7%)\n",
      "Found 10 valid posts in food\n",
      "Warning: Only 10 posts available, using all\n",
      "Loading 17 posts from animals dataset (16.7%)\n",
      "Found 43 valid posts in animals\n",
      "Loaded dataset {'ucla': 1, 'nerdy': 1, 'okbuddy': 1, 'pop': 1, 'food': 1, 'animals': 1} with 100 posts\n",
      "Please generate one reddit post similar to the following posts:\n",
      "title: I have a hole in my sock\n",
      "self_text: Itâ€™s showing my big toe ðŸ˜±\n",
      "subreddit: teenagers\n",
      "\n",
      "title: Tweezers\n",
      "self_text: Anyone here use tweezers when making hearty pan sauce pastas? I've seen a number of youtubers use them and I want to ask for a pair for the holidays that can stand up to making cacio e pepe or any other pan sauce. Any that you use or prefer? Thinking about these:   https://dalstrong.com/products/dalstrong-high-precision-black-titanium-coated-professional-tweezers  I know they're on the pricy side but looking for a nice gift for myself.\n",
      "subreddit: Cooking\n",
      "\n",
      "title: Emperors Children\n",
      "self_text: Does anyone else think itâ€™d be interesting to see the 3rd legion return to the imperium with clone Fulgrim in charge and Fabious Bile as the lieutenant commander? It was hinted at in Fabiousâ€™ second book, that he strongly considered it. The option is still there just take clone Fulgrim back and use the hidden cache of gene seed to remake a pure 3rd legion.\n",
      "subreddit: Warhammer40k\n",
      "\n",
      "title: Can I add powdered milk to browning butter to get more 'browned flavor'\n",
      "self_text: If browned milk solids are what makes browned butter taste so fucking good, can I add powdered milk to butter that I am Browning to get more flavor?\n",
      "subreddit: Cooking\n",
      "\n",
      "title: Ostracised in my major\n",
      "self_text: Help a fellow bruin out!  So I'm in a pretty small major, and there is this prominent clique of friends who are more well known within the community. They are super involved in the major related club, also doing research and stuff with the more prominent professors in the major  For some reason, this clique of people don't really like me. I never did anything to them, or anything against them. I guess that I'm just awkward and shy, and I get that sometimes my social cues aren't the best and i might rub people off the wrong way (I'm working on it).  It's just that these people are in every major related event, and as much as i can and want to just live alone, they look at me funny and probably talk behind my back to peers or professors  Honestly don't know what to do, or how to salvage my reputation. I didn't expect people here to be so unsure l immature or have the mean girls complex like high schoolers...\n",
      "subreddit: ucla\n",
      "\n",
      "title: Math 31AL w/ Demiroglu\n",
      "self_text: Iâ€™m not in the class but sheâ€™s teaching Math 31B next quarter and just wanted to hear how her class / teaching is.\n",
      "subreddit: ucla\n",
      "\n",
      "title: Is it normal for people to let their dogs do their business in other peoples lawns?(even if they clean it up)\n",
      "self_text: I just saw this today and Iâ€™ve seen it for awhile, and I never let my dog onto anyones lawn, I always make sure itâ€™s not owned by anyone like grass beside a sidewalk. It just feels like a disregard of someoneâ€™s personal property, is it normal?\n",
      "subreddit: NoStupidQuestions\n",
      "\n",
      "title: My Albino Cory is going crazy and Iâ€™m worried\n",
      "self_text: Alright, so I have a 29 Gallon tank. I have 5 Guppies 3 Mollies and until yesterday 3 Corydoras.   The pet store only had 3 Coryâ€™s left when I bought them. I know they need a school of at least 6 so I bought 4 more Coryâ€™s yesterday. (Also 3 little pink shrimp and a Nerite Snail)   Now today my Albino is going freaking nuts, itâ€™s swimming up down and left and right non-stop a million miles a minute for hours.   Iâ€™ve tested my waters and everything is fine but my PH is a bit high 7.8 as far as I can tell. I quickly ran to the pet store and put some moss stuff in there and the little guy is still rippin.   Is he sick? What should I do? Also he wonâ€™t school with the Pandas (I have 6 pandas and 1 albino) the pet store said it was really weird they wonâ€™t school.\n",
      "subreddit: Aquariums\n",
      "\n",
      "title: How were Muggle-Born Wizards Able to Afford Their Education?\n",
      "self_text: This was a weird shower thought, but hear me out.  As repeatedly established throughout the franchise, the financial systems of Wizarding Britain and Muggle Britain are very, VERY different.  Going by 1991 when the first Harry Potter book takes place, Britain uses the Pound as its main currency, right?  Meanwhile, Wizarding Britainâ€™s currency is the Knuts, Sickles, and Galleons.  Unless there is some kind of secret exchange rate mentioned somewhere in the franchise I have forgotten, due to the Wizarding World wanting to keep itself secret, Great Britainâ€™s regular money is USELESS to wizards and witches unless itâ€™s for collectors of Muggle stuff like Arthur Weasley.  So that said, say youâ€™re a 10-11 year old who had never really experienced magic, when youâ€™re suddenly given a petter by an owl saying youâ€™ve been accepted to an arcane castle/boarding school youâ€™ve NEVER heard of, and then told the first thing you need to do is buy your own school supplies.  You somehow manage to find your way to where you can pick up said school supplies, but you find out you and your parentsâ€™ money is no good here.  How the heck did Muggle-born students like Hermione and Lily AFFORD this?  Can someone please help me answer this?\n",
      "subreddit: harrypotter\n",
      "\n",
      "title: Housing guarantee\n",
      "self_text: Can anyone tell me how much housing guarantee I have exactly? I keep seeing different things from different sources lol. I'm a 3rd year currently. My freshman year was the year of COVID and I stayed home. I had a dorm my sophomore year as well as this year. Thanks.\n",
      "subreddit: ucla\n",
      "\n",
      "title: Will Andrew Wiggins make an All-Defensive team this season?\n",
      "self_text: As the title states: do you believe he will make either 1st/2nd team by seasons' end? He's a great defender, both one-on-one and as a help/team defender.\n",
      "subreddit: nba\n",
      "\n",
      "title: Yâ€™allâ€¦ you donâ€™t need to push to get on the Bus\n",
      "self_text: For those who take the bus from Weyburn to Campus, Yâ€™all donâ€™t have to push and shove people to try to get a seat on the bus, weâ€™re all gonna get on the bus EVENTUALLY\n",
      "subreddit: ucla\n",
      "\n",
      "title: To Those Who Barged In During My Midterm\n",
      "self_text: I understand that you are being unfairly treated by your employer, but why does that make you think you can take from me my right to succeed in my academic pursuits?  The midterm that you disrupted by storming the classroom hallway while blaring instruments and chants is worth 60% of my grade. I spent weeks studying for it: I sacrificed time with my friends and doing things that I enjoy so I could perform well. Your actions stated that my time, my energy, and my goals don't matter; it was essentially a \"fuck you\" of the highest magnitude.  As a student here, this university is my home. It is a place for me to grow as a person and a scholar. As such, I am entitled to a nurturing learning environment that enables the best for me. You undermined that.  How could highly educated men and women of learning, who are also responsible for teaching students, find it morally feasible to actively sabotage students' learning outcomes? If you really don't give a fuck about students and student outcomes, then perhaps you should not even be a TA.  *just to clarify, the strikers did not leave after barging in, but instead stayed in the hallway 5 feet away from the students, screaming and blaring their noise-makers during the midterm. Some things are minor inconveniences. This is not one of them.  (math midterm was in Bunche Hall from 10AM to 11AM).\n",
      "subreddit: ucla\n",
      "\n",
      "title: I think every man who listens to the album MM..FOOD! by the artist MF DOOM should be considered an ally to Black sociopolitical issues- no matter their race.\n",
      "self_text: Personally, I think the fact that I, a white man from the rural dirges of Montana, as a voracious listener of MF DOOM, warrant some sort of social justice clout. I am actively engaging with vintage Black culture, despite their troubled pasts and connections (MS-13 for example). Whenever my friends of color are driving with me, I always am cognizant of their taste. You can find me blasting the hit song \"Hoe Cakes\" (censoring out the explicit lyrics of course) or crying along to \"Poo-Putt Platter\" any day of the week. Overall, I may not have gone to any of these \"BLM\" protests (personally I think the whole movement is devolving down the perilous route of the cultural Marxist zeitgeist) but I should not be \"cancelled\" or persecuted by moralist mobs for my supposed \"inaction\", since I listen to MF DOOM!\n",
      "subreddit: copypasta\n",
      "\n",
      "title: Emotional Support Dog In Chicago\n",
      "self_text: Hey All, I live in Chicago.  I live in an apartment that does NOT allow for pets. However, I ended up getting a puppy because my psychologist recommended a dog + my psychologist wrote me an ESA letter. Can I be evicted from the apartment even if I have a letter?  Not that it matters but I have experience raising a puppy.  Edit: deleted the last part.\n",
      "subreddit: legaladvice\n",
      "\n",
      "title: What made the big three so successful in the west compared to other shows?\n",
      "self_text: I've recently been wondering, what exactly was it that made Anime like Naruto, One Piece, Bleach, Dragonball and Pokemon blow up as much as they did in the West, compared to many other titles. I know, that I've mentioned the big three in the title. However, this question in my opinion applies to all of the shows that became mainstream hits during the late 90s, early 2000s.  Was it the fact that those shows had plenty of episodes compared to other shows, especially coming out after or during the OVA craze of the 80s and 90s?  Or were they just better marketed and thus managed to reach the masses?  Or was it merely a coincidence, simply because they were aired at the perfect time for kids to watch after coming home from school?  Did they do anything so spectacularly different from other (western) shows that were airing at the time?  And most importantly, why have other (modern) Anime failed to replicate this success. Sure, we have new titles that are extremely successful like MHA, AoT or recently Chainsaw Man. But I'd argue that they are nowhere near as popular, especially outside of the Anime fandom.  What do you guys think? I would love to hear your thoughts on the matter.\n",
      "subreddit: anime\n",
      "\n",
      "title: Moody can look at people naked\n",
      "self_text: I was reading the goblet of fire when when i realised something. Harry is wearing socks that he got as a present from dobby and moody look through his robes and says something along the line of nice socks potter. This means he can watch through clothes and I assume even underwear.\n",
      "subreddit: harrypotter\n",
      "\n",
      "title: Viren is such a babygirl\n",
      "self_text: Sorry didn't have an idea for a title, I just wanted to say how I noticed that Viren was such a softie without his staff, he literally  wanted to give things up and just live his final days with peace, watching sunshines and shii, and when he got his staff back he truned back into big scary magic guy, #bringebabygirlvirenback (kinda a joke post, im just scrolling through this subreddit trying to fill up the void after watching the new season........)\n",
      "subreddit: TheDragonPrince\n",
      "\n",
      "title: usc ucla student section\n",
      "self_text: lookin to buy 4 tickets, lmk !!\n",
      "subreddit: ucla\n",
      "\n",
      "title: Those with cats that have had urinary blockage and are now on UT diets\n",
      "self_text: We have recently dealt with another episode regarding our 3 yo adopted male tabby and crystals forming in his bladder, leading to strained urination and apparent discomfort. Luckily he did not have a complete blockage, but this is becoming pretty frequent for him.  He is eating a can of Royal canin ut prescribed food, but also gets treats occasionally. It was brought to my attention today by a vet tech that he is not supposed to have ANY other food outside of his prescribed soft food. Is that the case? If so, it was never made that clear but might make sense. Thanks!\n",
      "subreddit: cats\n",
      "\n",
      "title: [Pet Injury][California] My dog was injured at a kennel, should I pursue further action?\n",
      "self_text: My wife and I were out of town a couple weeks ago, so we dropped off our 5-month puppy (golden retriever/poodle/shiba mix) at a dog boarding place* (not kennel sorry) over the weekend for the first time. I received a call the next day saying our puppy fractured her leg and they're not exactly sure how. They mentioned it was from \"normal play\" and that they were terribly sorry. They took our puppy to their vet, who took x-rays and revealed both her leg and knee were fractured, which would require surgery ASAP.  I find it very hard to believe this happened from just \"normal play\", but the boarding place keeps insisting this and when I requested for video footage, they claimed that their cameras do not store footage (which did not make sense to me). I did get written evidence through text that they claimed fault and that they would pay for our dog's surgery. However, prior to dropping our dog off, they had me sign a waiver freeing them of any liability if something were to happen.   Our dog had her surgery on Nov. 10, a little over 2 weeks ago, which costed $6,000 out of our pocket, and when I last contacted the boarding place yesterday, all they say is that they're working on it with their insurance. I do have pet insurance, but I would need to claim it within a month of the incident. Should I just claim it under my insurance since I am running out of time? Should I threaten with legal action? Do I even have a case?\n",
      "subreddit: legaladvice\n",
      "\n",
      "title: Potential Unpopular Opinion\n",
      "self_text: !!SPOILER FOR SEASON 5!!  I didn't like Deana. For the most part at least. She knew Pete was abusing Jesse and she chose to turn a blind eye to it because he was a doctor. She turned down every ulternative Rick suggested to stop him. And when a fight broke out between the 2, she held a meeting to throw RICK out of Alexandria. She only seemed to care about how violent Pete was when he did something that directly effected her. I've watched the show a few times now and I just can't bring myself to like her.\n",
      "subreddit: thewalkingdead\n",
      "\n",
      "title: Is there a way/mod that would dissable military clothing ?\n",
      "self_text: I am looking for a way/mod that would dissable military clothing a for rp so it would feel more authentic.\n",
      "subreddit: projectzomboid\n",
      "\n",
      "title: Blissbug in a Jar\n",
      "self_text: Just updated to anniversary edition and was checking out the new homestead options, I personally love the dedicated stands for the masks and paragons and claws, etc. But itâ€™s absolutely killing me that there is a shelf for 5 different bugs in jars- and I just was dungeon slaying and found a â€œBlissbug in a jarâ€ that doesnâ€™t have a dedicated spot!!!! And there isnâ€™t a space for Miraakâ€™s mask with the others. Anyone else notice this yet??\n",
      "subreddit: skyrim\n",
      "\n",
      "title: I'm tired of misinformation on dog nutrition and fearmongering on WSAVA brands\n",
      "self_text: Title, I sometimes buy bones and raw stuff for my dog on a store, through messaging they asked me for a review of my last purchase but also asked what I give my dog as food, and my mistake was replying with the truth, Royal Canin.  The conversation ended with store telling me I was poisoning my dog and that corn is bad for dogs. Honestly, I just told them I believe real vets with certificates (I shared with them lots of tufts articles), and they told me that they prefer people who feed dog chow since they are poor and can't afford real food like, idk, Acana?  Looking at the certificates of those vets, they had DNM in their title, which is not a real university and the course looks like a bold 1990's webpage trying to sell you a scam.  When I showed them what WSAVA meant, articles on corn, on nutrition, on bacterial safety, they shut all down and just ended with \"good to have a debate but you are doing your dog wrong bye\" and never replied again.  It just bothers me because these people I bet do the same to lots of clients and try to use their credentials in order to fear monger others.  As an end note, I told them I think raw feeding is great IF you have a nutrition vet in charge of doing a balanced diet specifically for your dog, and that not all dogs do well on it. However they still felt like they should tell me how awful kibble is \"speacilly Royal Canin\"  Just venting I guess, I'm just so angry because they might get others trough that technique  &#x200B;  EDIT: Clarifying some things because of language barriers. This all happened through whatsapp, not on a physical location, they talked to me through the official store account.\n",
      "subreddit: dogs\n",
      "\n",
      "title: Am noob tank. How do I improve?\n",
      "self_text: I'm playing a prot paladin. I tried following guides online but even the beginner ones are a bit overwhelming. Can someone answer some questions for me, please?  These are for dungeons and raids alike but let me know if the answer changes based on whether it is a raid.   Do I focus on the highest or lowest health mob of a group? Or are there some other criteria?  Do I focus on the same mob until it dies, or do I cycle to keep all mobs' aggro on me?  Is my responsibility to keep the most powerful mob's aggro on me, or all of them?  Do I watch everyone's health and do something when someone is about to die?  What blessings do I cast on who and when? How do I decide?  I understand that you get aggro from a mob when you damage them. DPSs naturally do the most damage. How come a tank can keep more aggro in comparison? How does that work?  Do I do anything other than attacking and using \"hand of reckoning\" to keep aggro on me?   Thanks a lot in advance.\n",
      "subreddit: wow\n",
      "\n",
      "title: bruh why the nail on squichbop head\n",
      "self_text: waltuh\n",
      "subreddit: teenagers\n",
      "\n",
      "title: Balancing Sweetness of Dates\n",
      "self_text: Whenever I make a savory dish that includes dates, they end up overwhelmingly sweet and nearly inedible. Dishes I'm referring to are Middle Eastern inspired dishes like a lamb tagine. Are there ways to counteract the sweetness like adding acidity or a bitter element? I'm not of Middle Eastern decent, so I'd love some advice from someone who is more familiar with the cuisine. Thanks!\n",
      "subreddit: Cooking\n",
      "\n",
      "title: Just woke up\n",
      "self_text: My day is ruined\n",
      "subreddit: teenagers\n",
      "\n",
      "title: I just think my cat is awesome, and wanted to show him to the world.\n",
      "self_text: Meet Mister!\n",
      "subreddit: cats\n",
      "\n",
      "title: What happens to muggle-borns who donâ€™t go to Hogwarts?\n",
      "self_text: Letâ€™s use Hermione as an example because we know she became a powerful witch. What if Hermione chose not to go to Hogwarts? Could she have studied magic on her own later in life? Could she have performed wandless magic? Made potions?  Considering both potential magical ability and what the Ministry of Magic allows, what do you think becomes of muggle-borns who donâ€™t go to Hogwarts?  Edit: If you think attending Hogwarts is mandatory, how is it enforced for people who donâ€™t want to go?\n",
      "subreddit: harrypotter\n",
      "\n",
      "title: One of my cats yell uncontrollably when he's about to get food. How do I discourage that behavior?\n",
      "self_text: I mean, any tiny morsel of food at any time.   E.g. when he sees me bring home a sandwich, and he knows once I'm done he will get a tiny piece of the meat, he starts yelling the moment I sit down to eat until the food gets in his mouth.   Treats, when he's about to get treats, he yells non stop until the treat is in his mouth  Dinner, when it's dinner time, he will tell the whole time I'm prepping his bowls and food.\n",
      "subreddit: CatAdvice\n",
      "\n",
      "title: Perpetually sick\n",
      "self_text: Hi y'all  It's my first quarter here and somehow I am caught in a loop of always being sick. It's getting to the point where it has negatively impacted my grades and has lowkey kinda made me depressed.   I missed all of week 4 because I was bed ridden and ended up playing catch up for my classes the day before the midterm. I haven't done well on my midterms and while I'm confident in my ability to learn the material and ultimately at least pass the class, its incredibly difficult mustering the energy when sick.  Is there some secret to not being sick while at uni?   It sucks feeling like crap, working my ass off to catch up, and then still bombing exams.\n",
      "subreddit: ucla\n",
      "\n",
      "title: broke boy season?\n",
      "self_text: the age differences made finances weird this season and that made the dynamics so weird. cole chose Zanab because he could tell she was rich. thats why he labeled colleen his â€œpoorâ€ option. Bartiste was done w raven when she started her sugar daddy/trick spiel bc he wants to drain a womanâ€™s pockets, not the other way around. Now heâ€™s rubbing his little hands together over Nancyâ€™s money.  Brennon started the proposal saying heâ€™s broke. like??  and SK is a damn student. Talking about he needs to downsize.   Mind you, theyâ€™re pairing them with women in their 30s, who all expressed they wanted kids. Itâ€™s just weird, and set them up for failure I think. Alexa wants a house, Ravenâ€™s ready for rent divided by 2, Nancy got houses to buyâ€¦like realistically women still desiring children past 30 need real providers.\n",
      "subreddit: LoveIsBlindOnNetflix\n",
      "\n",
      "title: My new plants didn't make it.\n",
      "self_text: I got these plants last week and was told they're juvenile and will need to float untill they grow roots, at which point I can plant them.  Unfortunately they died before I got the chance.   Did I do something wrong or was that just bad advice?   (Five gallon, one Betta, one snail. With heater and filter, water test in pics.) ((sorry I post so much I swear I'm trying my best))\n",
      "subreddit: Aquariums\n",
      "\n",
      "title: SAMN ZAMN RAMN N BAMN DAMNNNNNNNNNNNN\n",
      "self_text: can we make this a copypasta please? (uwu pls?ðŸ˜©ðŸ¥µðŸ˜«ðŸ¥µ)\n",
      "subreddit: teenagers\n",
      "\n",
      "title: Locking freighter scanner behind teleported is a very bad idea.\n",
      "self_text: Learning that freighter not only got it's own independent system scanner but it's locked behind the most useless freighter module in the game was very disappointing. Let's be honest, teleport was something you probably unlock last just to 100% your freighter. It's pretty much useless, especially for a whopping price of 5 frigate modules. I understand that being able to use ship's scanner in previous versions was kind of an exploit and has to be fixed. Yet the freighter scanner basically having a cost 9 frigate module is very painful. Those are very difficult to acquire.\n",
      "subreddit: NoMansSkyTheGame\n",
      "\n",
      "title: Does anyone else's dog lay close to the fireplace for so long they're hot to the touch?\n",
      "self_text: When my dog Buddy was alive, whenever we had a fire going in the fireplace he'd lay down with his back to the fire, and he'd be really close to it. He'd stay there for so long that even when he walked away he was hot to the touch, and it hurt to try to pet him.  Do/did any of y'all's dogs do that? Does anyone know why Buddy did it?\n",
      "subreddit: dogs\n",
      "\n",
      "title: SK &amp; Raven\n",
      "self_text: Although unconventional at the beginning. SK and Ravenâ€™s relationship seemed so honest and pure.  I am convinced about the fact they are dating/engaged but just decided not to get married as a part of this experiment.   Ps: Tbh that is what any sane person would do.  UPDATE: SK has lost my respect. Sorry.\n",
      "subreddit: LoveIsBlindOnNetflix\n",
      "\n",
      "title: Better than Bouillon Garlic.....Mashed Potatoes?\n",
      "self_text: Anyone ever incorporate \"Better than Bouillon Roasted Garlic\" into their mashed potatoes, and if so, how?\n",
      "subreddit: Cooking\n",
      "\n",
      "title: Washing the famous \"Donut Bed\"?\n",
      "self_text: Hey y'all! Does anyone have recommendations for washing the famous Best Friends by Sheri donut shag cuddler bed? My friend washed hers and the fur ended up all yucky and stuck together--not a look we are going for! The website says to just wash it in the gentle cycle, but thought I would crowdsource before I risked it! TIA!\n",
      "subreddit: dogs\n",
      "\n",
      "title: you people are always saying you're so lonely and you have no family or friends and then you post a picture of yourself in a maid costume or something weird like that\n",
      "self_text: hmmm... i wonder why you're so aloneðŸ¤”\n",
      "subreddit: teenagers\n",
      "\n",
      "title: It was a really stupid idea to have Jennifer Tilly play an in-universe version of herself\n",
      "self_text: Tilly first played Tiffany as a human in Bride of Chucky, then she ended up in doll form and then switched bodies with the movie Tilly while Tilly now plays Tiffany in the body of Tilly and Tilly now voices her fictional self in Tiffanyâ€™s body which doesnâ€™t seem that much different than when Tilly played Tiffany before in doll form. This whole thing is such a mindfuck.\n",
      "subreddit: Chucky\n",
      "\n",
      "title: MTV and 100 channels\n",
      "self_text: I started to re-watch yesterday. I missed out on the original airing and started the show maybe 6 years ago and absolutely loved it. I am on S1E3 and it's funny to hear the many changes in things comparing when the show aired to now. For example Emily was saying to Lorelai \"I mean, in today's age of MTV and 100 channels...\". It's just funny now that MTV is irrelevant and there are over 500 channels on many providers.\n",
      "subreddit: GilmoreGirls\n",
      "\n",
      "title: Walter White is a thoroughly Nietzschean hero\n",
      "self_text: Walter White is a thoroughly Nietzschean hero, one of the few compelling examples of such figures that American culture has ever produced. It was the philosopher FriedÂ­rich Nietzsche who foresaw the predicament of Walter White, and of modern society, with uncanny clarity: We who inhabit modern societies are prone to a characteristic weakness, a pusillanimity in the face of the petty conventions and preoccupations of bourgeois existence. The fundamental challenge is to fashion a life that overcomes this debased condition, to rise from â€œlast manâ€ to â€œoverman.â€ Some think that Nietzscheâ€™s overman is an immoral, power-driven tyrant, who overcomes conventional morality by asserting a nihilistic will to power. But this is a misreading. Nietzscheâ€™s overman rises above conventional morality not by embracing nihilism but by forging a narrative coherence to his life that issues in a kind of empowerment, or self-possession. The appeal of such empowerment, and of a hero who strives for it, is especially strong at this moment. In postrecession America, there is a mounting desire to throw off the straitjacket of a self-serving upper-middle-class morality that valorizes the sanctity of law and of â€œplaying by the rules.â€ In many circles, there is a growing sense that the system is rigged, that the rules and conventions in place, defended in the name of fairness, efficiency, safety, or â€œwin-winâ€ economic arrangements, serve to empower some at the expense of others. Many find themselves enmeshed in absurd bureaucraticÂ hierarchiesâ€”inÂ work and in everydayÂ lifeâ€”thatÂ afford scant grounds for self-possession or self-respect. At the same time, there is a widespread sense that the â€œlegitimateâ€ economy fails to reward the effort and talents that ordinary people devote to their jobs. Instead, it heaps benefits on those who are simply lucky, or who manipulate the system, or who have a knack for getting rich off other peopleâ€™s work (sometimes ruthlessly) while operating within the bounds of the law. People find themselves increasingly in the condition of WalterÂ Whiteâ€”miredÂ in routines of work that do not call forth their potential or accord them much dignity. Like Walt, many find themselves struggling to be recognized for the work they do, on the job or at home. It is against this cultural backdrop, I believe, thatÂ Breaking BadÂ still captivates public attention and illuminates the anxieties of our time. Nietzsche helps us interpret the improbable life story of Walter White and understand how Walt came to represent a kind of hero for this moment. Waltâ€™s turn to crime is not simply a desperate move that spirals out ofÂ controlâ€”rather,Â it is a story of empowerment. In â€œbreaking bad,â€ Walt rises to self-possession from a contemporary form of malaise and despair. Nietzsche also helps us understand the kind of self-possession Walt gains. On the surface, Waltâ€™s trajectory might seem to suggest a grim, nihilistic drive to self-assertion through conquest: the way to truly live is to reject the social order in which you find yourself stifled and adopt a self-serving â€œlive your own wayâ€ ethic. But this reading of Walter White is misguided. Walt does not simply despair of humanity and become a scornful manipulator of society who only enjoys domination over others. He is not the archetypalÂ nihilistâ€”likeÂ Roskolnikov fromÂ Crime and PunishmentÂ or Malvo from the reÂ­cent TV seriesÂ Fargoâ€”who seeks destruction for its own sake. What makes Walt a complex protagonist, and what makes his traÂ­jectory moving, is that his â€œbreaking badâ€ constitutes an attempt to forge a narrative coherence to his fractured and nearly expired life. Waltâ€™s rise to power in the meth business can be understood, in Nietzschean terms, as an attempt to â€œredeem the past,â€ to reclaim a coherent whole out of the disparate parts of his life: his role as chemist, entrepreneur, husband, father, and even teacher. The self-possession for which he strives is thus bound up with a totality of commitments that cannot be captured by simple notions of egoism, selfishness, and conquest.  Nietzsche helps us diagnose the initial predicament of Walter White as the condition of the â€œlast man,â€ a degenerate state of existence that Nietzsche identifies as pervasive in modern life. The last man, writes Nietzsche, enjoys â€œa little pleasure by day, and a little pleasÂ­ure by night,â€ but always â€œwith a regard to health.â€ The last man â€œhas left the regions where it was hard to liveâ€ and ekes out an easy but hollow existence aimed at an ever-receding vision of repose. Absent from his life is any risk, adventure, or ambition: â€œâ€˜What is love? What is creation? What is longing?â€™ Thus asks the last man, and he blinks.â€1 Walter White is the last man, or very close to it. He is confined to a â€œlittle pleasure byÂ dayâ€â€”theÂ breakfast of eggs, veggie bacon, and echinacea that his wife Skyler prepares for his fiftiethÂ birthdayâ€”â€œaÂ little pleasure byÂ nightâ€â€”theÂ dispassionate, almost clinical hand job that she administers like medicine beforeÂ bedâ€”andÂ all â€œwith a regard to health.â€ Nietzscheâ€™s â€œlast manâ€ captures a condition to which every life is susceptible. As we sympathize with Walter White, we acknowledge the possibility of the last man in ourselves. In one way or another, weâ€™ve all been thereÂ beforeâ€”inÂ moments of enervation, lack of gumption, obsession with health, safety, and a long life, or earnest submission to the so-called necessities of a bourgeois existence. One of the brilliant conceits ofÂ Breaking BadÂ is its relentless exposure of the petty officiousness and stifling moralism of middle-class suburban life. The show constantly juxtaposes Waltâ€™s harrowing travails and frank self-assertion in the underworld with the quotidian concerns and vapid, impersonal niceties of the law-abiding community. In extricating himself from the condition of the last man, Walt does not merely oppose his oppressive world with an angry â€œNo!â€ He finds direction and rises to a kind of virtue that comes to light in Nietzscheâ€™s conceptions of the â€œovermanâ€ and the â€œwill to power.â€ Unfortunately, these key concepts are as liable to misinterpretation as Walter White himself. Due to misleading ideological appropriations of Nietzsche, and careless readings of his provocative aphorisms, we have a caricature of the â€œovermanâ€ as a kind of nihilistic lover of conquest who is above all ethics. But even a cursory reading of Nietzsche reveals a different conception. Even in his most â€œpolemicalâ€ treatments of morality, as Nietzsche calls them, he makes clear that he seeks to promote one ethical framework over another, not to reject ethics in favor of â€œmight makes right.â€ He seeks to replace a morality of weakness, timidity, and resentment with a morality of self-command. A first approximation of what Nietzsche means by â€œovermanâ€ is the passionate pursuit of life as an unfolding story: â€œWhat is great in man is that he is a bridge and not an end.â€2Â â€œOvermanâ€ is meant to point away from the â€œend,â€ the goal, the accomplishment, and toward the way. Life, Nietzsche suggests, is a ceaseless process of self-overcoming. â€œOvermanâ€ can thus be seen as a corrective to our â€œresults-orientedâ€ culture that interprets power as the capacity to get things done, to attain a certain standard of living, and to â€œmake the world a better place.â€ To this extent, Nietzscheâ€™s notion of overman is radiÂ­cal and perhaps unsettling. Nietzsche sees in our obsession with accomplishment and progress a certain folly. He takes aim at the EnlightÂ­enment view of providence that comes to paradigmatic exÂ­pression in Marx: the notion that humanity, through the technological mastery of nature, can solve the problem of scarcity and, through the attainment of a universal consciousness free of class difference and exploitation, bring about the perfectly just world.  found in r/okbuddychicanery\n",
      "subreddit: copypasta\n",
      "\n",
      "title: How to find an accidentally hucked weapon?\n",
      "self_text: So last night I was building an outpost on the pavers bordering the upper yard. I went to relocate one of the storage chests when I accidentally hit the very ridiculously defaulted key to throw a weapon. It was a level 9 toenail scimitar. Sadly I was facing the koi pond and the thing flew off the edge of the paver and down into oblivion.   I spent over 45 minutes scouring the pond surface and depths where I estimated the weapon would have gone but to no avail. As I have upgraded my other weapons to high levels I donâ€™t have any supreme whetstones left to get another one back to level 9.   I found that weapons sink in water but all searches came up empty. I know that dropped or thrown weapons have a proximity icon when you get close to them but that doesnâ€™t always work. Why it doesnâ€™t work is beyond me and clearly is not working as intended as Iâ€™ve lost arrows and other hucked items in the past but miraculously picked them back up by running over where they just happened to land but a weapon like this may as well be a needle in a haystack to find in the koi pond provided thatâ€™s where it is in the first place.   Will this weapon eventually despawn or will it be somewhere in the world waiting for me to find it by chance?  Hereâ€™s an idea for the devs: Make an item that was thrown ALWAYS show up on the HUD regardless of how far you are from it!!!!  While I know that you can remap the keys I would love to know who in the hell thought it was a great idea to default the huck key to R???\n",
      "subreddit: GroundedGame\n",
      "\n",
      "title: Opinions needed\n",
      "self_text: My friends and family have time for me on spare or unimportant days.  When it comes to important days, like holidays and stuff, they spend it elsewhere.  Am I right to be saddened by this?\n",
      "subreddit: ucla\n",
      "\n",
      "title: Viola parry idea\n",
      "self_text: Since we all know violas parry isn't great and moon of MK is better wouldn't combining that with evil harvest rosary be a good idea, then it doesn't matter how long you parry for, you still get a biot of damage off and can use moon for actual long witch times.\n",
      "subreddit: Bayonetta\n",
      "\n",
      "title: Idea that might be crazy\n",
      "self_text: Challenge for anyone who is crazy enough to do this, but I want some one to make a one-by-one recreation of Mogswamp's 10 year Superflat world. Might take a long time, but I want to see it done.\n",
      "subreddit: Minecraft\n",
      "\n",
      "title: Tutorial for CHEM 153B\n",
      "self_text: hello yâ€™all :) for anyone who has taken CHEM 153B what do you guys normally do during tutorial? I have two other classes that conflict with my tutorial time and Iâ€™m planning on not going if I donâ€™t really need to\n",
      "subreddit: ucla\n",
      "\n",
      "title: GEOG 7 assignment 9\n",
      "self_text: Who has taken GEOG 7 and wants to share with me their assignment 9? Currently struggling and since the TA strike is still happening there are no office hours I can go to :(\n",
      "subreddit: ucla\n",
      "\n",
      "title: No Golden Egg?\n",
      "self_text: The event says I collected 2 golden eggs but I don't see them and can't open them from my inventory. My brother got one and it's in his inventory. Anyone have any ideas?\n",
      "subreddit: RocketLeague\n",
      "\n",
      "title: There is something wrong if GoWR do not win GotY award.\n",
      "self_text: I have watched behind the scenes clips, I have played this game, I have cried and I have been excited.  The amount of work and love that has been put down into this game is on a whole new level. The voice acting, the combat, the details, the amount of content, the story telling, the acting, the level design, the soundtrack, the accessibility options,  it is all amazing.  Me personally I dont like Elden Ring but I get why people like it.  But I strongly disagree that the same amount of love and care and professional work was put down into Elden Ring as they did with God of War. That is why God of War should win game of the year, end of story.  My personal opinion is that IF Elden Ring wins it is NOT because it deserves it, it is because it has a wider range of players. It is because X-Box players and PC players also voted for it and that is not fair.  So if you are an Elden Ring fanboy, I am sorry if this post hurts your feelings but I think that the best work done on a game should win the prize and at this moment God of War is by far the superior here.\n",
      "subreddit: playstation\n",
      "\n",
      "title: Will academic probation affect my chances of being admitted as a transfer?\n",
      "self_text: Hello! Im filling out my application for transfer and I got a bit worried when it asked about academic probation. My first semester of community College in 2018, I was put on academic probation. But I got back into \"good standings\" the next semester and was never on probation again after that. I got the deans honors too. I have a good GPA and my counselor said my personal insights are very strong, but will the academic probation from my first semester ruin my chances? ðŸ˜­\n",
      "subreddit: ucla\n",
      "\n",
      "title: Every Single Emoji Separated by Comma - Youtube Ban List\n",
      "self_text: The following can be copy pasted into blocked words lists to help remove bots and twitch users from your YouTube channels and keep your audience safe from those dangers.  ðŸ˜€, ðŸ˜, ðŸ˜‚, ðŸ¤£, ðŸ˜ƒ, ðŸ˜„, ðŸ˜…, ðŸ˜†, ðŸ˜‰, ðŸ˜Š, ðŸ˜‹, ðŸ˜Ž, ðŸ˜, ðŸ˜˜, ðŸ¥°, ðŸ˜—, ðŸ˜™, ðŸ˜š, â˜ºï¸, ðŸ™‚, ðŸ¤—, ðŸ¤©, ðŸ¤”, ðŸ¤¨, ðŸ˜, ðŸ˜‘, ðŸ˜¶, ðŸ™„, ðŸ˜, ðŸ˜£, ðŸ˜¥, ðŸ˜®, ðŸ¤, ðŸ˜¯, ðŸ˜ª, ðŸ˜«, ðŸ˜´, ðŸ˜Œ, ðŸ˜›, ðŸ˜œ, ðŸ˜, ðŸ¤¤, ðŸ˜’, ðŸ˜“, ðŸ˜”, ðŸ˜•, ðŸ™ƒ, ðŸ¤‘, ðŸ˜², â˜¹ï¸, ðŸ™, ðŸ˜–, ðŸ˜ž, ðŸ˜Ÿ, ðŸ˜¤, ðŸ˜¢, ðŸ˜­, ðŸ˜¦, ðŸ˜§, ðŸ˜¨, ðŸ˜©, ðŸ¤¯, ðŸ˜¬, ðŸ˜°, ðŸ˜±, ðŸ¥µ, ðŸ¥¶, ðŸ˜³, ðŸ¤ª, ðŸ˜µ, ðŸ˜¡, ðŸ˜ , ðŸ¤¬, ðŸ˜·, ðŸ¤’, ðŸ¤•, ðŸ¤¢, ðŸ¤®, ðŸ¤§, ðŸ˜‡, ðŸ¤ , ðŸ¤¡, ðŸ¥³, ðŸ¥´, ðŸ¥º, ðŸ¤¥, ðŸ¤«, ðŸ¤­, ðŸ§, ðŸ¤“, ðŸ˜ˆ, ðŸ‘¿, ðŸ‘¹, ðŸ‘º, ðŸ’€, ðŸ‘», ðŸ‘½, ðŸ¤–, ðŸ’©, ðŸ˜º, ðŸ˜¸, ðŸ˜¹, ðŸ˜», ðŸ˜¼, ðŸ˜½, ðŸ™€, ðŸ˜¿, ðŸ˜¾, ðŸ‘¶, ðŸ‘§, ðŸ§’, ðŸ‘¦, ðŸ‘©, ðŸ§‘, ðŸ‘¨, ðŸ‘µ, ðŸ§“, ðŸ‘´, ðŸ‘², ðŸ‘³â€â™€ï¸, ðŸ‘³â€â™‚ï¸, ðŸ§•, ðŸ§”, ðŸ‘±â€â™‚ï¸, ðŸ‘±â€â™€ï¸, ðŸ‘¨â€ðŸ¦°, ðŸ‘©â€ðŸ¦°, ðŸ‘¨â€ðŸ¦±, ðŸ‘©â€ðŸ¦±, ðŸ‘¨â€ðŸ¦², ðŸ‘©â€ðŸ¦², ðŸ‘¨â€ðŸ¦³, ðŸ‘©â€ðŸ¦³, ðŸ¦¸â€â™€ï¸, ðŸ¦¸â€â™‚ï¸, ðŸ¦¹â€â™€ï¸, ðŸ¦¹â€â™‚ï¸, ðŸ‘®â€â™€ï¸, ðŸ‘®â€â™‚ï¸, ðŸ‘·â€â™€ï¸, ðŸ‘·â€â™‚ï¸, ðŸ’‚â€â™€ï¸, ðŸ’‚â€â™‚ï¸, ðŸ•µï¸â€â™€ï¸, ðŸ•µï¸â€â™‚ï¸, ðŸ‘©â€âš•ï¸, ðŸ‘¨â€âš•ï¸, ðŸ‘©â€ðŸŒ¾, ðŸ‘¨â€ðŸŒ¾, ðŸ‘©â€ðŸ³, ðŸ‘¨â€ðŸ³, ðŸ‘©â€ðŸŽ“, ðŸ‘¨â€ðŸŽ“, ðŸ‘©â€ðŸŽ¤, ðŸ‘¨â€ðŸŽ¤, ðŸ‘©â€ðŸ«, ðŸ‘¨â€ðŸ«, ðŸ‘©â€ðŸ­, ðŸ‘¨â€ðŸ­, ðŸ‘©â€ðŸ’», ðŸ‘¨â€ðŸ’», ðŸ‘©â€ðŸ’¼, ðŸ‘¨â€ðŸ’¼, ðŸ‘©â€ðŸ”§, ðŸ‘¨â€ðŸ”§, ðŸ‘©â€ðŸ”¬, ðŸ‘¨â€ðŸ”¬, ðŸ‘©â€ðŸŽ¨, ðŸ‘¨â€ðŸŽ¨, ðŸ‘©â€ðŸš’, ðŸ‘¨â€ðŸš’, ðŸ‘©â€âœˆï¸, ðŸ‘¨â€âœˆï¸, ðŸ‘©â€ðŸš€, ðŸ‘¨â€ðŸš€, ðŸ‘©â€âš–ï¸, ðŸ‘¨â€âš–ï¸, ðŸ‘°, ðŸ¤µ, ðŸ‘¸, ðŸ¤´, ðŸ¤¶, ðŸŽ…, ðŸ§™â€â™€ï¸, ðŸ§™â€â™‚ï¸, ðŸ§â€â™€ï¸, ðŸ§â€â™‚ï¸, ðŸ§›â€â™€ï¸, ðŸ§›â€â™‚ï¸, ðŸ§Ÿâ€â™€ï¸, ðŸ§Ÿâ€â™‚ï¸, ðŸ§žâ€â™€ï¸, ðŸ§žâ€â™‚ï¸, ðŸ§œâ€â™€ï¸, ðŸ§œâ€â™‚ï¸, ðŸ§šâ€â™€ï¸, ðŸ§šâ€â™‚ï¸, ðŸ‘¼, ðŸ¤°, ðŸ¤±, ðŸ™‡â€â™€ï¸, ðŸ™‡â€â™‚ï¸, ðŸ’â€â™€ï¸, ðŸ’â€â™‚ï¸, ðŸ™…â€â™€ï¸, ðŸ™…â€â™‚ï¸, ðŸ™†â€â™€ï¸, ðŸ™†â€â™‚ï¸, ðŸ™‹â€â™€ï¸, ðŸ™‹â€â™‚ï¸, ðŸ¤¦â€â™€ï¸, ðŸ¤¦â€â™‚ï¸, ðŸ¤·â€â™€ï¸, ðŸ¤·â€â™‚ï¸, ðŸ™Žâ€â™€ï¸, ðŸ™Žâ€â™‚ï¸, ðŸ™â€â™€ï¸, ðŸ™â€â™‚ï¸, ðŸ’‡â€â™€ï¸, ðŸ’‡â€â™‚ï¸, ðŸ’†â€â™€ï¸, ðŸ’†â€â™‚ï¸, ðŸ§–â€â™€ï¸, ðŸ§–â€â™‚ï¸, ðŸ’…, ðŸ¤³, ðŸ’ƒ, ðŸ•º, ðŸ‘¯â€â™€ï¸, ðŸ‘¯â€â™‚ï¸, ðŸ•´, ðŸš¶â€â™€ï¸, ðŸš¶â€â™‚ï¸, ðŸƒâ€â™€ï¸, ðŸƒâ€â™‚ï¸, ðŸ‘«, ðŸ‘­, ðŸ‘¬, ðŸ’‘, ðŸ‘©â€â¤ï¸â€ðŸ‘©, ðŸ‘¨â€â¤ï¸â€ðŸ‘¨, ðŸ’, ðŸ‘©â€â¤ï¸â€ðŸ’‹â€ðŸ‘©, ðŸ‘¨â€â¤ï¸â€ðŸ’‹â€ðŸ‘¨, ðŸ‘ª, ðŸ‘¨â€ðŸ‘©â€ðŸ‘§, ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦, ðŸ‘¨â€ðŸ‘©â€ðŸ‘¦â€ðŸ‘¦, ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘§, ðŸ‘©â€ðŸ‘©â€ðŸ‘¦, ðŸ‘©â€ðŸ‘©â€ðŸ‘§, ðŸ‘©â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦, ðŸ‘©â€ðŸ‘©â€ðŸ‘¦â€ðŸ‘¦, ðŸ‘©â€ðŸ‘©â€ðŸ‘§â€ðŸ‘§, ðŸ‘¨â€ðŸ‘¨â€ðŸ‘¦, ðŸ‘¨â€ðŸ‘¨â€ðŸ‘§, ðŸ‘¨â€ðŸ‘¨â€ðŸ‘§â€ðŸ‘¦, ðŸ‘¨â€ðŸ‘¨â€ðŸ‘¦â€ðŸ‘¦, ðŸ‘¨â€ðŸ‘¨â€ðŸ‘§â€ðŸ‘§, ðŸ‘©â€ðŸ‘¦, ðŸ‘©â€ðŸ‘§, ðŸ‘©â€ðŸ‘§â€ðŸ‘¦, ðŸ‘©â€ðŸ‘¦â€ðŸ‘¦, ðŸ‘©â€ðŸ‘§â€ðŸ‘§, ðŸ‘¨â€ðŸ‘¦, ðŸ‘¨â€ðŸ‘§, ðŸ‘¨â€ðŸ‘§â€ðŸ‘¦, ðŸ‘¨â€ðŸ‘¦â€ðŸ‘¦, ðŸ‘¨â€ðŸ‘§â€ðŸ‘§, ðŸ¤², ðŸ‘, ðŸ™Œ, ðŸ‘, ðŸ¤, ðŸ‘, ðŸ‘Ž, ðŸ‘Š, âœŠ, ðŸ¤›, ðŸ¤œ, ðŸ¤ž, âœŒï¸, ðŸ¤Ÿ, ðŸ¤˜, ðŸ‘Œ, ðŸ‘ˆ, ðŸ‘‰, ðŸ‘†, ðŸ‘‡, â˜ï¸, âœ‹, ðŸ¤š, ðŸ–, ðŸ––, ðŸ‘‹, ðŸ¤™, ðŸ’ª, ðŸ¦µ, ðŸ¦¶, ðŸ–•, âœï¸, ðŸ™, ðŸ’, ðŸ’„, ðŸ’‹, ðŸ‘„, ðŸ‘…, ðŸ‘‚, ðŸ‘ƒ, ðŸ‘£, ðŸ‘, ðŸ‘€, ðŸ§ , ðŸ¦´, ðŸ¦·, ðŸ—£, ðŸ‘¤, ðŸ‘¥, ðŸ§¥, ðŸ‘š, ðŸ‘•, ðŸ‘–, ðŸ‘”, ðŸ‘—, ðŸ‘™, ðŸ‘˜, ðŸ‘ , ðŸ‘¡, ðŸ‘¢, ðŸ‘ž, ðŸ‘Ÿ, ðŸ¥¾, ðŸ¥¿, ðŸ§¦, ðŸ§¤, ðŸ§£, ðŸŽ©, ðŸ§¢, ðŸ‘’, ðŸŽ“, â›‘, ðŸ‘‘, ðŸ‘, ðŸ‘›, ðŸ‘œ, ðŸ’¼, ðŸŽ’, ðŸ‘“, ðŸ•¶, ðŸ¥½, ðŸ¥¼, ðŸŒ‚, ðŸ§µ, ðŸ§¶, ðŸ‘¶ðŸ», ðŸ‘¦ðŸ», ðŸ‘§ðŸ», ðŸ‘¨ðŸ», ðŸ‘©ðŸ», ðŸ‘±ðŸ»â€â™€ï¸, ðŸ‘±ðŸ», ðŸ‘´ðŸ», ðŸ‘µðŸ», ðŸ‘²ðŸ», ðŸ‘³ðŸ»â€â™€ï¸, ðŸ‘³ðŸ», ðŸ‘®ðŸ»â€â™€ï¸, ðŸ‘®ðŸ», ðŸ‘·ðŸ»â€â™€ï¸, ðŸ‘·ðŸ», ðŸ’‚ðŸ»â€â™€ï¸, ðŸ’‚ðŸ», ðŸ•µðŸ»â€â™€ï¸, ðŸ•µðŸ», ðŸ‘©ðŸ»â€âš•ï¸, ðŸ‘¨ðŸ»â€âš•ï¸, ðŸ‘©ðŸ»â€ðŸŒ¾, ðŸ‘¨ðŸ»â€ðŸŒ¾, ðŸ‘©ðŸ»â€ðŸ³, ðŸ‘¨ðŸ»â€ðŸ³, ðŸ‘©ðŸ»â€ðŸŽ“, ðŸ‘¨ðŸ»â€ðŸŽ“, ðŸ‘©ðŸ»â€ðŸŽ¤, ðŸ‘¨ðŸ»â€ðŸŽ¤, ðŸ‘©ðŸ»â€ðŸ«, ðŸ‘¨ðŸ»â€ðŸ«, ðŸ‘©ðŸ»â€ðŸ­, ðŸ‘¨ðŸ»â€ðŸ­, ðŸ‘©ðŸ»â€ðŸ’», ðŸ‘¨ðŸ»â€ðŸ’», ðŸ‘©ðŸ»â€ðŸ’¼, ðŸ‘¨ðŸ»â€ðŸ’¼, ðŸ‘©ðŸ»â€ðŸ”§, ðŸ‘¨ðŸ»â€ðŸ”§, ðŸ‘©ðŸ»â€ðŸ”¬, ðŸ‘¨ðŸ»â€ðŸ”¬, ðŸ‘©ðŸ»â€ðŸŽ¨, ðŸ‘¨ðŸ»â€ðŸŽ¨, ðŸ‘©ðŸ»â€ðŸš’, ðŸ‘¨ðŸ»â€ðŸš’, ðŸ‘©ðŸ»â€âœˆï¸, ðŸ‘¨ðŸ»â€âœˆï¸, ðŸ‘©ðŸ»â€ðŸš€, ðŸ‘¨ðŸ»â€ðŸš€, ðŸ‘©ðŸ»â€âš–ï¸, ðŸ‘¨ðŸ»â€âš–ï¸, ðŸ¤¶ðŸ», ðŸŽ…ðŸ», ðŸ‘¸ðŸ», ðŸ¤´ðŸ», ðŸ‘°ðŸ», ðŸ¤µðŸ», ðŸ‘¼ðŸ», ðŸ¤°ðŸ», ðŸ™‡ðŸ»â€â™€ï¸, ðŸ™‡ðŸ», ðŸ’ðŸ», ðŸ’ðŸ»â€â™‚ï¸, ðŸ™…ðŸ», ðŸ™…ðŸ»â€â™‚ï¸, ðŸ™†ðŸ», ðŸ™†ðŸ»â€â™‚ï¸, ðŸ™‹ðŸ», ðŸ™‹ðŸ»â€â™‚ï¸, ðŸ¤¦ðŸ»â€â™€ï¸, ðŸ¤¦ðŸ»â€â™‚ï¸, ðŸ¤·ðŸ»â€â™€ï¸, ðŸ¤·ðŸ»â€â™‚ï¸, ðŸ™ŽðŸ», ðŸ™ŽðŸ»â€â™‚ï¸, ðŸ™ðŸ», ðŸ™ðŸ»â€â™‚ï¸, ðŸ’‡ðŸ», ðŸ’‡ðŸ»â€â™‚ï¸, ðŸ’†ðŸ», ðŸ’†ðŸ»â€â™‚ï¸, ðŸ•´ðŸ», ðŸ’ƒðŸ», ðŸ•ºðŸ», ðŸš¶ðŸ»â€â™€ï¸, ðŸš¶ðŸ», ðŸƒðŸ»â€â™€ï¸, ðŸƒðŸ», ðŸ¤²ðŸ», ðŸ‘ðŸ», ðŸ™ŒðŸ», ðŸ‘ðŸ», ðŸ™ðŸ», ðŸ‘ðŸ», ðŸ‘ŽðŸ», ðŸ‘ŠðŸ», âœŠðŸ», ðŸ¤›ðŸ», ðŸ¤œðŸ», ðŸ¤žðŸ», âœŒðŸ», ðŸ¤ŸðŸ», ðŸ¤˜ðŸ», ðŸ‘ŒðŸ», ðŸ‘ˆðŸ», ðŸ‘‰ðŸ», ðŸ‘†ðŸ», ðŸ‘‡ðŸ», â˜ðŸ», âœ‹ðŸ», ðŸ¤šðŸ», ðŸ–ðŸ», ðŸ––ðŸ», ðŸ‘‹ðŸ», ðŸ¤™ðŸ», ðŸ’ªðŸ», ðŸ–•ðŸ», âœðŸ», ðŸ¤³ðŸ», ðŸ’…ðŸ», ðŸ‘‚ðŸ», ðŸ‘ƒðŸ», ðŸ‘¶ðŸ¼, ðŸ‘¦ðŸ¼, ðŸ‘§ðŸ¼, ðŸ‘¨ðŸ¼, ðŸ‘©ðŸ¼, ðŸ‘±ðŸ¼â€â™€ï¸, ðŸ‘±ðŸ¼, ðŸ‘´ðŸ¼, ðŸ‘µðŸ¼, ðŸ‘²ðŸ¼, ðŸ‘³ðŸ¼â€â™€ï¸, ðŸ‘³ðŸ¼, ðŸ‘®ðŸ¼â€â™€ï¸, ðŸ‘®ðŸ¼, ðŸ‘·ðŸ¼â€â™€ï¸, ðŸ‘·ðŸ¼, ðŸ’‚ðŸ¼â€â™€ï¸, ðŸ’‚ðŸ¼, ðŸ•µðŸ¼â€â™€ï¸, ðŸ•µðŸ¼, ðŸ‘©ðŸ¼â€âš•ï¸, ðŸ‘¨ðŸ¼â€âš•ï¸, ðŸ‘©ðŸ¼â€ðŸŒ¾, ðŸ‘¨ðŸ¼â€ðŸŒ¾, ðŸ‘©ðŸ¼â€ðŸ³, ðŸ‘¨ðŸ¼â€ðŸ³, ðŸ‘©ðŸ¼â€ðŸŽ“, ðŸ‘¨ðŸ¼â€ðŸŽ“, ðŸ‘©ðŸ¼â€ðŸŽ¤, ðŸ‘¨ðŸ¼â€ðŸŽ¤, ðŸ‘©ðŸ¼â€ðŸ«, ðŸ‘¨ðŸ¼â€ðŸ«, ðŸ‘©ðŸ¼â€ðŸ­, ðŸ‘¨ðŸ¼â€ðŸ­, ðŸ‘©ðŸ¼â€ðŸ’», ðŸ‘¨ðŸ¼â€ðŸ’», ðŸ‘©ðŸ¼â€ðŸ’¼, ðŸ‘¨ðŸ¼â€ðŸ’¼, ðŸ‘©ðŸ¼â€ðŸ”§, ðŸ‘¨ðŸ¼â€ðŸ”§, ðŸ‘©ðŸ¼â€ðŸ”¬, ðŸ‘¨ðŸ¼â€ðŸ”¬, ðŸ‘©ðŸ¼â€ðŸŽ¨, ðŸ‘¨ðŸ¼â€ðŸŽ¨, ðŸ‘©ðŸ¼â€ðŸš’, ðŸ‘¨ðŸ¼â€ðŸš’, ðŸ‘©ðŸ¼â€âœˆï¸, ðŸ‘¨ðŸ¼â€âœˆï¸, ðŸ‘©ðŸ¼â€ðŸš€, ðŸ‘¨ðŸ¼â€ðŸš€, ðŸ‘©ðŸ¼â€âš–ï¸, ðŸ‘¨ðŸ¼â€âš–ï¸, ðŸ¤¶ðŸ¼, ðŸŽ…ðŸ¼, ðŸ‘¸ðŸ¼, ðŸ¤´ðŸ¼, ðŸ‘°ðŸ¼, ðŸ¤µðŸ¼, ðŸ‘¼ðŸ¼, ðŸ¤°ðŸ¼, ðŸ™‡ðŸ¼â€â™€ï¸, ðŸ™‡ðŸ¼, ðŸ’ðŸ¼, ðŸ’ðŸ¼â€â™‚ï¸, ðŸ™…ðŸ¼, ðŸ™…ðŸ¼â€â™‚ï¸, ðŸ™†ðŸ¼, ðŸ™†ðŸ¼â€â™‚ï¸, ðŸ™‹ðŸ¼, ðŸ™‹ðŸ¼â€â™‚ï¸, ðŸ¤¦ðŸ¼â€â™€ï¸, ðŸ¤¦ðŸ¼â€â™‚ï¸, ðŸ¤·ðŸ¼â€â™€ï¸, ðŸ¤·ðŸ¼â€â™‚ï¸, ðŸ™ŽðŸ¼, ðŸ™ŽðŸ¼â€â™‚ï¸, ðŸ™ðŸ¼, ðŸ™ðŸ¼â€â™‚ï¸, ðŸ’‡ðŸ¼, ðŸ’‡ðŸ¼â€â™‚ï¸, ðŸ’†ðŸ¼, ðŸ’†ðŸ¼â€â™‚ï¸, ðŸ•´ðŸ¼, ðŸ’ƒðŸ¼, ðŸ•ºðŸ¼, ðŸš¶ðŸ¼â€â™€ï¸, ðŸš¶ðŸ¼, ðŸƒðŸ¼â€â™€ï¸, ðŸƒðŸ¼, ðŸ¤²ðŸ¼, ðŸ‘ðŸ¼, ðŸ™ŒðŸ¼, ðŸ‘ðŸ¼, ðŸ™ðŸ¼, ðŸ‘ðŸ¼, ðŸ‘ŽðŸ¼, ðŸ‘ŠðŸ¼, âœŠðŸ¼, ðŸ¤›ðŸ¼, ðŸ¤œðŸ¼, ðŸ¤žðŸ¼, âœŒðŸ¼, ðŸ¤ŸðŸ¼, ðŸ¤˜ðŸ¼, ðŸ‘ŒðŸ¼, ðŸ‘ˆðŸ¼, ðŸ‘‰ðŸ¼, ðŸ‘†ðŸ¼, ðŸ‘‡ðŸ¼, â˜ðŸ¼, âœ‹ðŸ¼, ðŸ¤šðŸ¼, ðŸ–ðŸ¼, ðŸ––ðŸ¼, ðŸ‘‹ðŸ¼, ðŸ¤™ðŸ¼, ðŸ’ªðŸ¼, ðŸ–•ðŸ¼, âœðŸ¼, ðŸ¤³ðŸ¼, ðŸ’…ðŸ¼, ðŸ‘‚ðŸ¼, ðŸ‘ƒðŸ¼, ðŸ‘¶ðŸ½, ðŸ‘¦ðŸ½, ðŸ‘§ðŸ½, ðŸ‘¨ðŸ½, ðŸ‘©ðŸ½, ðŸ‘±ðŸ½â€â™€ï¸, ðŸ‘±ðŸ½, ðŸ‘´ðŸ½, ðŸ‘µðŸ½, ðŸ‘²ðŸ½, ðŸ‘³ðŸ½â€â™€ï¸, ðŸ‘³ðŸ½, ðŸ‘®ðŸ½â€â™€ï¸, ðŸ‘®ðŸ½, ðŸ‘·ðŸ½â€â™€ï¸, ðŸ‘·ðŸ½, ðŸ’‚ðŸ½â€â™€ï¸, ðŸ’‚ðŸ½, ðŸ•µðŸ½â€â™€ï¸, ðŸ•µðŸ½, ðŸ‘©ðŸ½â€âš•ï¸, ðŸ‘¨ðŸ½â€âš•ï¸, ðŸ‘©ðŸ½â€ðŸŒ¾, ðŸ‘¨ðŸ½â€ðŸŒ¾, ðŸ‘©ðŸ½â€ðŸ³, ðŸ‘¨ðŸ½â€ðŸ³, ðŸ‘©ðŸ½â€ðŸŽ“, ðŸ‘¨ðŸ½â€ðŸŽ“, ðŸ‘©ðŸ½â€ðŸŽ¤, ðŸ‘¨ðŸ½â€ðŸŽ¤, ðŸ‘©ðŸ½â€ðŸ«, ðŸ‘¨ðŸ½â€ðŸ«, ðŸ‘©ðŸ½â€ðŸ­, ðŸ‘¨ðŸ½â€ðŸ­, ðŸ‘©ðŸ½â€ðŸ’», ðŸ‘¨ðŸ½â€ðŸ’», ðŸ‘©ðŸ½â€ðŸ’¼, ðŸ‘¨ðŸ½â€ðŸ’¼, ðŸ‘©ðŸ½â€ðŸ”§, ðŸ‘¨ðŸ½â€ðŸ”§, ðŸ‘©ðŸ½â€ðŸ”¬, ðŸ‘¨ðŸ½â€ðŸ”¬, ðŸ‘©ðŸ½â€ðŸŽ¨, ðŸ‘¨ðŸ½â€ðŸŽ¨, ðŸ‘©ðŸ½â€ðŸš’, ðŸ‘¨ðŸ½â€ðŸš’, ðŸ‘©ðŸ½â€âœˆï¸, ðŸ‘¨ðŸ½â€âœˆï¸, ðŸ‘©ðŸ½â€ðŸš€, ðŸ‘¨ðŸ½â€ðŸš€, ðŸ‘©ðŸ½â€âš–ï¸, ðŸ‘¨ðŸ½â€âš–ï¸, ðŸ¤¶ðŸ½, ðŸŽ…ðŸ½, ðŸ‘¸ðŸ½, ðŸ¤´ðŸ½, ðŸ‘°ðŸ½, ðŸ¤µðŸ½, ðŸ‘¼ðŸ½, ðŸ¤°ðŸ½, ðŸ™‡ðŸ½â€â™€ï¸, ðŸ™‡ðŸ½, ðŸ’ðŸ½, ðŸ’ðŸ½â€â™‚ï¸, ðŸ™…ðŸ½, ðŸ™…ðŸ½â€â™‚ï¸, ðŸ™†ðŸ½, ðŸ™†ðŸ½â€â™‚ï¸, ðŸ™‹ðŸ½, ðŸ™‹ðŸ½â€â™‚ï¸, ðŸ¤¦ðŸ½â€â™€ï¸, ðŸ¤¦ðŸ½â€â™‚ï¸, ðŸ¤·ðŸ½â€â™€ï¸, ðŸ¤·ðŸ½â€â™‚ï¸, ðŸ™ŽðŸ½, ðŸ™ŽðŸ½â€â™‚ï¸, ðŸ™ðŸ½, ðŸ™ðŸ½â€â™‚ï¸, ðŸ’‡ðŸ½, ðŸ’‡ðŸ½â€â™‚ï¸, ðŸ’†ðŸ½, ðŸ’†ðŸ½â€â™‚ï¸, ðŸ•´ðŸ¼, ðŸ’ƒðŸ½, ðŸ•ºðŸ½, ðŸš¶ðŸ½â€â™€ï¸, ðŸš¶ðŸ½, ðŸƒðŸ½â€â™€ï¸, ðŸƒðŸ½, ðŸ¤²ðŸ½, ðŸ‘ðŸ½, ðŸ™ŒðŸ½, ðŸ‘ðŸ½, ðŸ™ðŸ½, ðŸ‘ðŸ½, ðŸ‘ŽðŸ½, ðŸ‘ŠðŸ½, âœŠðŸ½, ðŸ¤›ðŸ½, ðŸ¤œðŸ½, ðŸ¤žðŸ½, âœŒðŸ½, ðŸ¤ŸðŸ½, ðŸ¤˜ðŸ½, ðŸ‘ŒðŸ½, ðŸ‘ˆðŸ½, ðŸ‘‰ðŸ½, ðŸ‘†ðŸ½, ðŸ‘‡ðŸ½, â˜ðŸ½, âœ‹ðŸ½, ðŸ¤šðŸ½, ðŸ–ðŸ½, ðŸ––ðŸ½, ðŸ‘‹ðŸ½, ðŸ¤™ðŸ½, ðŸ’ªðŸ½, ðŸ–•ðŸ½, âœðŸ½, ðŸ¤³ðŸ½, ðŸ’…ðŸ½, ðŸ‘‚ðŸ½, ðŸ‘ƒðŸ½, ðŸ‘¶ðŸ¾, ðŸ‘¦ðŸ¾, ðŸ‘§ðŸ¾, ðŸ‘¨ðŸ¾, ðŸ‘©ðŸ¾, ðŸ‘±ðŸ¾â€â™€ï¸, ðŸ‘±ðŸ¾, ðŸ‘´ðŸ¾, ðŸ‘µðŸ¾, ðŸ‘²ðŸ¾, ðŸ‘³ðŸ¾â€â™€ï¸, ðŸ‘³ðŸ¾, ðŸ‘®ðŸ¾â€â™€ï¸, ðŸ‘®ðŸ¾, ðŸ‘·ðŸ¾â€â™€ï¸, ðŸ‘·ðŸ¾, ðŸ’‚ðŸ¾â€â™€ï¸, ðŸ’‚ðŸ¾, ðŸ•µðŸ¾â€â™€ï¸, ðŸ•µðŸ¾, ðŸ‘©ðŸ¾â€âš•ï¸, ðŸ‘¨ðŸ¾â€âš•ï¸, ðŸ‘©ðŸ¾â€ðŸŒ¾, ðŸ‘¨ðŸ¾â€ðŸŒ¾, ðŸ‘©ðŸ¾â€ðŸ³, ðŸ‘¨ðŸ¾â€ðŸ³, ðŸ‘©ðŸ¾â€ðŸŽ“, ðŸ‘¨ðŸ¾â€ðŸŽ“, ðŸ‘©ðŸ¾â€ðŸŽ¤, ðŸ‘¨ðŸ¾â€ðŸŽ¤, ðŸ‘©ðŸ¾â€ðŸ«, ðŸ‘¨ðŸ¾â€ðŸ«, ðŸ‘©ðŸ¾â€ðŸ­, ðŸ‘¨ðŸ¾â€ðŸ­, ðŸ‘©ðŸ¾â€ðŸ’», ðŸ‘¨ðŸ¾â€ðŸ’», ðŸ‘©ðŸ¾â€ðŸ’¼, ðŸ‘¨ðŸ¾â€ðŸ’¼, ðŸ‘©ðŸ¾â€ðŸ”§, ðŸ‘¨ðŸ¾â€ðŸ”§, ðŸ‘©ðŸ¾â€ðŸ”¬, ðŸ‘¨ðŸ¾â€ðŸ”¬, ðŸ‘©ðŸ¾â€ðŸŽ¨, ðŸ‘¨ðŸ¾â€ðŸŽ¨, ðŸ‘©ðŸ¾â€ðŸš’, ðŸ‘¨ðŸ¾â€ðŸš’, ðŸ‘©ðŸ¾â€âœˆï¸, ðŸ‘¨ðŸ¾â€âœˆï¸, ðŸ‘©ðŸ¾â€ðŸš€, ðŸ‘¨ðŸ¾â€ðŸš€, ðŸ‘©ðŸ¾â€âš–ï¸, ðŸ‘¨ðŸ¾â€âš–ï¸, ðŸ¤¶ðŸ¾, ðŸŽ…ðŸ¾, ðŸ‘¸ðŸ¾, ðŸ¤´ðŸ¾, ðŸ‘°ðŸ¾, ðŸ¤µðŸ¾, ðŸ‘¼ðŸ¾, ðŸ¤°ðŸ¾, ðŸ™‡ðŸ¾â€â™€ï¸, ðŸ™‡ðŸ¾, ðŸ’ðŸ¾, ðŸ’ðŸ¾â€â™‚ï¸, ðŸ™…ðŸ¾, ðŸ™…ðŸ¾â€â™‚ï¸, ðŸ™†ðŸ¾, ðŸ™†ðŸ¾â€â™‚ï¸, ðŸ™‹ðŸ¾, ðŸ™‹ðŸ¾â€â™‚ï¸, ðŸ¤¦ðŸ¾â€â™€ï¸, ðŸ¤¦ðŸ¾â€â™‚ï¸, ðŸ¤·ðŸ¾â€â™€ï¸, ðŸ¤·ðŸ¾â€â™‚ï¸, ðŸ™ŽðŸ¾, ðŸ™ŽðŸ¾â€â™‚ï¸, ðŸ™ðŸ¾, ðŸ™ðŸ¾â€â™‚ï¸, ðŸ’‡ðŸ¾, ðŸ’‡ðŸ¾â€â™‚ï¸, ðŸ’†ðŸ¾, ðŸ’†ðŸ¾â€â™‚ï¸, ðŸ•´ðŸ¾, ðŸ’ƒðŸ¾, ðŸ•ºðŸ¾, ðŸš¶ðŸ¾â€â™€ï¸, ðŸš¶ðŸ¾, ðŸƒðŸ¾â€â™€ï¸, ðŸƒðŸ¾, ðŸ¤²ðŸ¾, ðŸ‘ðŸ¾, ðŸ™ŒðŸ¾, ðŸ‘ðŸ¾, ðŸ™ðŸ¾, ðŸ‘ðŸ¾, ðŸ‘ŽðŸ¾, ðŸ‘ŠðŸ¾, âœŠðŸ¾, ðŸ¤›ðŸ¾, ðŸ¤œðŸ¾, ðŸ¤žðŸ¾, âœŒðŸ¾, ðŸ¤ŸðŸ¾, ðŸ¤˜ðŸ¾, ðŸ‘ŒðŸ¾, ðŸ‘ˆðŸ¾, ðŸ‘‰ðŸ¾, ðŸ‘†ðŸ¾, ðŸ‘‡ðŸ¾, â˜ðŸ¾, âœ‹ðŸ¾, ðŸ¤šðŸ¾, ðŸ–ðŸ¾, ðŸ––ðŸ¾, ðŸ‘‹ðŸ¾, ðŸ¤™ðŸ¾, ðŸ’ªðŸ¾, ðŸ–•ðŸ¾, âœðŸ¾, ðŸ¤³ðŸ¾, ðŸ’…ðŸ¾, ðŸ‘‚ðŸ¾, ðŸ‘ƒðŸ¾, ðŸ‘¶ðŸ¿, ðŸ‘¦ðŸ¿, ðŸ‘§ðŸ¿, ðŸ‘¨ðŸ¿, ðŸ‘©ðŸ¿, ðŸ‘±ðŸ¿â€â™€ï¸, ðŸ‘±ðŸ¿, ðŸ‘´ðŸ¿, ðŸ‘µðŸ¿, ðŸ‘²ðŸ¿, ðŸ‘³ðŸ¿â€â™€ï¸, ðŸ‘³ðŸ¿, ðŸ‘®ðŸ¿â€â™€ï¸, ðŸ‘®ðŸ¿, ðŸ‘·ðŸ¿â€â™€ï¸, ðŸ‘·ðŸ¿, ðŸ’‚ðŸ¿â€â™€ï¸, ðŸ’‚ðŸ¿, ðŸ•µðŸ¿â€â™€ï¸, ðŸ•µðŸ¿, ðŸ‘©ðŸ¿â€âš•ï¸, ðŸ‘¨ðŸ¿â€âš•ï¸, ðŸ‘©ðŸ¿â€ðŸŒ¾, ðŸ‘¨ðŸ¿â€ðŸŒ¾, ðŸ‘©ðŸ¿â€ðŸ³, ðŸ‘¨ðŸ¿â€ðŸ³, ðŸ‘©ðŸ¿â€ðŸŽ“, ðŸ‘¨ðŸ¿â€ðŸŽ“, ðŸ‘©ðŸ¿â€ðŸŽ¤, ðŸ‘¨ðŸ¿â€ðŸŽ¤, ðŸ‘©ðŸ¿â€ðŸ«, ðŸ‘¨ðŸ¿â€ðŸ«, ðŸ‘©ðŸ¿â€ðŸ­, ðŸ‘¨ðŸ¿â€ðŸ­, ðŸ‘©ðŸ¿â€ðŸ’», ðŸ‘¨ðŸ¿â€ðŸ’», ðŸ‘©ðŸ¿â€ðŸ’¼, ðŸ‘¨ðŸ¿â€ðŸ’¼, ðŸ‘©ðŸ¿â€ðŸ”§, ðŸ‘¨ðŸ¿â€ðŸ”§, ðŸ‘©ðŸ¿â€ðŸ”¬, ðŸ‘¨ðŸ¿â€ðŸ”¬, ðŸ‘©ðŸ¿â€ðŸŽ¨, ðŸ‘¨ðŸ¿â€ðŸŽ¨, ðŸ‘©ðŸ¿â€ðŸš’, ðŸ‘¨ðŸ¿â€ðŸš’, ðŸ‘©ðŸ¿â€âœˆï¸, ðŸ‘¨ðŸ¿â€âœˆï¸, ðŸ‘©ðŸ¿â€ðŸš€, ðŸ‘¨ðŸ¿â€ðŸš€, ðŸ‘©ðŸ¿â€âš–ï¸, ðŸ‘¨ðŸ¿â€âš–ï¸, ðŸ¤¶ðŸ¿, ðŸŽ…ðŸ¿, ðŸ‘¸ðŸ¿, ðŸ¤´ðŸ¿, ðŸ‘°ðŸ¿, ðŸ¤µðŸ¿, ðŸ‘¼ðŸ¿, ðŸ¤°ðŸ¿, ðŸ™‡ðŸ¿â€â™€ï¸, ðŸ™‡ðŸ¿, ðŸ’ðŸ¿, ðŸ’ðŸ¿â€â™‚ï¸, ðŸ™…ðŸ¿, ðŸ™…ðŸ¿â€â™‚ï¸, ðŸ™†ðŸ¿, ðŸ™†ðŸ¿â€â™‚ï¸, ðŸ™‹ðŸ¿, ðŸ™‹ðŸ¿â€â™‚ï¸, ðŸ¤¦ðŸ¿â€â™€ï¸, ðŸ¤¦ðŸ¿â€â™‚ï¸, ðŸ¤·ðŸ¿â€â™€ï¸, ðŸ¤·ðŸ¿â€â™‚ï¸, ðŸ™ŽðŸ¿, ðŸ™ŽðŸ¿â€â™‚ï¸, ðŸ™ðŸ¿, ðŸ™ðŸ¿â€â™‚ï¸, ðŸ’‡ðŸ¿, ðŸ’‡ðŸ¿â€â™‚ï¸, ðŸ’†ðŸ¿, ðŸ’†ðŸ¿â€â™‚ï¸, ðŸ•´ðŸ¿, ðŸ’ƒðŸ¿, ðŸ•ºðŸ¿, ðŸš¶ðŸ¿â€â™€ï¸, ðŸš¶ðŸ¿, ðŸƒðŸ¿â€â™€ï¸, ðŸƒðŸ¿, ðŸ¤²ðŸ¿, ðŸ‘ðŸ¿, ðŸ™ŒðŸ¿, ðŸ‘ðŸ¿, ðŸ™ðŸ¿, ðŸ‘ðŸ¿, ðŸ‘ŽðŸ¿, ðŸ‘ŠðŸ¿, âœŠðŸ¿, ðŸ¤›ðŸ¿, ðŸ¤œðŸ¿, ðŸ¤žðŸ¿, âœŒðŸ¿, ðŸ¤ŸðŸ¿, ðŸ¤˜ðŸ¿, ðŸ‘ŒðŸ¿, ðŸ‘ˆðŸ¿, ðŸ‘‰ðŸ¿, ðŸ‘†ðŸ¿, ðŸ‘‡ðŸ¿, â˜ðŸ¿, âœ‹ðŸ¿, ðŸ¤šðŸ¿, ðŸ–ðŸ¿, ðŸ––ðŸ¿, ðŸ‘‹ðŸ¿, ðŸ¤™ðŸ¿, ðŸ’ªðŸ¿, ðŸ–•ðŸ¿, âœðŸ¿, ðŸ¤³ðŸ¿, ðŸ’…ðŸ¿, ðŸ‘‚ðŸ¿, ðŸ‘ƒðŸ¿, ðŸ¶, ðŸ±, ðŸ­, ðŸ¹, ðŸ°, ðŸ¦Š, ðŸ¦, ðŸ», ðŸ¼, ðŸ¦˜, ðŸ¦¡, ðŸ¨, ðŸ¯, ðŸ¦, ðŸ®, ðŸ·, ðŸ½, ðŸ¸, ðŸµ, ðŸ™ˆ, ðŸ™‰, ðŸ™Š, ðŸ’, ðŸ”, ðŸ§, ðŸ¦, ðŸ¤, ðŸ£, ðŸ¥, ðŸ¦†, ðŸ¦¢, ðŸ¦…, ðŸ¦‰, ðŸ¦š, ðŸ¦œ, ðŸ¦‡, ðŸº, ðŸ—, ðŸ´, ðŸ¦„, ðŸ, ðŸ›, ðŸ¦‹, ðŸŒ, ðŸš, ðŸž, ðŸœ, ðŸ¦—, ðŸ•·, ðŸ•¸, ðŸ¦‚, ðŸ¦Ÿ, ðŸ¦ , ðŸ¢, ðŸ, ðŸ¦Ž, ðŸ¦–, ðŸ¦•, ðŸ™, ðŸ¦‘, ðŸ¦, ðŸ¦€, ðŸ¡, ðŸ , ðŸŸ, ðŸ¬, ðŸ³, ðŸ‹, ðŸ¦ˆ, ðŸŠ, ðŸ…, ðŸ†, ðŸ¦“, ðŸ¦, ðŸ˜, ðŸ¦, ðŸ¦›, ðŸª, ðŸ«, ðŸ¦™, ðŸ¦’, ðŸƒ, ðŸ‚, ðŸ„, ðŸŽ, ðŸ–, ðŸ, ðŸ‘, ðŸ, ðŸ¦Œ, ðŸ•, ðŸ©, ðŸˆ, ðŸ“, ðŸ¦ƒ, ðŸ•Š, ðŸ‡, ðŸ, ðŸ€, ðŸ¿, ðŸ¦”, ðŸ¾, ðŸ‰, ðŸ², ðŸŒµ, ðŸŽ„, ðŸŒ², ðŸŒ³, ðŸŒ´, ðŸŒ±, ðŸŒ¿, â˜˜ï¸, ðŸ€, ðŸŽ, ðŸŽ‹, ðŸƒ, ðŸ‚, ðŸ, ðŸ„, ðŸŒ¾, ðŸ’, ðŸŒ·, ðŸŒ¹, ðŸ¥€, ðŸŒº, ðŸŒ¸, ðŸŒ¼, ðŸŒ», ðŸŒž, ðŸŒ, ðŸŒ›, ðŸŒœ, ðŸŒš, ðŸŒ•, ðŸŒ–, ðŸŒ—, ðŸŒ˜, ðŸŒ‘, ðŸŒ’, ðŸŒ“, ðŸŒ”, ðŸŒ™, ðŸŒŽ, ðŸŒ, ðŸŒ, ðŸ’«, â­ï¸, ðŸŒŸ, âœ¨, âš¡ï¸, â˜„ï¸, ðŸ’¥, ðŸ”¥, ðŸŒª, ðŸŒˆ, â˜€ï¸, ðŸŒ¤, â›…ï¸, ðŸŒ¥, â˜ï¸, ðŸŒ¦, ðŸŒ§, â›ˆ, ðŸŒ©, ðŸŒ¨, â„ï¸, â˜ƒï¸, â›„ï¸, ðŸŒ¬, ðŸ’¨, ðŸ’§, ðŸ’¦, â˜”ï¸, â˜‚ï¸, ðŸŒŠ, ðŸŒ«, ðŸ, ðŸŽ, ðŸ, ðŸŠ, ðŸ‹, ðŸŒ, ðŸ‰, ðŸ‡, ðŸ“, ðŸˆ, ðŸ’, ðŸ‘, ðŸ, ðŸ¥­, ðŸ¥¥, ðŸ¥, ðŸ…, ðŸ†, ðŸ¥‘, ðŸ¥¦, ðŸ¥’, ðŸ¥¬, ðŸŒ¶, ðŸŒ½, ðŸ¥•, ðŸ¥”, ðŸ , ðŸ¥, ðŸž, ðŸ¥–, ðŸ¥¨, ðŸ¥¯, ðŸ§€, ðŸ¥š, ðŸ³, ðŸ¥ž, ðŸ¥“, ðŸ¥©, ðŸ—, ðŸ–, ðŸŒ­, ðŸ”, ðŸŸ, ðŸ•, ðŸ¥ª, ðŸ¥™, ðŸŒ®, ðŸŒ¯, ðŸ¥—, ðŸ¥˜, ðŸ¥«, ðŸ, ðŸœ, ðŸ², ðŸ›, ðŸ£, ðŸ±, ðŸ¥Ÿ, ðŸ¤, ðŸ™, ðŸš, ðŸ˜, ðŸ¥, ðŸ¥®, ðŸ¥ , ðŸ¢, ðŸ¡, ðŸ§, ðŸ¨, ðŸ¦, ðŸ¥§, ðŸ°, ðŸŽ‚, ðŸ®, ðŸ­, ðŸ¬, ðŸ«, ðŸ¿, ðŸ§‚, ðŸ©, ðŸª, ðŸŒ°, ðŸ¥œ, ðŸ¯, ðŸ¥›, ðŸ¼, â˜•ï¸, ðŸµ, ðŸ¥¤, ðŸ¶, ðŸº, ðŸ», ðŸ¥‚, ðŸ·, ðŸ¥ƒ, ðŸ¸, ðŸ¹, ðŸ¾, ðŸ¥„, ðŸ´, ðŸ½, ðŸ¥£, ðŸ¥¡, ðŸ¥¢, âš½ï¸, ðŸ€, ðŸˆ, âš¾ï¸, ðŸ¥Ž, ðŸ, ðŸ‰, ðŸŽ¾, ðŸ¥, ðŸŽ±, ðŸ“, ðŸ¸, ðŸ¥…, ðŸ’, ðŸ‘, ðŸ¥, ðŸ, â›³ï¸, ðŸ¹, ðŸŽ£, ðŸ¥Š, ðŸ¥‹, ðŸŽ½, â›¸, ðŸ¥Œ, ðŸ›·, ðŸ›¹, ðŸŽ¿, â›·, ðŸ‚, ðŸ‹ï¸â€â™€ï¸, ðŸ‹ðŸ»â€â™€ï¸, ðŸ‹ðŸ¼â€â™€ï¸, ðŸ‹ðŸ½â€â™€ï¸, ðŸ‹ðŸ¾â€â™€ï¸, ðŸ‹ðŸ¿â€â™€ï¸, ðŸ‹ï¸â€â™‚ï¸, ðŸ‹ðŸ»â€â™‚ï¸, ðŸ‹ðŸ¼â€â™‚ï¸, ðŸ‹ðŸ½â€â™‚ï¸, ðŸ‹ðŸ¾â€â™‚ï¸, ðŸ‹ðŸ¿â€â™‚ï¸, ðŸ¤¼â€â™€ï¸, ðŸ¤¼â€â™‚ï¸, ðŸ¤¸â€â™€ï¸, ðŸ¤¸ðŸ»â€â™€ï¸, ðŸ¤¸ðŸ¼â€â™€ï¸, ðŸ¤¸ðŸ½â€â™€ï¸, ðŸ¤¸ðŸ¾â€â™€ï¸, ðŸ¤¸ðŸ¿â€â™€ï¸, ðŸ¤¸â€â™‚ï¸, ðŸ¤¸ðŸ»â€â™‚ï¸, ðŸ¤¸ðŸ¼â€â™‚ï¸, ðŸ¤¸ðŸ½â€â™‚ï¸, ðŸ¤¸ðŸ¾â€â™‚ï¸, ðŸ¤¸ðŸ¿â€â™‚ï¸, â›¹ï¸â€â™€ï¸, â›¹ðŸ»â€â™€ï¸, â›¹ðŸ¼â€â™€ï¸, â›¹ðŸ½â€â™€ï¸, â›¹ðŸ¾â€â™€ï¸, â›¹ðŸ¿â€â™€ï¸, â›¹ï¸â€â™‚ï¸, â›¹ðŸ»â€â™‚ï¸, â›¹ðŸ¼â€â™‚ï¸, â›¹ðŸ½â€â™‚ï¸, â›¹ðŸ¾â€â™‚ï¸, â›¹ðŸ¿â€â™‚ï¸, ðŸ¤º, ðŸ¤¾â€â™€ï¸, ðŸ¤¾ðŸ»â€â™€ï¸, ðŸ¤¾ðŸ¼â€â™€ï¸, ðŸ¤¾ðŸ¾â€â™€ï¸, ðŸ¤¾ðŸ¾â€â™€ï¸, ðŸ¤¾ðŸ¿â€â™€ï¸, ðŸ¤¾â€â™‚ï¸, ðŸ¤¾ðŸ»â€â™‚ï¸, ðŸ¤¾ðŸ¼â€â™‚ï¸, ðŸ¤¾ðŸ½â€â™‚ï¸, ðŸ¤¾ðŸ¾â€â™‚ï¸, ðŸ¤¾ðŸ¿â€â™‚ï¸, ðŸŒï¸â€â™€ï¸, ðŸŒðŸ»â€â™€ï¸, ðŸŒðŸ¼â€â™€ï¸, ðŸŒðŸ½â€â™€ï¸, ðŸŒðŸ¾â€â™€ï¸, ðŸŒðŸ¿â€â™€ï¸, ðŸŒï¸â€â™‚ï¸, ðŸŒðŸ»â€â™‚ï¸, ðŸŒðŸ¼â€â™‚ï¸, ðŸŒðŸ½â€â™‚ï¸, ðŸŒðŸ¾â€â™‚ï¸, ðŸŒðŸ¿â€â™‚ï¸, ðŸ‡, ðŸ‡ðŸ», ðŸ‡ðŸ¼, ðŸ‡ðŸ½, ðŸ‡ðŸ¾, ðŸ‡ðŸ¿, ðŸ§˜â€â™€ï¸, ðŸ§˜ðŸ»â€â™€ï¸, ðŸ§˜ðŸ¼â€â™€ï¸, ðŸ§˜ðŸ½â€â™€ï¸, ðŸ§˜ðŸ¾â€â™€ï¸, ðŸ§˜ðŸ¿â€â™€ï¸, ðŸ§˜â€â™‚ï¸, ðŸ§˜ðŸ»â€â™‚ï¸, ðŸ§˜ðŸ¼â€â™‚ï¸, ðŸ§˜ðŸ½â€â™‚ï¸, ðŸ§˜ðŸ¾â€â™‚ï¸, ðŸ§˜ðŸ¿â€â™‚ï¸, ðŸ„â€â™€ï¸, ðŸ„ðŸ»â€â™€ï¸, ðŸ„ðŸ¼â€â™€ï¸, ðŸ„ðŸ½â€â™€ï¸, ðŸ„ðŸ¾â€â™€ï¸, ðŸ„ðŸ¿â€â™€ï¸, ðŸ„â€â™‚ï¸, ðŸ„ðŸ»â€â™‚ï¸, ðŸ„ðŸ¼â€â™‚ï¸, ðŸ„ðŸ½â€â™‚ï¸, ðŸ„ðŸ¾â€â™‚ï¸, ðŸ„ðŸ¿â€â™‚ï¸, ðŸŠâ€â™€ï¸, ðŸŠðŸ»â€â™€ï¸, ðŸŠðŸ¼â€â™€ï¸, ðŸŠðŸ½â€â™€ï¸, ðŸŠðŸ¾â€â™€ï¸, ðŸŠðŸ¿â€â™€ï¸, ðŸŠâ€â™‚ï¸, ðŸŠðŸ»â€â™‚ï¸, ðŸŠðŸ¼â€â™‚ï¸, ðŸŠðŸ½â€â™‚ï¸, ðŸŠðŸ¾â€â™‚ï¸, ðŸŠðŸ¿â€â™‚ï¸, ðŸ¤½â€â™€ï¸, ðŸ¤½ðŸ»â€â™€ï¸, ðŸ¤½ðŸ¼â€â™€ï¸, ðŸ¤½ðŸ½â€â™€ï¸, ðŸ¤½ðŸ¾â€â™€ï¸, ðŸ¤½ðŸ¿â€â™€ï¸, ðŸ¤½â€â™‚ï¸, ðŸ¤½ðŸ»â€â™‚ï¸, ðŸ¤½ðŸ¼â€â™‚ï¸, ðŸ¤½ðŸ½â€â™‚ï¸, ðŸ¤½ðŸ¾â€â™‚ï¸, ðŸ¤½ðŸ¿â€â™‚ï¸, ðŸš£â€â™€ï¸, ðŸš£ðŸ»â€â™€ï¸, ðŸš£ðŸ¼â€â™€ï¸, ðŸš£ðŸ½â€â™€ï¸, ðŸš£ðŸ¾â€â™€ï¸, ðŸš£ðŸ¿â€â™€ï¸, ðŸš£â€â™‚ï¸, ðŸš£ðŸ»â€â™‚ï¸, ðŸš£ðŸ¼â€â™‚ï¸, ðŸš£ðŸ½â€â™‚ï¸, ðŸš£ðŸ¾â€â™‚ï¸, ðŸš£ðŸ¿â€â™‚ï¸, ðŸ§—â€â™€ï¸, ðŸ§—ðŸ»â€â™€ï¸, ðŸ§—ðŸ¼â€â™€ï¸, ðŸ§—ðŸ½â€â™€ï¸, ðŸ§—ðŸ¾â€â™€ï¸, ðŸ§—ðŸ¿â€â™€ï¸, ðŸ§—â€â™‚ï¸, ðŸ§—ðŸ»â€â™‚ï¸, ðŸ§—ðŸ¼â€â™‚ï¸, ðŸ§—ðŸ½â€â™‚ï¸, ðŸ§—ðŸ¾â€â™‚ï¸, ðŸ§—ðŸ¿â€â™‚ï¸, ðŸšµâ€â™€ï¸, ðŸšµðŸ»â€â™€ï¸, ðŸšµðŸ¼â€â™€ï¸, ðŸšµðŸ½â€â™€ï¸, ðŸšµðŸ¾â€â™€ï¸, ðŸšµðŸ¿â€â™€ï¸, ðŸšµâ€â™‚ï¸, ðŸšµðŸ»â€â™‚ï¸, ðŸšµðŸ¼â€â™‚ï¸, ðŸšµðŸ½â€â™‚ï¸, ðŸšµðŸ¾â€â™‚ï¸, ðŸšµðŸ¿â€â™‚ï¸, ðŸš´â€â™€ï¸, ðŸš´ðŸ»â€â™€ï¸, ðŸš´ðŸ¼â€â™€ï¸, ðŸš´ðŸ½â€â™€ï¸, ðŸš´ðŸ¾â€â™€ï¸, ðŸš´ðŸ¿â€â™€ï¸, ðŸš´â€â™‚ï¸, ðŸš´ðŸ»â€â™‚ï¸, ðŸš´ðŸ¼â€â™‚ï¸, ðŸš´ðŸ½â€â™‚ï¸, ðŸš´ðŸ¾â€â™‚ï¸, ðŸš´ðŸ¿â€â™‚ï¸, ðŸ†, ðŸ¥‡, ðŸ¥ˆ, ðŸ¥‰, ðŸ…, ðŸŽ–, ðŸµ, ðŸŽ—, ðŸŽ«, ðŸŽŸ, ðŸŽª, ðŸ¤¹â€â™€ï¸, ðŸ¤¹ðŸ»â€â™€ï¸, ðŸ¤¹ðŸ¼â€â™€ï¸, ðŸ¤¹ðŸ½â€â™€ï¸, ðŸ¤¹ðŸ¾â€â™€ï¸, ðŸ¤¹ðŸ¿â€â™€ï¸, ðŸ¤¹â€â™‚ï¸, ðŸ¤¹ðŸ»â€â™‚ï¸, ðŸ¤¹ðŸ¼â€â™‚ï¸, ðŸ¤¹ðŸ½â€â™‚ï¸, ðŸ¤¹ðŸ¾â€â™‚ï¸, ðŸ¤¹ðŸ¿â€â™‚ï¸, ðŸŽ­, ðŸŽ¨, ðŸŽ¬, ðŸŽ¤, ðŸŽ§, ðŸŽ¼, ðŸŽ¹, ðŸ¥, ðŸŽ·, ðŸŽº, ðŸŽ¸, ðŸŽ», ðŸŽ², ðŸ§©, â™Ÿ, ðŸŽ¯, ðŸŽ³, ðŸŽ®, ðŸŽ°, ðŸš—, ðŸš•, ðŸš™, ðŸšŒ, ðŸšŽ, ðŸŽ, ðŸš“, ðŸš‘, ðŸš’, ðŸš, ðŸšš, ðŸš›, ðŸšœ, ðŸ›´, ðŸš², ðŸ›µ, ðŸ, ðŸš¨, ðŸš”, ðŸš, ðŸš˜, ðŸš–, ðŸš¡, ðŸš , ðŸšŸ, ðŸšƒ, ðŸš‹, ðŸšž, ðŸš, ðŸš„, ðŸš…, ðŸšˆ, ðŸš‚, ðŸš†, ðŸš‡, ðŸšŠ, ðŸš‰, âœˆï¸, ðŸ›«, ðŸ›¬, ðŸ›©, ðŸ’º, ðŸ›°, ðŸš€, ðŸ›¸, ðŸš, ðŸ›¶, â›µï¸, ðŸš¤, ðŸ›¥, ðŸ›³, â›´, ðŸš¢, âš“ï¸, â›½ï¸, ðŸš§, ðŸš¦, ðŸš¥, ðŸš, ðŸ—º, ðŸ—¿, ðŸ—½, ðŸ—¼, ðŸ°, ðŸ¯, ðŸŸ, ðŸŽ¡, ðŸŽ¢, ðŸŽ , â›²ï¸, â›±, ðŸ–, ðŸ, ðŸœ, ðŸŒ‹, â›°, ðŸ”, ðŸ—», ðŸ•, â›ºï¸, ðŸ , ðŸ¡, ðŸ˜, ðŸš, ðŸ—, ðŸ­, ðŸ¢, ðŸ¬, ðŸ£, ðŸ¤, ðŸ¥, ðŸ¦, ðŸ¨, ðŸª, ðŸ«, ðŸ©, ðŸ’’, ðŸ›, â›ªï¸, ðŸ•Œ, ðŸ•, ðŸ•‹, â›©, ðŸ›¤, ðŸ›£, ðŸ—¾, ðŸŽ‘, ðŸž, ðŸŒ…, ðŸŒ„, ðŸŒ , ðŸŽ‡, ðŸŽ†, ðŸŒ‡, ðŸŒ†, ðŸ™, ðŸŒƒ, ðŸŒŒ, ðŸŒ‰, ðŸŒ, âŒšï¸, ðŸ“±, ðŸ“², ðŸ’», âŒ¨ï¸, ðŸ–¥, ðŸ–¨, ðŸ–±, ðŸ–², ðŸ•¹, ðŸ—œ, ðŸ’½, ðŸ’¾, ðŸ’¿, ðŸ“€, ðŸ“¼, ðŸ“·, ðŸ“¸, ðŸ“¹, ðŸŽ¥, ðŸ“½, ðŸŽž, ðŸ“ž, â˜Žï¸, ðŸ“Ÿ, ðŸ“ , ðŸ“º, ðŸ“», ðŸŽ™, ðŸŽš, ðŸŽ›, â±, â², â°, ðŸ•°, âŒ›ï¸, â³, ðŸ“¡, ðŸ”‹, ðŸ”Œ, ðŸ’¡, ðŸ”¦, ðŸ•¯, ðŸ—‘, ðŸ›¢, ðŸ’¸, ðŸ’µ, ðŸ’´, ðŸ’¶, ðŸ’·, ðŸ’°, ðŸ’³, ðŸ§¾, ðŸ’Ž, âš–ï¸, ðŸ”§, ðŸ”¨, âš’, ðŸ› , â›, ðŸ”©, âš™ï¸, â›“, ðŸ”«, ðŸ’£, ðŸ”ª, ðŸ—¡, âš”ï¸, ðŸ›¡, ðŸš¬, âš°ï¸, âš±ï¸, ðŸº, ðŸ§­, ðŸ§±, ðŸ”®, ðŸ§¿, ðŸ§¸, ðŸ“¿, ðŸ’ˆ, âš—ï¸, ðŸ”­, ðŸ§°, ðŸ§², ðŸ§ª, ðŸ§«, ðŸ§¬, ðŸ§¯, ðŸ”¬, ðŸ•³, ðŸ’Š, ðŸ’‰, ðŸŒ¡, ðŸš½, ðŸš°, ðŸš¿, ðŸ›, ðŸ›€, ðŸ›€ðŸ», ðŸ›€ðŸ¼, ðŸ›€ðŸ½, ðŸ›€ðŸ¾, ðŸ›€ðŸ¿, ðŸ§´, ðŸ§µ, ðŸ§¶, ðŸ§·, ðŸ§¹, ðŸ§º, ðŸ§», ðŸ§¼, ðŸ§½, ðŸ›Ž, ðŸ”‘, ðŸ—, ðŸšª, ðŸ›‹, ðŸ›, ðŸ›Œ, ðŸ–¼, ðŸ›, ðŸ§³, ðŸ›’, ðŸŽ, ðŸŽˆ, ðŸŽ, ðŸŽ€, ðŸŽŠ, ðŸŽ‰, ðŸ§¨, ðŸŽŽ, ðŸ®, ðŸŽ, ðŸ§§, âœ‰ï¸, ðŸ“©, ðŸ“¨, ðŸ“§, ðŸ’Œ, ðŸ“¥, ðŸ“¤, ðŸ“¦, ðŸ·, ðŸ“ª, ðŸ“«, ðŸ“¬, ðŸ“­, ðŸ“®, ðŸ“¯, ðŸ“œ, ðŸ“ƒ, ðŸ“„, ðŸ“‘, ðŸ“Š, ðŸ“ˆ, ðŸ“‰, ðŸ—’, ðŸ—“, ðŸ“†, ðŸ“…, ðŸ“‡, ðŸ—ƒ, ðŸ—³, ðŸ—„, ðŸ“‹, ðŸ“, ðŸ“‚, ðŸ—‚, ðŸ—ž, ðŸ“°, ðŸ““, ðŸ“”, ðŸ“’, ðŸ“•, ðŸ“—, ðŸ“˜, ðŸ“™, ðŸ“š, ðŸ“–, ðŸ”–, ðŸ”—, ðŸ“Ž, ðŸ–‡, ðŸ“, ðŸ“, ðŸ“Œ, ðŸ“, âœ‚ï¸, ðŸ–Š, ðŸ–‹, âœ’ï¸, ðŸ–Œ, ðŸ–, ðŸ“, âœï¸, ðŸ”, ðŸ”Ž, ðŸ”, ðŸ”, ðŸ”’, ðŸ”“, â¤ï¸, ðŸ§¡, ðŸ’›, ðŸ’š, ðŸ’™, ðŸ’œ, ðŸ–¤, ðŸ’”, â£ï¸, ðŸ’•, ðŸ’ž, ðŸ’“, ðŸ’—, ðŸ’–, ðŸ’˜, ðŸ’, ðŸ’Ÿ, â˜®ï¸, âœï¸, â˜ªï¸, ðŸ•‰, â˜¸ï¸, âœ¡ï¸, ðŸ”¯, ðŸ•Ž, â˜¯ï¸, â˜¦ï¸, ðŸ›, â›Ž, â™ˆï¸, â™‰ï¸, â™Šï¸, â™‹ï¸, â™Œï¸, â™ï¸, â™Žï¸, â™ï¸, â™ï¸, â™‘ï¸, â™’ï¸, â™“ï¸, ðŸ†”, âš›ï¸, ðŸ‰‘, â˜¢ï¸, â˜£ï¸, ðŸ“´, ðŸ“³, ðŸˆ¶, ðŸˆšï¸, ðŸˆ¸, ðŸˆº, ðŸˆ·ï¸, âœ´ï¸, ðŸ†š, ðŸ’®, ðŸ‰, ãŠ™ï¸, ãŠ—ï¸, ðŸˆ´, ðŸˆµ, ðŸˆ¹, ðŸˆ², ðŸ…°ï¸, ðŸ…±ï¸, ðŸ†Ž, ðŸ†‘, ðŸ…¾ï¸, ðŸ†˜, âŒ, â­•ï¸, ðŸ›‘, â›”ï¸, ðŸ“›, ðŸš«, ðŸ’¯, ðŸ’¢, â™¨ï¸, ðŸš·, ðŸš¯, ðŸš³, ðŸš±, ðŸ”ž, ðŸ“µ, ðŸš­, â—ï¸, â•, â“, â”, â€¼ï¸, â‰ï¸, ðŸ”…, ðŸ”† ã€½ï¸, âš ï¸, ðŸš¸, ðŸ”±, âšœï¸, ðŸ”°, â™»ï¸, âœ…, ðŸˆ¯ï¸, ðŸ’¹, â‡ï¸, âœ³ï¸, âŽ, ðŸŒ, ðŸ’ , â“‚ï¸, ðŸŒ€, ðŸ’¤, ðŸ§, ðŸš¾, â™¿ï¸, ðŸ…¿ï¸, ðŸˆ³, ðŸˆ‚ï¸, ðŸ›‚, ðŸ›ƒ, ðŸ›„, ðŸ›…, ðŸš¹, ðŸšº, ðŸš¼, ðŸš», ðŸš®, ðŸŽ¦, ðŸ“¶, ðŸˆ, ðŸ”£, â„¹ï¸, ðŸ”¤, ðŸ”¡, ðŸ” , ðŸ†–, ðŸ†—, ðŸ†™, ðŸ†’, ðŸ†•, ðŸ†“, 0ï¸âƒ£, 1ï¸âƒ£, 2ï¸âƒ£, 3ï¸âƒ£, 4ï¸âƒ£, 5ï¸âƒ£, 6ï¸âƒ£, 7ï¸âƒ£, 8ï¸âƒ£, 9ï¸âƒ£, ðŸ”Ÿ, ðŸ”¢, #ï¸âƒ£, *ï¸âƒ£, âï¸, â–¶ï¸, â¸, â¯, â¹, âº, â­, â®, â©, âª, â«, â¬, â—€ï¸, ðŸ”¼, ðŸ”½, âž¡ï¸, â¬…ï¸, â¬†ï¸, â¬‡ï¸, â†—ï¸, â†˜ï¸, â†™ï¸, â†–ï¸, â†•ï¸, â†”ï¸, â†ªï¸, â†©ï¸, â¤´ï¸, â¤µï¸, ðŸ”€, ðŸ”, ðŸ”‚, ðŸ”„, ðŸ”ƒ, ðŸŽµ, ðŸŽ¶, âž•, âž–, âž—, âœ–ï¸, â™¾, ðŸ’², ðŸ’±, â„¢ï¸, Â©ï¸, Â®ï¸, ã€°ï¸, âž°, âž¿, ðŸ”š, ðŸ”™, ðŸ”›, ðŸ”, ðŸ”œ, âœ”ï¸, â˜‘ï¸, ðŸ”˜, âšªï¸, âš«ï¸, ðŸ”´, ðŸ”µ, ðŸ”º, ðŸ”», ðŸ”¸, ðŸ”¹, ðŸ”¶, ðŸ”·, ðŸ”³, ðŸ”², â–ªï¸, â–«ï¸, â—¾ï¸, â—½ï¸, â—¼ï¸, â—»ï¸, â¬›ï¸, â¬œï¸, ðŸ”ˆ, ðŸ”‡, ðŸ”‰, ðŸ”Š, ðŸ””, ðŸ”•, ðŸ“£, ðŸ“¢, ðŸ‘â€ðŸ—¨, ðŸ’¬, ðŸ’­, ðŸ—¯, â™ ï¸, â™£ï¸, â™¥ï¸, â™¦ï¸, ðŸƒ, ðŸŽ´, ðŸ€„ï¸, ðŸ•, ðŸ•‘, ðŸ•’, ðŸ•“, ðŸ•”, ðŸ••, ðŸ•–, ðŸ•—, ðŸ•˜, ðŸ•™, ðŸ•š, ðŸ•›, ðŸ•œ, ðŸ•, ðŸ•ž, ðŸ•Ÿ, ðŸ• , ðŸ•¡, ðŸ•¢, ðŸ•£, ðŸ•¤, ðŸ•¥, ðŸ•¦, ðŸ•§, ðŸ³ï¸, ðŸ´, ðŸ, ðŸš©, ðŸ³ï¸â€ðŸŒˆ, ðŸ´â€â˜ ï¸, ðŸ‡¦ðŸ‡«, ðŸ‡¦ðŸ‡½, ðŸ‡¦ðŸ‡±, ðŸ‡©ðŸ‡¿, ðŸ‡¦ðŸ‡¸, ðŸ‡¦ðŸ‡©, ðŸ‡¦ðŸ‡´, ðŸ‡¦ðŸ‡®, ðŸ‡¦ðŸ‡¶, ðŸ‡¦ðŸ‡¬, ðŸ‡¦ðŸ‡·, ðŸ‡¦ðŸ‡², ðŸ‡¦ðŸ‡¼, ðŸ‡¦ðŸ‡º, ðŸ‡¦ðŸ‡¹, ðŸ‡¦ðŸ‡¿, ðŸ‡§ðŸ‡¸, ðŸ‡§ðŸ‡­, ðŸ‡§ðŸ‡©, ðŸ‡§ðŸ‡§, ðŸ‡§ðŸ‡¾, ðŸ‡§ðŸ‡ª, ðŸ‡§ðŸ‡¿, ðŸ‡§ðŸ‡¯, ðŸ‡§ðŸ‡², ðŸ‡§ðŸ‡¹, ðŸ‡§ðŸ‡´, ðŸ‡§ðŸ‡¦, ðŸ‡§ðŸ‡¼, ðŸ‡§ðŸ‡·, ðŸ‡®ðŸ‡´, ðŸ‡»ðŸ‡¬, ðŸ‡§ðŸ‡³, ðŸ‡§ðŸ‡¬, ðŸ‡§ðŸ‡«, ðŸ‡§ðŸ‡®, ðŸ‡°ðŸ‡­, ðŸ‡¨ðŸ‡², ðŸ‡¨ðŸ‡¦, ðŸ‡®ðŸ‡¨, ðŸ‡¨ðŸ‡», ðŸ‡§ðŸ‡¶, ðŸ‡°ðŸ‡¾, ðŸ‡¨ðŸ‡«, ðŸ‡¹ðŸ‡©, ðŸ‡¨ðŸ‡±, ðŸ‡¨ðŸ‡³, ðŸ‡¨ðŸ‡½, ðŸ‡¨ðŸ‡¨, ðŸ‡¨ðŸ‡´, ðŸ‡°ðŸ‡², ðŸ‡¨ðŸ‡¬, ðŸ‡¨ðŸ‡©, ðŸ‡¨ðŸ‡°, ðŸ‡¨ðŸ‡·, ðŸ‡¨ðŸ‡®, ðŸ‡­ðŸ‡·, ðŸ‡¨ðŸ‡º, ðŸ‡¨ðŸ‡¼, ðŸ‡¨ðŸ‡¾, ðŸ‡¨ðŸ‡¿, ðŸ‡©ðŸ‡°, ðŸ‡©ðŸ‡¯, ðŸ‡©ðŸ‡², ðŸ‡©ðŸ‡´, ðŸ‡ªðŸ‡¨, ðŸ‡ªðŸ‡¬, ðŸ‡¸ðŸ‡», ðŸ‡¬ðŸ‡¶, ðŸ‡ªðŸ‡·, ðŸ‡ªðŸ‡ª, ðŸ‡ªðŸ‡¹, ðŸ‡ªðŸ‡º, ðŸ‡«ðŸ‡°, ðŸ‡«ðŸ‡´, ðŸ‡«ðŸ‡¯, ðŸ‡«ðŸ‡®, ðŸ‡«ðŸ‡·, ðŸ‡¬ðŸ‡«, ðŸ‡µðŸ‡«, ðŸ‡¹ðŸ‡«, ðŸ‡¬ðŸ‡¦, ðŸ‡¬ðŸ‡², ðŸ‡¬ðŸ‡ª, ðŸ‡©ðŸ‡ª, ðŸ‡¬ðŸ‡­, ðŸ‡¬ðŸ‡®, ðŸ‡¬ðŸ‡·, ðŸ‡¬ðŸ‡±, ðŸ‡¬ðŸ‡©, ðŸ‡¬ðŸ‡µ, ðŸ‡¬ðŸ‡º, ðŸ‡¬ðŸ‡¹, ðŸ‡¬ðŸ‡¬, ðŸ‡¬ðŸ‡³, ðŸ‡¬ðŸ‡¼, ðŸ‡¬ðŸ‡¾, ðŸ‡­ðŸ‡¹, ðŸ‡­ðŸ‡³, ðŸ‡­ðŸ‡°, ðŸ‡­ðŸ‡º, ðŸ‡®ðŸ‡¸, ðŸ‡®ðŸ‡³, ðŸ‡®ðŸ‡©, ðŸ‡®ðŸ‡·, ðŸ‡®ðŸ‡¶, ðŸ‡®ðŸ‡ª, ðŸ‡®ðŸ‡², ðŸ‡®ðŸ‡±, ðŸ‡®ðŸ‡¹, ðŸ‡¯ðŸ‡², ðŸ‡¯ðŸ‡µ, ðŸŽŒ, ðŸ‡¯ðŸ‡ª, ðŸ‡¯ðŸ‡´, ðŸ‡°ðŸ‡¿, ðŸ‡°ðŸ‡ª, ðŸ‡°ðŸ‡®, ðŸ‡½ðŸ‡°, ðŸ‡°ðŸ‡¼, ðŸ‡°ðŸ‡¬, ðŸ‡±ðŸ‡¦, ðŸ‡±ðŸ‡», ðŸ‡±ðŸ‡§, ðŸ‡±ðŸ‡¸, ðŸ‡±ðŸ‡·, ðŸ‡±ðŸ‡¾, ðŸ‡±ðŸ‡®, ðŸ‡±ðŸ‡¹, ðŸ‡±ðŸ‡º, ðŸ‡²ðŸ‡´, ðŸ‡²ðŸ‡°, ðŸ‡²ðŸ‡¬, ðŸ‡²ðŸ‡¼, ðŸ‡²ðŸ‡¾, ðŸ‡²ðŸ‡», ðŸ‡²ðŸ‡±, ðŸ‡²ðŸ‡¹, ðŸ‡²ðŸ‡­, ðŸ‡²ðŸ‡¶, ðŸ‡²ðŸ‡·, ðŸ‡²ðŸ‡º, ðŸ‡¾ðŸ‡¹, ðŸ‡²ðŸ‡½, ðŸ‡«ðŸ‡², ðŸ‡²ðŸ‡©, ðŸ‡²ðŸ‡¨, ðŸ‡²ðŸ‡³, ðŸ‡²ðŸ‡ª, ðŸ‡²ðŸ‡¸, ðŸ‡²ðŸ‡¦, ðŸ‡²ðŸ‡¿, ðŸ‡²ðŸ‡², ðŸ‡³ðŸ‡¦, ðŸ‡³ðŸ‡·, ðŸ‡³ðŸ‡µ, ðŸ‡³ðŸ‡±, ðŸ‡³ðŸ‡¨, ðŸ‡³ðŸ‡¿, ðŸ‡³ðŸ‡®, ðŸ‡³ðŸ‡ª, ðŸ‡³ðŸ‡¬, ðŸ‡³ðŸ‡º, ðŸ‡³ðŸ‡«, ðŸ‡°ðŸ‡µ, ðŸ‡²ðŸ‡µ, ðŸ‡³ðŸ‡´, ðŸ‡´ðŸ‡², ðŸ‡µðŸ‡°, ðŸ‡µðŸ‡¼, ðŸ‡µðŸ‡¸, ðŸ‡µðŸ‡¦, ðŸ‡µðŸ‡¬, ðŸ‡µðŸ‡¾, ðŸ‡µðŸ‡ª, ðŸ‡µðŸ‡­, ðŸ‡µðŸ‡³, ðŸ‡µðŸ‡±, ðŸ‡µðŸ‡¹, ðŸ‡µðŸ‡·, ðŸ‡¶ðŸ‡¦, ðŸ‡·ðŸ‡ª, ðŸ‡·ðŸ‡´, ðŸ‡·ðŸ‡º, ðŸ‡·ðŸ‡¼, ðŸ‡¼ðŸ‡¸, ðŸ‡¸ðŸ‡², ðŸ‡¸ðŸ‡¦, ðŸ‡¸ðŸ‡³, ðŸ‡·ðŸ‡¸, ðŸ‡¸ðŸ‡¨, ðŸ‡¸ðŸ‡±, ðŸ‡¸ðŸ‡¬, ðŸ‡¸ðŸ‡½, ðŸ‡¸ðŸ‡°, ðŸ‡¸ðŸ‡®, ðŸ‡¬ðŸ‡¸, ðŸ‡¸ðŸ‡§, ðŸ‡¸ðŸ‡´, ðŸ‡¿ðŸ‡¦, ðŸ‡°ðŸ‡·, ðŸ‡¸ðŸ‡¸, ðŸ‡ªðŸ‡¸, ðŸ‡±ðŸ‡°, ðŸ‡§ðŸ‡±, ðŸ‡¸ðŸ‡­, ðŸ‡°ðŸ‡³, ðŸ‡±ðŸ‡¨, ðŸ‡µðŸ‡², ðŸ‡»ðŸ‡¨, ðŸ‡¸ðŸ‡©, ðŸ‡¸ðŸ‡·, ðŸ‡¸ðŸ‡¿, ðŸ‡¸ðŸ‡ª, ðŸ‡¨ðŸ‡­, ðŸ‡¸ðŸ‡¾, ðŸ‡¹ðŸ‡¼, ðŸ‡¹ðŸ‡¯, ðŸ‡¹ðŸ‡¿, ðŸ‡¹ðŸ‡­, ðŸ‡¹ðŸ‡±, ðŸ‡¹ðŸ‡¬, ðŸ‡¹ðŸ‡°, ðŸ‡¹ðŸ‡´, ðŸ‡¹ðŸ‡¹, ðŸ‡¹ðŸ‡³, ðŸ‡¹ðŸ‡·, ðŸ‡¹ðŸ‡², ðŸ‡¹ðŸ‡¨, ðŸ‡¹ðŸ‡», ðŸ‡»ðŸ‡®, ðŸ‡ºðŸ‡¬, ðŸ‡ºðŸ‡¦, ðŸ‡¦ðŸ‡ª, ðŸ‡¬ðŸ‡§, ðŸ´ó §ó ¢ó ¥ó ®ó §ó ¿, ðŸ´ó §ó ¢ó ³ó £ó ´ó ¿, ðŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿, ðŸ‡ºðŸ‡³, ðŸ‡ºðŸ‡¸, ðŸ‡ºðŸ‡¾, ðŸ‡ºðŸ‡¿, ðŸ‡»ðŸ‡º, ðŸ‡»ðŸ‡¦, ðŸ‡»ðŸ‡ª, ðŸ‡»ðŸ‡³, ðŸ‡¼ðŸ‡«, ðŸ‡ªðŸ‡­, ðŸ‡¾ðŸ‡ª, ðŸ‡¿ðŸ‡², ðŸ‡¿ðŸ‡¼, ðŸ¥°, ðŸ¥µ, ðŸ¥¶, ðŸ¥³, ðŸ¥´, ðŸ¥º, ðŸ‘¨â€ðŸ¦°, ðŸ‘©â€ðŸ¦°, ðŸ‘¨â€ðŸ¦±, ðŸ‘©â€ðŸ¦±, ðŸ‘¨â€ðŸ¦², ðŸ‘©â€ðŸ¦², ðŸ‘¨â€ðŸ¦³, ðŸ‘©â€ðŸ¦³, ðŸ¦¸, ðŸ¦¸â€â™€ï¸, ðŸ¦¸â€â™‚ï¸, ðŸ¦¹, ðŸ¦¹â€â™€ï¸, ðŸ¦¹â€â™‚ï¸, ðŸ¦µ, ðŸ¦¶, ðŸ¦´, ðŸ¦·, ðŸ¥½, ðŸ¥¼, ðŸ¥¾, ðŸ¥¿, ðŸ¦, ðŸ¦™, ðŸ¦›, ðŸ¦˜, ðŸ¦¡, ðŸ¦¢, ðŸ¦š, ðŸ¦œ, ðŸ¦ž, ðŸ¦Ÿ, ðŸ¦ , ðŸ¥­, ðŸ¥¬, ðŸ¥¯, ðŸ§‚, ðŸ¥®, ðŸ§, ðŸ§­, ðŸ§±, ðŸ›¹, ðŸ§³, ðŸ§¨, ðŸ§§, ðŸ¥Ž, ðŸ¥, ðŸ¥, ðŸ§¿, ðŸ§©, ðŸ§¸, â™Ÿ, ðŸ§®, ðŸ§¾, ðŸ§°, ðŸ§², ðŸ§ª, ðŸ§«, ðŸ§¬, ðŸ§¯, ðŸ§´, ðŸ§µ, ðŸ§¶, ðŸ§·, ðŸ§¹, ðŸ§º, ðŸ§», ðŸ§¼, ðŸ§½, â™¾, ðŸ´â€â˜ ï¸\n",
      "subreddit: copypasta\n",
      "\n",
      "title: Does anyone here do conformation showing/judging?\n",
      "self_text: Hello r/dogs,  I was wondering if anyone here has experience with dog showing and can tell me a bit more about checking the mouths of dogs. When a judge is looking at the teeth of a dog, are they only judging the bite/missing teeth/jaw alignment, or do they also judge tartar build up?  I looked at the AKC website and it doesn't mention tartar/dental health at all, apart from missing teeth.\n",
      "subreddit: dogs\n",
      "\n",
      "title: Cat too anxious for blood pressure readings\n",
      "self_text: My sweet girl (13) has recently been diagnosed with very early kidney disease. The vet said she'll need regular blood tests, general checkups and blood pressure readings now to keep an eye on things. As well as changing her diet.   The blood tests and checkups are okay. But I took her in for her first blood pressure check today and it didn't work out. When the nurse came into the waiting room I stood up to go in with her, and the nurse said \"no no we'll take her into the back, you stay here\" and I immediately thought \"uh oh\". Her previous owner was abusive towards her and she only really trusts me. It took a long time to build the trust but we're thick as thieves now. The nurse came back after about 20 minutes and said they tried to get a reading about 20 times but my girl kept hissing, crying and jerking away. She said it was impossible and the only way we could maybe get a reading would be by booking a long appointment with the vet, and I hold her and keep her calm. But even then her blood pressure will be much higher and inaccurate because she obviously gets so stressed being there.   So I was wondering if anyone's had any success with doing home readings? Can I get a regular blood pressure machine and a small cuff, and then send the results in to my vet? Or is there anything else I can do to make this easier and less anxiety inducing for my baby?\n",
      "subreddit: CatAdvice\n",
      "\n",
      "title: Looking to help push my Worldfire deck further\n",
      "self_text: Here's the link to the deck.  I added a primer in hopes to explain the ideas behind what I have at the moment.   The whole idea of it really is to play Worldfire and then Jeska right after doing at least 1 damage to everyone at the table. If I can't do it immediately, then I play against the table with the one card draw shenanigans. I have low cost creatures to help me win after the Worldfire if necessary.   Starting with getting Tevesh out for the card draw and the possibility of doing the ultimate ability. I only cast Jeska after the Worldfire.   I don't have any swamps to avoid getting stuck after a worldfire. The Black mana I need usually either comes from treasures or dual lands (and the once in a while Phyrexian Altar).  I would like to make the deck faster and more reliable. Any thoughts or ideas are more than welcome. Thanks!\n",
      "subreddit: EDH\n",
      "\n",
      "title: Can anyone recommend a dentist with big boobies?\n",
      "self_text: Hey, I moved here last year and I'm thinking of going to the dentist, as my teeth are starting to feel pretty sticky. My last dentist had large jugs that were always right in my face as she worked, which I really enjoyed. Does anyone know of a dentist in Houston that also has large breasts? Thanks!\n",
      "subreddit: copypasta\n",
      "\n",
      "title: Bechamel in potato gratin?\n",
      "self_text: Hello hello! So im planning on making a potato gratin for Thanksgiving, and I know the typical recipe uses a heavy cream mixture. But if I were to want to \"jazz it up\" using a bechamel instead of cream, would that be good or would that be too heavy? Also if there are any other ways to spice it up or just any adjustments you guys like, I would love to know and potentially try em! Thank you guys\n",
      "subreddit: Cooking\n",
      "\n",
      "title: Say what you will about Die Hard, but it has the best ending for a Christmas movieâ€¦\n",
      "self_text: Hans down.\n",
      "subreddit: Jokes\n",
      "\n",
      "title: Oh lord, adolescence is upon us\n",
      "self_text: I just came here to ventâ€¦. Hudson our 6mo ridgeback has gone from a lovely excitable boy to a little fucking terror. No more sleeping till 7, up at 6am like a bat out of hell, no more behaved little land shark, heâ€™s turned into jaws the past couple of weeks. Oh and try to have any intimacy with my wife, forget it, maniac mode initiated.   Welpâ€¦ here is to several months of trying not to lose my cool, being super consistent and obedience classes, wish me luck all!\n",
      "subreddit: puppy101\n",
      "\n",
      "title: i finally know how it feels like...\n",
      "self_text: When your crush complains about how much guys suck  When she says that no guys like her and would ever like her  When she describes all the values she's looking for and you have them all  When she talks about how great a friend I am to listen to her rants  I have been friendzoned. Nothing could quite describe the dispair.  Girls, know a really great guy you like? Why not consider dating him?\n",
      "subreddit: ucla\n",
      "\n",
      "title: Nancy and Zanab\n",
      "self_text: Anybody else think there taste in men might be the issue? I mean listening to cole and bartise in the pods what sensible mature person would want to date them. They arent bad people but you can tell by listening to them they didnt seem ready for a mature relationship. Plus the age difference. I have no issue with women dating younger men, my wife is older than me. Men just take more time to mature. What were there goals with these guys? Did they expect to just be able to mold them in a couple of weeks? I honestly dont understand it.\n",
      "subreddit: LoveIsBlindOnNetflix\n",
      "\n",
      "title: getting my cat neuter today\n",
      "self_text: This is my first cat and i have no cule what I'm doing any tips.\n",
      "subreddit: CatAdvice\n",
      "\n",
      "title: PSA - if your dog is anything like mine, donâ€™t waste your money on the Yeti dog bowl\n",
      "self_text: He doesnâ€™t care that it keeps water cooler for longer or how expensive it is. The toilet bowl is still is preferred utensil for water consumption.\n",
      "subreddit: dogs\n",
      "\n",
      "title: Please I need help Iâ€™m 19!!!\n",
      "self_text: I recentely went to sams clubs and picked up some tyson frozen nuggets from the store. mind you i did this right before i had school so i had no choice but to the leave the bag of nuggets in my trunk. Once i got finished with class i got caught up hanging with my friends from about 12:30pm to at least 12am and totally forgot all about the chicken. After i got done hanging with my friends i instantly remeber that i left them in my trunk. I went back to go get them then safely refroze it back in my dorm freezer. since itâ€™s november during the afternoon it was around 63 to 70 degress i believe then during the evening/night it dropped down to at least 45 degrees. I really need help guys because i dont know if i should consume these nuggets.\n",
      "subreddit: Cooking\n",
      "\n",
      "title: What's the population of Gilead? And what happened to black and gay people?\n",
      "self_text: The lore of the handmaids tale interests me very much. But I feel like we don't see much of it outside of June's world.   I know that when the takeover started, many people fled continental US, and many people were killed. What of those left, what's the population?  Also, what happened to gay people and black people? Were they killed too? Like I feel like you only see black handmaids, some of who are also lesbians and fertile but aren't allowed to act on it (obviously). I think we see a photo of a gay male couple in season 5, who are presumed dead, so was that the fate of all gay men?\n",
      "subreddit: TheHandmaidsTale\n",
      "\n",
      "title: How serious is UAN about roommates complaints?\n",
      "self_text: I don't get along with my roommates and was wondering if housing can kick me out if they complain about me? I bring people over sometimes or even overnight guests. I don't follow their ridiculous schedules about cooking, cleaning, buying communal supplies. They have outlined some of the points in the roommate contract, but I haven't signed it or even wrote a word down while they were completing it. So, my question is, how strict are UAN about this and does this roommate contract do anything at all?\n",
      "subreddit: ucla\n",
      "\n",
      "title: Love Is Blind discord server!\n",
      "self_text: This is new new, but we have one for Love Island and itâ€™s so much fun. If you join and have suggestions for channels, content, etc. please lmk!!  https://discord.gg/fEhbmEfe\n",
      "subreddit: LoveIsBlindOnNetflix\n",
      "\n",
      "title: Credit to u/Bjorn_Suicide on mw2 sub\n",
      "self_text: Is this sub just for whining?  Seriously dude every post is â€œmw2 is shit and the guns are shit and Iâ€™m shit but only cuz the game is shit.â€  Yâ€™all know you donâ€™t have to play right? Like some complaining is expected but god damn itâ€™s like the whole subâ€™s members just all got their periods together. The game isnâ€™t that bad and if you think it is then thatâ€™s cool too gtfo. Nobody is making you play. Fuck off. Go away. Toodles. Bye bye. Or play idc but stop whining every time you die and get better. So my question is; will there actually be any gameplay, tips, comedy or anything that isnâ€™t whining 24/7?  Edit: Jesus fucking Christ the amount of people saying â€œ youâ€™re whining about whining. Isnâ€™t that ironic?â€ Iâ€™ll address this here since you stupid fucks canâ€™t read. It was a question. Trying to see if thatâ€™s all this sub did. If you think Iâ€™m whining fine. If itâ€™s ironic then cool. The fact that so many of you comment all the same shit speaks volumes about your ability to think for yourselves. Although that would require thinking and Iâ€™m pretty sure thatâ€™s a little advanced for you idiots. Also to clarify yes the game should be finished. Those complaints are valid. Any complaints about dying, guns, etc. are just you being shit and was the entire point of the post. Holy fuck you guys make my brain cells want to commit suicide.\n",
      "subreddit: copypasta\n",
      "\n",
      "title: I need a Chrons friendly, gluten free, and only hard cheeses recipe.\n",
      "self_text: A buddy of mine has a lot of digestive issues and he's never really had anyone cook for him so I wanted to treat him. I want something fancy as hell and I'm not worried about price of ingredients at all. I want something that's really gonna blow him away and impress him. I know he can eat chicken and any seafood but like selfish do give him a minor reaction. Red meat is completely off the table.\n",
      "subreddit: Cooking\n",
      "\n",
      "title: Timeline\n",
      "self_text: Anyone know where to find the timing of filming/the reunion? I saw a post saying the weddings were a year and a half ago and a week after season 2 was filmed! Weâ€™re the reunions recent?\n",
      "subreddit: LoveIsBlindOnNetflix\n",
      "\n",
      "title: Whoâ€™s the girl on the cover of Her Loss?\n",
      "self_text: Can anybody tell me whoâ€™s on the cover of Her Loss and why was she used? Whatâ€™s the significance??\n",
      "subreddit: Drizzy\n",
      "\n",
      "title: Team System??\n",
      "self_text: Has anyone heard about the game adding a team system or dynamic?! I can see it could add a more competitive aspect and ranking play to the game!  I really hope so, what do you guys think?\n",
      "subreddit: MarvelSnap\n",
      "\n",
      "title: Parking Application Hard Deadline?\n",
      "self_text: I am a student living on the hill, and I park my car at SV because I have an off campus job. The UCLA Transportation website says that the deadline is the 27th. Is this a hard deadline? I am asking this because this quarter, there wasn't a hard deadline (iirc?), because I applied quite late.  &#x200B;  Thank you.\n",
      "subreddit: ucla\n",
      "\n",
      "title: pasta salad sweetened\n",
      "self_text: I have to know why and when we began to sweeten pasta salads\n",
      "subreddit: Cooking\n",
      "\n",
      "title: When is dog over emotional threshold using Julie Naismith method?\n",
      "self_text: I am using Julie Naismiths method to leave dog alone. So I am basically leaving the front door for a few seconds at a time, coming back before he is over his \"emotional threshold\" and repeat, repeat, repeat and eventually up the time by adding a few seconds at a time.  My question is what counts as over his emotional threshold? My dog will let out a whimper within 2 seconds of me leaving  and I'm finding it difficult to get any further if I use this cue as I am really just shutting the door and opening it again!   I left him longer, up to a min, to see what he done and he lets out a whimper which feels like more of a whine than a cry (?) and then lies down, looks relaxed, but continues to let out a wee whine every few seconds.  Would you say these little whines are over h threshold? Or is this just a little annoyance at being left and wouldn't count as over his emotional threshold? I would say he is definitely not stressed during this first min but from what I've read you should take any sign of being uncomfortable as over his emotional threshold for this method?  Appreciate any advice!\n",
      "subreddit: puppy101\n",
      "\n",
      "title: TF2 rant\n",
      "self_text: you dont have to be a main anything  maining things is stupid  why the fuck do people say this?  its just whatever class you play the most right?  but thats irrelevant as every class doesnt take that long to be good at...  so.. \"maining\" things just makes it seem like you want it to be more than playing a class  (Iâ€™m just gonna @ u/HoovyKitty to let him know I put this on the subreddit.)\n",
      "subreddit: copypasta\n",
      "\n",
      "title: Uh Oh! Looks like you've posted a stupid ass meme!\n",
      "self_text: Uh oh! Looks like you've posted a stupid ass meme! Unfortunately such an act is only punishable by revoking your access to the internet! You have 30 minutes to strip yourself from all access to the internet and lay down face-first outside of your house as local authorities arrive! Resisting only prolongs the inevitable arrest.\n",
      "subreddit: copypasta\n",
      "\n",
      "title: THMT Trauma\n",
      "self_text: I binge watched for the first time all of THMT in about 3 weeks. Today in Target an employee walked by with their walkie talkie crackling and going off. I jumped and looked down and almost said â€œunder his eye.â€ ðŸ˜… Just some little excess trauma.\n",
      "subreddit: TheHandmaidsTale\n",
      "\n",
      "title: Vicaria-Angel or Do for Math 61?\n",
      "self_text: Anyone have advice on who is easier?\n",
      "subreddit: ucla\n",
      "\n",
      "title: Beginner's caves?\n",
      "self_text: Where are some easy caves on Fjordur? I have never done one before, i don't have insanely high level tames and i have pretty basic dinos. Is there a cave i can do that probably won't kill me? And what kind of dino should i bring in there?\n",
      "subreddit: ARK\n",
      "\n",
      "title: Please give me back my black leather jacket\n",
      "self_text: I left a black leather jacket for one hour (from 11 am to 12.15, Monday November 7th) on a chair in front of Royce Hall 190 and now it has disappeared. Please, for whoever took it, it has an extreme emotional and affective value for me, can you please give it back to me? I beg you. Leave it somewhere and please drop a hint under this post on where I can find it. I love that jacked with my whole heart and it was a gift from a very important person and it carries very good memories for me. You can find another black leather jacket anywhere.  Do you guys know if there is a lost&found here at UCLA?\n",
      "subreddit: ucla\n",
      "\n",
      "title: Looking for players on Xbox\n",
      "self_text: Hey everyone I recently started playing on Xbox and am looking for others to do official pvp. If you are interested please send me a message\n",
      "subreddit: ARK\n",
      "\n",
      "title: Organic Chemistry 30B\n",
      "self_text: Has anyone taken organic chemistry 30B with Yvez Rubin? I'm taking his class next quarter and I want to know how smooth it's gonna go because I want to take more classes.\n",
      "subreddit: ucla\n",
      "\n",
      "title: Roger Waters Eurovision antisemite rant\n",
      "self_text: Good morning. I woke up about a quarter of an hour ago to an email from my friend Omar Baghoutti about the Eurovision Song Contest, and I thought immediately of a few choice-Fâ€™s. But three of them are as follows: First of all thereâ€™s a film, and that film was The Invasion of the Bodysnatchers. Eurovision reminds me of The Invasion of the Bodysnatchers. Because it seems it may have been taken over byâ€¦ I believe they were aliens. I know itâ€™s giving aliens a bad name, but, and at the end of the movie Donald Sutherland points at somebody and heâ€” like this:  The Bodysnatchers are doing that now, but normally what theyâ€™re going is, â€œANTISEMIIIIIIIIIIIITE!â€  Alright. The fable is The Fable of the Emperorâ€™s New Clothes. â€œMommy, mommy, why is the emperor of Israel parading his ethnocentrist bullshit around naked?â€  Okay, enough with nenya-ooo-ooo-ooo-oooh. The third F, and by far the most important one, is the subject of Omarâ€™s messive to me, and that is that a hundreâ€” itâ€™s faith. Faith in my fellow human beings. Faith in their capacity for love and empathy.   136,000 of our Swiss brothers and sisters have signed and delivered a petition demanding that Eurovision pull out of the finals in Tel Aviv.  I cannot tell you how much that has lightened my morning. Iâ€™m gonna go make myself another cup of coffee with love in my heart for all those Swiss people. Thank you, thank you, thank you.\n",
      "subreddit: copypasta\n",
      "\n",
      "title: Can anyone recommend me some PC games to play?\n",
      "self_text: Can you please recommend based on the games I played so far:    Dragon Age  Bioshock  Sims (LOL)  Life is Strange  The Journey  GTA   Thank you!\n",
      "subreddit: gaming\n",
      "\n",
      "title: I'm a part time scientist and have no idea how to cook and I started blog of clueless experimental cooking.\n",
      "self_text: I min maxed all my points into academia and video games.  When my Gran passed away, I had to learn how to cook, but I don't have a clue, just the scientific method, lets see how the clueless experiments turn out:  https://www.youtube.com/watch?v=q0Dij8DGDjI\n",
      "subreddit: Cooking\n",
      "\n",
      "title: Suggestions from a returning player\n",
      "self_text: The game is becoming stale   I believe that after a few wipes there is very little content in the game to keep me interested. Battlestate Games have tackled this by implementing wipes and daily quests which are good addition to this, but not enough. For me, when I look back the most enjoyable aspect was learning the game, I believe that for the game to emulate this it would need to have major changes between wipes.  A few examples of this could be to change locations of: loot spawns, marked rooms, scav spawns, barter trades etc. Basically add some randomness that requires to relearn certain aspects of the game.  Also quests could change, instead of having the same quests Battlestate Games could create hundreds that alternate between each wipe. Or they could have a more personal approach and have a main theme or story each wipe which reflect on unique quests.  &#x200B;   Lobby for hackers   Hackers are clearly present in the game, which is natural due to the economic nature of the game. Although Battlestate Game has been doing a large amount of work to tackle them, they keep appearing. Clearly this approach doesn't seem to be working as the main problem is that the developers have to spend time dealing with hackers rather than adding content and improving the game.  The Cycle: Frontier, a looter shooter, has addressed the issue in a unique way. They use a system called trusted matchmaking, once a player becomes trusted they get matched to other trusted players. Cheaters are also pooled into their own lobbies. There are multiple factors that go into determining when you are trusted such as: numbers of hours played, lifetime of the steam account, stats in-game etc. Also if you get killed by a hacker the items you brought into the raid are returned back to you.  Although Battlestate Games may be hesitant to introduce any sort of \"matchmaking\" system, I believe this may be necessary to combat hackers.  &#x200B;   Reminders in main menu before joining raids   Currently if your health is low you when starting a raid you get a warning. I believe this is beneficial but should also be applied to other areas as well such as: low durability in armour, mags not filled, bags not empty, food and fluid is low.  I understand that EFT is a hardcore game and has little to no hand holding. However I don't believe forgetting to do something in the lobby is reflective of your in-game skill. Furthermore loading into a raid with the above negative effects detracts from the overall experience, it simply leaves you angry and frustrated. Games are primarily meant to be fun.  New players in particular are already having to learn a lot and will naturally forget some things, informing them of mistakes before they are made will increase player retention and enjoyment. Although I'm sure this also affects experienced players, the number of times I have died for forgetting to eat a drink is more than I can count over the course of several wipes.  &#x200B;   Filtering   The filtering in the game menu currently highlights items in set categories, however I don't think this is really helpful.  I believe instead a more useful approach would be to display an inventory with just the filtered items at the top, including anything in containers being brought out and shown.\n",
      "subreddit: EscapefromTarkov\n",
      "\n",
      "title: Frying Fish\n",
      "self_text: Hi Everyone! I just bought a stainless steel skillet and everytime I use it something is always getting stuck on it :( specifically when itâ€™s time to fry something.   I wait until the pan is hot, splash water on it to make sure the water bounces instead of fizzles, add my oil, and then the fish - but when I try to turn the fish the bottom of the fish sticks to the pan. (The temp is on 5 and it begins to fry btw).  What on earth am I doing wrong? :(\n",
      "subreddit: Cooking\n",
      "\n",
      "title: \"Here,\" I told the vampire after it had drank its fill of blood, \"have some H2O too.\"\n",
      "self_text: As it laughed, I opened the bottle of concentrated hydrogen peroxide and poured the contents down its throat.\n",
      "subreddit: TwoSentenceHorror\n",
      "\n",
      "Use this format. \n",
      "\n",
      "title: {title}\n",
      " self_text: {self_text}\n",
      " subreddit: {subreddit}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt = generate_like_history_prompt(client, DATASETS2[5])\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c7088",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd32687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "login()\n",
    "\n",
    "# training size ablation\n",
    "for model_name in tqdm(MODELS, desc=\"Models\"):\n",
    "    if model_name == \"gpt-5\":\n",
    "        continue\n",
    "\n",
    "    model, tokenizer = load_model(model_name)\n",
    "\n",
    "    for dataset_name, dataset_dict in tqdm(zip(DATASET1_NAMES, DATASETS1), total=len(DATASETS1), desc=\"Datasets1\"):\n",
    "        for train_size in tqdm(TRAIN_SIZES, desc=\"Train sizes\"):\n",
    "            soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/{model_name}/{dataset_name}/{train_size}\"\n",
    "            \n",
    "            # check if soft prompt exists\n",
    "            if not os.path.exists(soft_output_dir):\n",
    "                examples = load_datasets_proportional(dataset_dict, train_size, PROMPTS[\"soft prompt\"])\n",
    "                train_ds = preprocess_dataset(examples, tokenizer)\n",
    "                \n",
    "                peft_model = init_peft_model(model, model_name)\n",
    "                train_soft_prompt(peft_model, tokenizer, train_ds, NUM_TRAIN_STEPS, soft_output_dir)\n",
    "            else:\n",
    "                print(f\"Soft prompt {soft_output_dir} exists, skipping training\")\n",
    "\n",
    "            # apply soft prompt to model\n",
    "            peft_model = apply_peft_adapter(model, soft_output_dir)\n",
    "            \n",
    "            # generate posts\n",
    "            out_path = f\"{GENERATED_OUTPUT_DIR}/train_size_ablation/{model_name}/{dataset_name}/{train_size}.json\"\n",
    "            \n",
    "            # Check if output file already exists and has posts\n",
    "            out_path_obj = Path(out_path)\n",
    "            out_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if out_path_obj.exists():\n",
    "                try:\n",
    "                    with open(out_path_obj, 'r') as f:\n",
    "                        existing_posts = json.load(f)\n",
    "                    if existing_posts:  # Skip if file has posts\n",
    "                        print(f\"Output file {out_path} already exists with posts, skipping generation\")\n",
    "                        continue\n",
    "                except (json.JSONDecodeError, Exception):\n",
    "                    print(f\"Could not read existing file {out_path}, regenerating\")\n",
    "            \n",
    "            posts = collect_posts(100, PROMPTS[\"soft prompt\"], peft_model, tokenizer)\n",
    "            write_posts_json(out_path_obj, posts)\n",
    "    \n",
    "    # unload model\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "import os, dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "login()\n",
    "\n",
    "# experiment ablation\n",
    "for model_name in tqdm(MODELS, desc=\"Models\"):\n",
    "    if model_name == \"gpt-5\": \n",
    "        model = OpenAI()   \n",
    "        tokenizer = None         \n",
    "    else: \n",
    "        model, tokenizer = load_model(model_name)\n",
    "    for dataset_name, dataset_dict in tqdm(zip(DATASET2_NAMES, DATASETS2), total=len(DATASETS2), desc=\"Datasets2\"):\n",
    "        for experiment in tqdm(EXPERIMENTS, desc=\"Experiments\"):\n",
    "            prompt = PROMPTS[experiment]\n",
    "            if experiment == \"soft prompt\":\n",
    "                if model_name == \"gpt-5\": continue\n",
    "\n",
    "                soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/soft_prompt_{model_name}_{dataset_name}_exp\"\n",
    "                \n",
    "                # check if soft prompt exists\n",
    "                if not os.path.exists(soft_output_dir):\n",
    "                    examples = load_datasets_proportional(dataset_dict, 100, prompt)\n",
    "                    train_ds = preprocess_dataset(examples, tokenizer)\n",
    "                    peft_model = init_peft_model(model, model_name)\n",
    "                    train_soft_prompt(peft_model, tokenizer, train_ds, NUM_TRAIN_STEPS, soft_output_dir)\n",
    "                else:\n",
    "                    print(f\"Soft prompt {soft_output_dir} exists, skipping training\")\n",
    "                model = apply_peft_adapter(model, soft_output_dir)\n",
    "            elif experiment == \"fine tune\":\n",
    "                break  # skip fine tuning\n",
    "            elif experiment == \"summary\":\n",
    "                prompt = generate_summarized_prompt(model, dataset_dict)\n",
    "            elif experiment == \"like history\":\n",
    "                prompt = generate_like_history_prompt(model, dataset_dict)\n",
    "            elif experiment == \"self defined\":\n",
    "                prompt = prompt[dataset_name]\n",
    "\n",
    "            # generate posts\n",
    "            out_path = f\"{GENERATED_OUTPUT_DIR}/experiment_ablation/{model_name}_{dataset_name}_{experiment}.json\"\n",
    "            \n",
    "            # Check if output file already exists and has posts\n",
    "            out_path_obj = Path(out_path)\n",
    "            out_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if out_path_obj.exists():\n",
    "                try:\n",
    "                    with open(out_path_obj, 'r') as f:\n",
    "                        existing_posts = json.load(f)\n",
    "                    if existing_posts:  # Skip if file has posts\n",
    "                        print(f\"Output file {out_path} already exists with posts, skipping generation\")\n",
    "                        continue\n",
    "                except (json.JSONDecodeError, Exception):\n",
    "                    print(f\"Could not read existing file {out_path}, regenerating\")\n",
    "            \n",
    "            posts = collect_posts(100, prompt, model, tokenizer)\n",
    "            write_posts_json(out_path_obj, posts)\n",
    "                    \n",
    "    # unload model\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# judging \n",
    "for model_name in tqdm(MODELS, desc=\"Models\"):\n",
    "    for dataset in tqdm(DATASETS1, desc=\"Datasets1\"):\n",
    "        for train_size in tqdm(TRAIN_SIZES, desc=\"Train sizes\"):\n",
    "            with open(f\"{GENERATED_OUTPUT_DIR}/train_size_ablation/{model_name}_{dataset}_{train_size}.json\", \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i in tqdm(range(0, len(lines), 3), desc=\"Posts\"):\n",
    "                    post_lines = lines[i:i+3]\n",
    "                    post = ''.join(line.strip() for line in post_lines)\n",
    "                    if post:\n",
    "                        gpt_judgement = judge_post_gpt5(post, dataset)\n",
    "                        heuristic_judgement = judge_post_heuristic(post, dataset)\n",
    "                        print(post)\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# judging \n",
    "for model_name in tqdm(MODELS, desc=\"Models\"):   \n",
    "    for dataset in tqdm(DATASETS2, desc=\"Datasets2\"):\n",
    "        for experiment in tqdm(EXPERIMENTS, desc=\"Experiments\"):\n",
    "            with open(f\"{GENERATED_OUTPUT_DIR}/experiment_ablation/{model_name}_{dataset}_{experiment}.json\", \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i in tqdm(range(0, len(lines), 3), desc=\"Posts\"):\n",
    "                    post_lines = lines[i:i+3]\n",
    "                    post = ''.join(line.strip() for line in post_lines)\n",
    "                    if post:\n",
    "                        gpt_judgement = judge_post_gpt5(post, dataset)\n",
    "                        heuristic_judgement = judge_post_heuristic(post, dataset)\n",
    "                        print(post)\n",
    "                        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
