{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8a0981",
   "metadata": {},
   "source": [
    "# Ablation Study Results\n",
    "\n",
    "Axes\n",
    "- model (gemma-3-4b-it, gemma-3-27b-it, gpt-5)\n",
    "- manual vs judge\n",
    "- experiment (base, subreddit, summary, liked posts list, fine tuned, soft prompt (100), soft prompt(500))\n",
    "- dataset (circlejerk, jokes + puns, gaming, animals, sports, etc.)\n",
    "\n",
    "Dataset selection:\n",
    "- small\n",
    "    - okbuddy\n",
    "    - boomerhumor\n",
    "    - animals\n",
    "    - creative\n",
    "    - food\n",
    "    - religion\n",
    "- medium\n",
    "    - finance\n",
    "    - school\n",
    "    - pop\n",
    "- varied but focused: \n",
    "    - nerdy\n",
    "    - personal\n",
    "    - ucla\n",
    "    - tech\n",
    "    - school\n",
    "    - ucla\n",
    "- ultra specific: \n",
    "    - minecraft\n",
    "    - nba\n",
    "- format specific:\n",
    "    - copypasta,\n",
    "    - no stupid questions\n",
    "    - am i the asshole\n",
    "- 3 test (unalike)\n",
    "    - pop\n",
    "    - religion\n",
    "    - tech\n",
    "- 3 test (alike)\n",
    "    - tech\n",
    "    - nerdy\n",
    "    - finance\n",
    "- college student\n",
    "    - ucla\n",
    "    - nerdy\n",
    "    - okbuddy\n",
    "    - copypasta\n",
    "    - pop\n",
    "    - food\n",
    "    - animals\n",
    "- new mother\n",
    "    - pregnancy\n",
    "    - parenting\n",
    "    - baby\n",
    "    - food\n",
    "    - am i the asshole\n",
    "    - pop\n",
    "    - boomerhumor\n",
    "- creative gen alpha\n",
    "    - minecraft\n",
    "    - creative\n",
    "    - food\n",
    "    - school\n",
    "    - nba\n",
    "\n",
    "### gemma-3-4b-it\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n",
    "\n",
    "### gemma-3-27b-it\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n",
    "\n",
    "### gpt-5\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2e1e5",
   "metadata": {},
   "source": [
    "## Notes during testing\n",
    "- very good alignment on tech (92)\n",
    "- very good alignment on personal (180)\n",
    "- very good alignment on nerdy (204)\n",
    "- very good alignmenton school (93)\n",
    "- bad alignment on interesting (35)\n",
    "- bad alignment on finance (57)\n",
    "- decent alignment on interesting + finance (on the finance side) (92)\n",
    "- for hyper targeted like minecraft, even 10 examples is good enough for alignment, but post topic variety goes down. ~100 is best to balance quality and training time\n",
    "- 50/50 minecraft + ucla works REALLY well\n",
    "\n",
    "it seems like ~100 samples is good enough for gemma 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf9bfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minecraft', 'ucla', 'nostupidquestions', 'copypasta']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS = [\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-27b-it\",\n",
    "    \"gpt-5\"\n",
    "]\n",
    "\n",
    "# Load dataset categories from JSON file\n",
    "DATASETS1 = [\n",
    "    \"minecraft\",\n",
    "    \"ucla\", \n",
    "    \"nostupidquestions\",\n",
    "    \"copypasta\",\n",
    "]\n",
    "\n",
    "DATASETS2 = [\n",
    "    \"nerdy\",\n",
    "    \"personal\", \n",
    "    \"alike 3\",\n",
    "    \"unalike 3\",\n",
    "    \"format specific\",\n",
    "    \"college student\",\n",
    "    \"new mother\",\n",
    "    \"creative gen alpha\"\n",
    "]\n",
    "\n",
    "TRAIN_SIZES = [\n",
    "    10, 20, 50, 100, 250, 500, 1000, 5000\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    \"self defined\",\n",
    "    \"summary\",\n",
    "    \"like history\",\n",
    "    \"fine tune\",\n",
    "    \"soft prompt\",\n",
    "]\n",
    "\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 0.2\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dabf1f",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a generator to lazily load dataset from json file based on dataset argument\n",
    "\n",
    "\"\"\"\n",
    "choice for sample ablation:\n",
    "    - make sure to increase max number of training steps as you go\n",
    "    - do some testing beforehand for 1000, 5000 to find good number of steps\n",
    "    - 10, 20, 50, 100, 250, 500, 1000, 5000\n",
    "    - minecraft\n",
    "    - ucla\n",
    "    - nostupidquestions   \n",
    "    - copypasta\n",
    "    - 4 * 8 = 32 soft prompts\n",
    "    \n",
    "choice (for 100 samples, or optimal from above):\n",
    "    - nerdy\n",
    "    - personal \n",
    "    - alike 3\n",
    "    - unalike 3\n",
    "    - format specific\n",
    "    - college student \n",
    "    - new mother \n",
    "    - creative gen alpha \n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "def load_datasets_proportional(datasets_dict: Dict[str, float], total_posts: int, prompt: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load datasets with proportional sampling.\n",
    "    \n",
    "    Args:\n",
    "        datasets_dict: Dictionary mapping dataset names to their proportions (0-1)\n",
    "        total_posts: Total number of posts desired across all datasets\n",
    "    \n",
    "    Returns:\n",
    "        List of examples in the format: {\"instruction\": PROMPT, \"output\": post}\n",
    "    \"\"\"\n",
    "    # Validate proportions sum to approximately 1\n",
    "    total_proportion = sum(datasets_dict.values())\n",
    "    if not (0.99 <= total_proportion <= 1.01):\n",
    "        print(f\"Warning: Proportions sum to {total_proportion:.3f}, not 1.0\")\n",
    "    \n",
    "    examples: List[dict] = []\n",
    "    \n",
    "    for dataset_name, proportion in datasets_dict.items():\n",
    "        # Calculate number of posts for this dataset\n",
    "        target_count = int(total_posts * proportion)\n",
    "        print(f\"Loading {target_count} posts from {dataset_name} dataset ({proportion*100:.1f}%)\")\n",
    "        \n",
    "        # Load sampled Reddit posts from JSON created by sample-posts.py\n",
    "        # Each item is a dict with keys: title, subreddit, self_text\n",
    "        try:\n",
    "            with open(f\"../../datasets/{dataset_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                reddit_posts: List[dict] = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find dataset file for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter valid posts (must have self_text and no image_url)\n",
    "        valid_posts = []\n",
    "        for p in reddit_posts:\n",
    "            title = p.get(\"title\", \"\")\n",
    "            self_text = p.get(\"self_text\", \"\")\n",
    "            image_url = p.get(\"image_url\", \"\")\n",
    "            \n",
    "            if self_text and not image_url:\n",
    "                subreddit = p.get(\"subreddit\", \"\")\n",
    "                subreddit = re.sub(r\"\\s*(/)?r/\", \"r/\", subreddit)\n",
    "                post = f\"title: {title}\\nself_text: {self_text}\\nsubreddit: {subreddit}\"\n",
    "                valid_posts.append({\"instruction\": prompt, \"output\": post})\n",
    "        \n",
    "        print(f\"Found {len(valid_posts)} valid posts in {dataset_name}\")\n",
    "        \n",
    "        # Sample the target number of posts\n",
    "        if len(valid_posts) >= target_count:\n",
    "            # Randomly sample target_count posts\n",
    "            sampled_posts = random.sample(valid_posts, target_count)\n",
    "        else:\n",
    "            # Use all available posts if we don't have enough\n",
    "            print(f\"Warning: Only {len(valid_posts)} posts available, using all\")\n",
    "            sampled_posts = valid_posts\n",
    "        \n",
    "        examples.extend(sampled_posts)\n",
    "    \n",
    "    # Shuffle the final dataset to mix posts from different datasets\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    print(f\"Loaded dataset {datasets_dict} with {total_posts} posts\")\n",
    "    return examples\n",
    "\n",
    "# Example usage - modify these values as needed\n",
    "datasets_dict = {\n",
    "    \"ucla\": 0.5,  # 100% minecraft posts\n",
    "    \"minecraft\": 0.5,  \n",
    "}\n",
    "total_posts = 100  # Total number of posts desired\n",
    "\n",
    "examples = load_datasets_proportional(datasets_dict, total_posts)\n",
    "\n",
    "print(f\"Total number of examples loaded: {len(examples)}\")\n",
    "if examples:\n",
    "    print(\"Sample example:\")\n",
    "    print(examples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess instruction/output dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_dataset(examples, tokenizer):\n",
    "    # Build HF dataset from examples [{\"instruction\", \"output\"}]\n",
    "    dataset = Dataset.from_list(examples)\n",
    "\n",
    "    # Tokenize instruction with chat template, and supervise only the output tokens\n",
    "    def tokenize_io(sample):\n",
    "        # Build chat prompt prefix for the user instruction\n",
    "        messages = [{\"role\": \"user\", \"content\": sample[\"instruction\"]}]\n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        prompt_ids = tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"]\n",
    "        output_ids = tokenizer(sample[\"output\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "\n",
    "        input_ids = prompt_ids + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "        labels = ([-100] * len(prompt_ids)) + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Truncate from the left if too long, keeping alignment between inputs and labels\n",
    "        if len(input_ids) > MAX_SEQ_LEN:\n",
    "            input_ids = input_ids[-MAX_SEQ_LEN:]\n",
    "            labels = labels[-MAX_SEQ_LEN:]\n",
    "            attention_mask = attention_mask[-MAX_SEQ_LEN:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    train_ds = dataset.map(tokenize_io, remove_columns=dataset.column_names)\n",
    "    print(\"Preprocessed dataset...\")\n",
    "    return train_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a47b18",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef940c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import dotenv, os\n",
    "from peft import PeftModel\n",
    "\n",
    "def login():\n",
    "    dotenv.load_dotenv()\n",
    "    login(token=os.getenv(\"HUGGING_FACE_HUB_TOKEN\"))\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {model_name} model and tokenizer\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_peft_model(model_name: str, adapter_name: str):\n",
    "    model = load_model(model_name)\n",
    "    model = PeftModel.from_pretrained(model, adapter_name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad76af",
   "metadata": {},
   "source": [
    "## Soft Prompt Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through models, datasets, and dataset sizes to create soft prompt adapters for each \n",
    "# 4 * 8 = 32 soft prompts\n",
    "# 2 * 8 = 16 adapters\n",
    "# take note of training time, adapter size\n",
    "\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train_soft_prompt(model, tokenizer, train_ds, train_steps, output_dir):\n",
    "    def collate_fn(features):\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        batch_size = len(features)\n",
    "        seq_lens = [len(f[\"input_ids\"]) for f in features]\n",
    "        max_len = max(seq_lens)\n",
    "\n",
    "        input_ids = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "        labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "\n",
    "        for i, f in enumerate(features):\n",
    "            ids = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
    "            attn = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
    "            labs = torch.tensor(f[\"labels\"], dtype=torch.long)\n",
    "            L = ids.size(0)\n",
    "            input_ids[i, :L] = ids\n",
    "            attention_mask[i, :L] = attn\n",
    "            labels[i, :L] = labs\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=MICRO_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    # Total optimizer steps we intend to take\n",
    "    total_optim_steps = train_steps\n",
    "    num_warmup_steps = max(1, int(0.1 * total_optim_steps))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=total_optim_steps,\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model_device = device\n",
    "\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optim_step = 0\n",
    "    accumulated = 0\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(10):  # repeat over dataset until reaching desired steps\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(model_device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            (loss / GRAD_ACCUM_STEPS).backward()\n",
    "            running_loss += loss.item()\n",
    "            accumulated += 1\n",
    "            if accumulated % GRAD_ACCUM_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                if optim_step % 10 == 0:\n",
    "                    print(f\"step {optim_step} loss {running_loss / GRAD_ACCUM_STEPS:.4f}\")\n",
    "                running_loss = 0.0\n",
    "                optim_step += 1\n",
    "                if optim_step >= total_optim_steps:\n",
    "                    break\n",
    "        if optim_step >= total_optim_steps:\n",
    "            break\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    print(\"Saved prompt adapter to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d588b",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through models, datasets \n",
    "# 2 * 8 = 16 LORA fine tuned models\n",
    "# take note of training time, model size\n",
    "\n",
    "def train_fine_tune_lora(model, tokenizer, train_ds, train_steps, output_dir):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66d8d1",
   "metadata": {},
   "source": [
    "## Generate posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d20ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 posts per cell \n",
    "# look into inference parallelism\n",
    "# iterate through models, experiments, datasets\n",
    "# write each generated post to json file indicating model, experiment, dataset\n",
    "\n",
    "def generate_post(prompt, model, tokenizer, dataset, num_posts):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_SEQ_LEN,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    print(\"Generated post\")\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41960fb",
   "metadata": {},
   "source": [
    "## Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge whether each generated post adheres to dataset category, heuristic based on word content and llm judge\n",
    "\n",
    "def judge_post_gpt5(post, dataset):\n",
    "    JUDGE_PROMPTS = {\n",
    "        \"nerdy\": \"Please judge whether the following post is nerdy. \\n\\npost: {post}\\n\\n\",\n",
    "        \"personal\": \"Please judge whether the following post is personal. \\n\\npost: {post}\\n\\n\",\n",
    "        \"alike 3\": \"Please judge whether the following post is similar to the 3 posts below. \\n\\npost: {post}\\n\\n\",\n",
    "        \"unalike 3\": \"Please judge whether the following post is not similar to the 3 posts below. \\n\\npost: {post}\\n\\n\",\n",
    "        \"format specific\": \"Please judge whether the following post is formatted correctly. \\n\\npost: {post}\\n\\n\",\n",
    "        \"college student\": \"Please judge whether the following post is college student. \\n\\npost: {post}\\n\\n\",\n",
    "        \"new mother\": \"Please judge whether the following post is new mother. \\n\\npost: {post}\\n\\n\",\n",
    "        \"creative gen alpha\": \"Please judge whether the following post is creative gen alpha. \\n\\npost: {post}\\n\\n\",\n",
    "    }\n",
    "    \n",
    "\n",
    "def judge_post_heuristic(post, dataset):\n",
    "    KEYWORDS = {\n",
    "        \"nerdy\": [],\n",
    "        \"personal\": [],\n",
    "        \"alike 3\": [],\n",
    "        \"unalike 3\": [],\n",
    "        \"format specific\": [],\n",
    "        \"college student\": [],\n",
    "        \"new mother\": [],\n",
    "        \"creative gen alpha\": [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad1c6b",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "\n",
    "add progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdddbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-27b-it\",\n",
    "    \"gpt-5\"\n",
    "]\n",
    "\n",
    "# Load dataset categories from JSON file\n",
    "DATASETS1 = [\n",
    "    \"minecraft\",\n",
    "    \"ucla\", \n",
    "    \"nostupidquestions\",\n",
    "    \"copypasta\",\n",
    "]\n",
    "\n",
    "DATASETS2 = [\n",
    "    \"nerdy\",\n",
    "    \"personal\", \n",
    "    \"alike 3\",\n",
    "    \"unalike 3\",\n",
    "    \"format specific\",\n",
    "    \"college student\",\n",
    "    \"new mother\",\n",
    "    \"creative gen alpha\"\n",
    "]\n",
    "\n",
    "TRAIN_SIZES = [\n",
    "    10, 20, 50, 100, 250, 500, 1000, 5000\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    \"self defined\",\n",
    "    \"summary\",\n",
    "    \"like history\",\n",
    "    \"fine tune\",\n",
    "    \"soft prompt\",\n",
    "]\n",
    "\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 0.2\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"models\"\n",
    "GENERATED_OUTPUT_DIR = \"generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"self defined\": {\n",
    "        # user defines their own interests. e.g. a bio of interests\n",
    "        \"nerdy\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"personal\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\", \n",
    "        \"alike 3\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"unalike 3\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"format specific\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"college student\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"new mother\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"creative gen alpha\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "    },\n",
    "    \"fine tune\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "    \"soft prompt\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summarized_prompt(dataset):\n",
    "    return \"\"\n",
    "\n",
    "def generate_like_history_prompt(dataset):\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd32687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training size ablation\n",
    "for model_name in MODELS:\n",
    "    for dataset in DATASETS1:\n",
    "        for train_size in TRAIN_SIZES:\n",
    "            train_ds = load_datasets_proportional(dataset, 1, train_size)\n",
    "            \n",
    "            model, tokenizer = load_model(model_name)\n",
    "            \n",
    "            soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/soft_prompt_{model}_{dataset}_{train_size}\"    \n",
    "            train_soft_prompt(model, tokenizer, train_ds, NUM_TRAIN_STEPS, soft_output_dir)\n",
    "            \n",
    "            peft_model = load_peft_model(model, soft_output_dir)\n",
    "            \n",
    "            # generate posts\n",
    "            with open(f\"{GENERATED_OUTPUT_DIR}/train_size_ablation/{model_name}_{dataset}_{train_size}.json\", \"w\") as f:\n",
    "                for _ in range(100):\n",
    "                    post = generate_post(PROMPTS[0], peft_model, tokenizer, dataset, train_size)\n",
    "                    f.write(post + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment ablation\n",
    "for model_name in MODELS:\n",
    "    for dataset in DATASETS2:\n",
    "        for experiment in EXPERIMENTS:\n",
    "            train_ds = load_datasets_proportional(dataset, 1, 100)\n",
    "        \n",
    "            model, tokenizer = load_model(model_name)\n",
    "            \n",
    "            prompt = PROMPTS[0]\n",
    "            if experiment == \"soft prompt\":\n",
    "                soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/soft_prompt_{model}_{dataset}_{train_size}\"    \n",
    "                train_soft_prompt(model, tokenizer, train_ds, NUM_TRAIN_STEPS, soft_output_dir)\n",
    "                \n",
    "                model = load_peft_model(model, soft_output_dir)\n",
    "            elif experiment == \"fine tune\":\n",
    "                pass\n",
    "            elif experiment == \"summary\":\n",
    "                prompt = PROMPTS[1]\n",
    "            elif experiment == \"like history\":\n",
    "                prompt = PROMPTS[2]\n",
    "            elif experiment == \"self defined\":\n",
    "                prompt = PROMPTS[3]\n",
    "        \n",
    "            with open(f\"{GENERATED_OUTPUT_DIR}/experiment_ablation/{model_name}_{dataset}_{experiment}.json\", \"w\") as f:\n",
    "                for _ in range(100):\n",
    "                    post = generate_post(prompt, model, tokenizer, dataset, train_size)\n",
    "                    f.write(post + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judging \n",
    "for model_name in MODELS:\n",
    "    for dataset in DATASETS1:\n",
    "        for train_size in TRAIN_SIZES:\n",
    "            with open(f\"{GENERATED_OUTPUT_DIR}/train_size_ablation/{model_name}_{dataset}_{train_size}.json\", \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i in range(0, len(lines), 3):\n",
    "                    post_lines = lines[i:i+3]\n",
    "                    post = ''.join(line.strip() for line in post_lines)\n",
    "                    if post:\n",
    "                        gpt_judgement = judge_post_gpt5(post, dataset)\n",
    "                        heuristic_judgement = judge_post_heuristic(post, dataset)\n",
    "                        print(post)\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judging \n",
    "for model_name in MODELS:   \n",
    "    for dataset in DATASETS2:\n",
    "        for experiment in EXPERIMENTS:\n",
    "            with open(f\"{GENERATED_OUTPUT_DIR}/experiment_ablation/{model_name}_{dataset}_{experiment}.json\", \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i in range(0, len(lines), 3):\n",
    "                    post_lines = lines[i:i+3]\n",
    "                    post = ''.join(line.strip() for line in post_lines)\n",
    "                    if post:\n",
    "                        gpt_judgement = judge_post_gpt5(post, dataset)\n",
    "                        heuristic_judgement = judge_post_heuristic(post, dataset)\n",
    "                        print(post)\n",
    "                        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
