{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8a0981",
   "metadata": {},
   "source": [
    "# Ablation Study Results\n",
    "\n",
    "Axes\n",
    "- model (gemma-3-4b-it, gemma-3-27b-it, gpt-5)\n",
    "- manual vs judge\n",
    "- experiment (base, subreddit, summary, liked posts list, fine tuned, soft prompt (100), soft prompt(500))\n",
    "- dataset (circlejerk, jokes + puns, gaming, animals, sports, etc.)\n",
    "\n",
    "Dataset selection:\n",
    "- small\n",
    "    - okbuddy\n",
    "    - boomerhumor\n",
    "    - animals\n",
    "    - creative\n",
    "    - food\n",
    "    - religion\n",
    "- medium\n",
    "    - finance\n",
    "    - school\n",
    "    - pop\n",
    "- varied but focused: \n",
    "    - nerdy\n",
    "    - personal\n",
    "    - ucla\n",
    "    - tech\n",
    "    - school\n",
    "    - ucla\n",
    "- ultra specific: \n",
    "    - minecraft\n",
    "    - nba\n",
    "- format specific:\n",
    "    - copypasta,\n",
    "    - no stupid questions\n",
    "    - am i the asshole\n",
    "- 3 test (unalike)\n",
    "    - pop\n",
    "    - religion\n",
    "    - tech\n",
    "- 3 test (alike)\n",
    "    - tech\n",
    "    - nerdy\n",
    "    - finance\n",
    "- college student\n",
    "    - ucla\n",
    "    - nerdy\n",
    "    - okbuddy\n",
    "    - copypasta\n",
    "    - pop\n",
    "    - food\n",
    "    - animals\n",
    "- new mother\n",
    "    - pregnancy\n",
    "    - parenting\n",
    "    - baby\n",
    "    - food\n",
    "    - am i the asshole\n",
    "    - pop\n",
    "    - boomerhumor\n",
    "- creative gen alpha\n",
    "    - minecraft\n",
    "    - creative\n",
    "    - food\n",
    "    - school\n",
    "    - nba\n",
    "\n",
    "### gemma-3-4b-it\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n",
    "\n",
    "### gemma-3-27b-it\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n",
    "\n",
    "### gpt-5\n",
    "|                   | Self Defined | Summary | Like History | Fine Tune | Soft Prompt (100) | Soft Prompt (500) |\n",
    "|-------------------|-----------|---------|--------------|-----------|-------------------|-------------------|\n",
    "| Circlejerk        |           |         |              |           |                   |                   |\n",
    "| Jokes             |           |         |              |           |                   |                   |\n",
    "| Gaming            |           |         |              |           |                   |                   |\n",
    "| Animals           |           |         |              |           |                   |                   |\n",
    "| Personal          |           |         |              |           |                   |                   |\n",
    "| Personal + Gaming |           |         |              |           |                   |                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2e1e5",
   "metadata": {},
   "source": [
    "## Notes during testing\n",
    "- very good alignment on tech (92)\n",
    "- very good alignment on personal (180)\n",
    "- very good alignment on nerdy (204)\n",
    "- very good alignmenton school (93)\n",
    "- bad alignment on interesting (35)\n",
    "- bad alignment on finance (57)\n",
    "- decent alignment on interesting + finance (on the finance side) (92)\n",
    "- for hyper targeted like minecraft, even 10 examples is good enough for alignment, but post topic variety goes down. ~100 is best to balance quality and training time\n",
    "- 50/50 minecraft + ucla works REALLY well\n",
    "\n",
    "it seems like ~100 samples is good enough for gemma 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae67d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers peft accelerate datasets trl einops sentencepiece bitsandbytes jinja2>=3.1.0 dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf9bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"gpt-5\"\n",
    "]\n",
    "\n",
    "# Load dataset categories from JSON file\n",
    "DATASETS1 = [\n",
    "    {\n",
    "    \"minecraft\": 1,  \n",
    "    },\n",
    "    {\n",
    "    \"ucla\": 1,  \n",
    "    },\n",
    "    {\n",
    "    \"nostupidquestions\": 1,  \n",
    "    },\n",
    "    {\n",
    "    \"copypasta\": 1,  \n",
    "    },\n",
    "]\n",
    "\n",
    "DATASETS2 = [\n",
    "    {\n",
    "    \"nerdy\": 1,  \n",
    "    },\n",
    "    {\n",
    "    \"personal\": 1,  \n",
    "    },\n",
    "    { # unalike\n",
    "    \"pop\": 1,  \n",
    "    \"religion\": 1,\n",
    "    \"tech\": 1\n",
    "    },\n",
    "    { # alike\n",
    "        \"tech\": 1,\n",
    "        \"nerdy\": 1,\n",
    "        \"finance\": 1,\n",
    "    },\n",
    "    { # format specific\n",
    "        \"copypasta\": 1,\n",
    "        \"nostupidquestions\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "    },\n",
    "    { # college student\n",
    "        \"ucla\": 1,\n",
    "        \"nerdy\": 1,\n",
    "        \"okbuddy\": 1,\n",
    "        \"copypasta\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"food\": 1,\n",
    "        \"animals\": 1,\n",
    "    },\n",
    "    { # new mother\n",
    "        \"pregnancy\": 1,\n",
    "        \"parenting\": 1,\n",
    "        \"baby\": 1,\n",
    "        \"food\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"boomerhumor\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "TRAIN_SIZES = [\n",
    "    10, 20, 50, 100, 250, 500, 1000, 2000\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    \"self defined\",\n",
    "    \"summary\",\n",
    "    \"like history\",\n",
    "    \"fine tune\",\n",
    "    \"soft prompt\",\n",
    "]\n",
    "\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 0.2\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"models\"\n",
    "GENERATED_OUTPUT_DIR = \"generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dabf1f",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f1bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a generator to lazily load dataset from json file based on dataset argument\n",
    "\n",
    "\"\"\"\n",
    "choice for sample ablation:\n",
    "    - make sure to increase max number of training steps as you go\n",
    "    - do some testing beforehand for 1000, 5000 to find good number of steps\n",
    "    - 10, 20, 50, 100, 250, 500, 1000, 5000\n",
    "    - minecraft\n",
    "    - ucla\n",
    "    - nostupidquestions   \n",
    "    - copypasta\n",
    "    - 4 * 8 = 32 soft prompts\n",
    "    \n",
    "choice (for 100 samples, or optimal from above):\n",
    "    - nerdy\n",
    "    - personal \n",
    "    - alike 3\n",
    "    - unalike 3\n",
    "    - format specific\n",
    "    - college student \n",
    "    - new mother \n",
    "    - creative gen alpha \n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import re\n",
    "import random, math\n",
    "\n",
    "def load_datasets_proportional(datasets_dict: Dict[str, float], total_posts: int, prompt: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load datasets with proportional sampling.\n",
    "    \n",
    "    Args:\n",
    "        datasets_dict: Dictionary mapping dataset names to their proportions, e.g. {\"minecraft\": 1, \"ucla\": 1} will load half minecraft, half ucla\n",
    "        total_posts: Total number of posts desired across all datasets\n",
    "    \n",
    "    Returns:\n",
    "        List of examples in the format: {\"instruction\": PROMPT, \"output\": post}\n",
    "    \"\"\"\n",
    "    \n",
    "    examples: List[dict] = []\n",
    "    \n",
    "    # Get total of all values in datasets_dict\n",
    "    total_proportion = sum(datasets_dict.values())\n",
    "    for dataset_name, proportion in datasets_dict.items():\n",
    "        # Calculate number of posts for this dataset\n",
    "        factor = proportion / total_proportion\n",
    "        target_count = math.ceil(total_posts * factor)\n",
    "        print(f\"Loading {target_count} posts from {dataset_name} dataset ({factor*100:.1f}%)\")\n",
    "        \n",
    "        # Load sampled Reddit posts from JSON created by sample-posts.py\n",
    "        # Each item is a dict with keys: title, subreddit, self_text\n",
    "        try:\n",
    "            with open(f\"../../datasets/{dataset_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                reddit_posts: List[dict] = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find dataset file for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter valid posts (must have self_text and no image_url)\n",
    "        valid_posts = []\n",
    "        for p in reddit_posts:\n",
    "            title = p.get(\"title\", \"\")\n",
    "            self_text = p.get(\"self_text\", \"\")\n",
    "            image_url = p.get(\"image_url\", \"\")\n",
    "            \n",
    "            if self_text and not image_url:\n",
    "                subreddit = p.get(\"subreddit\", \"\")\n",
    "                subreddit = re.sub(r\"\\s*(/)?r/\", \"r/\", subreddit)\n",
    "                post = f\"title: {title}\\nself_text: {self_text}\\nsubreddit: {subreddit}\"\n",
    "                valid_posts.append({\"instruction\": prompt, \"output\": post})\n",
    "        \n",
    "        print(f\"Found {len(valid_posts)} valid posts in {dataset_name}\")\n",
    "        \n",
    "        # Sample the target number of posts\n",
    "        if len(valid_posts) >= target_count:\n",
    "            # Randomly sample target_count posts\n",
    "            sampled_posts = random.sample(valid_posts, target_count)\n",
    "        else:\n",
    "            # Use all available posts if we don't have enough\n",
    "            print(f\"Warning: Only {len(valid_posts)} posts available, using all\")\n",
    "            sampled_posts = valid_posts\n",
    "        \n",
    "        examples.extend(sampled_posts)\n",
    "    \n",
    "    # Shuffle the final dataset to mix posts from different datasets\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    print(f\"Loaded dataset {datasets_dict} with {total_posts} posts\")\n",
    "    return examples\n",
    "\n",
    "# Example usage - modify these values as needed\n",
    "# datasets_dict = {\n",
    "#     \"ucla\": 0.5,  # 100% minecraft posts\n",
    "#     \"minecraft\": 0.5,  \n",
    "# }\n",
    "# total_posts = 100  # Total number of posts desired\n",
    "\n",
    "# examples = load_datasets_proportional(datasets_dict, total_posts, \"prompt\")\n",
    "\n",
    "# print(f\"Total number of examples loaded: {len(examples)}\")\n",
    "# if examples:\n",
    "#     print(\"Sample example:\")\n",
    "#     print(examples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b9c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess instruction/output dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_dataset(examples, tokenizer):\n",
    "    # Build HF dataset from examples [{\"instruction\", \"output\"}]\n",
    "    dataset = Dataset.from_list(examples)\n",
    "\n",
    "    # Tokenize instruction with chat template, and supervise only the output tokens\n",
    "    def tokenize_io(sample):\n",
    "        # Build chat prompt prefix for the user instruction\n",
    "        messages = [{\"role\": \"user\", \"content\": sample[\"instruction\"]}]\n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        prompt_ids = tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"]\n",
    "        output_ids = tokenizer(sample[\"output\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "\n",
    "        input_ids = prompt_ids + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "        labels = ([-100] * len(prompt_ids)) + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Truncate from the left if too long, keeping alignment between inputs and labels\n",
    "        if len(input_ids) > MAX_SEQ_LEN:\n",
    "            input_ids = input_ids[-MAX_SEQ_LEN:]\n",
    "            labels = labels[-MAX_SEQ_LEN:]\n",
    "            attention_mask = attention_mask[-MAX_SEQ_LEN:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    train_ds = dataset.map(tokenize_io, remove_columns=dataset.column_names)\n",
    "    print(\"Preprocessed dataset...\")\n",
    "    return train_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a47b18",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef940c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login as huggingface_login\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import dotenv, os\n",
    "from peft import PeftModel, PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
    "\n",
    "def login():\n",
    "    dotenv.load_dotenv()\n",
    "    huggingface_login(token=os.getenv(\"HUGGING_FACE_HUB_TOKEN\"))\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {model_name} model and tokenizer\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def init_peft_model(model, model_name: str):\n",
    "    config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=PROMPT_TOKENS,\n",
    "        prompt_tuning_init_text=\"Generate a reddit post.\",\n",
    "        tokenizer_name_or_path=model_name,\n",
    "    )\n",
    "    return get_peft_model(model, config)\n",
    "\n",
    "def apply_peft_adapter(base_model, adapter_name: str):\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_name)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad76af",
   "metadata": {},
   "source": [
    "## Soft Prompt Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fb50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through models, datasets, and dataset sizes to create soft prompt adapters for each \n",
    "# 4 * 8 = 32 soft prompts\n",
    "# 2 * 8 = 16 adapters\n",
    "# take note of training time, adapter size\n",
    "\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train_soft_prompt(model, tokenizer, train_ds, train_steps, output_dir):\n",
    "    def collate_fn(features):\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        batch_size = len(features)\n",
    "        seq_lens = [len(f[\"input_ids\"]) for f in features]\n",
    "        max_len = max(seq_lens)\n",
    "\n",
    "        input_ids = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "        labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "\n",
    "        for i, f in enumerate(features):\n",
    "            ids = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
    "            attn = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
    "            labs = torch.tensor(f[\"labels\"], dtype=torch.long)\n",
    "            L = ids.size(0)\n",
    "            input_ids[i, :L] = ids\n",
    "            attention_mask[i, :L] = attn\n",
    "            labels[i, :L] = labs\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=MICRO_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    # Total optimizer steps we intend to take\n",
    "    total_optim_steps = train_steps\n",
    "    num_warmup_steps = max(1, int(0.1 * total_optim_steps))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=total_optim_steps,\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model_device = device\n",
    "\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optim_step = 0\n",
    "    accumulated = 0\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(10):  # repeat over dataset until reaching desired steps\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(model_device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            (loss / GRAD_ACCUM_STEPS).backward()\n",
    "            running_loss += loss.item()\n",
    "            accumulated += 1\n",
    "            if accumulated % GRAD_ACCUM_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                if optim_step % 10 == 0:\n",
    "                    print(f\"step {optim_step} loss {running_loss / GRAD_ACCUM_STEPS:.4f}\")\n",
    "                running_loss = 0.0\n",
    "                optim_step += 1\n",
    "                if optim_step >= total_optim_steps:\n",
    "                    break\n",
    "        if optim_step >= total_optim_steps:\n",
    "            break\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    print(\"Saved prompt adapter to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d588b",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through models, datasets \n",
    "# 2 * 8 = 16 LORA fine tuned models\n",
    "# take note of training time, model size\n",
    "\n",
    "def train_fine_tune_lora(model, tokenizer, train_ds, train_steps, output_dir):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66d8d1",
   "metadata": {},
   "source": [
    "## Generate posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d20ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 posts per cell \n",
    "# look into inference parallelism\n",
    "# iterate through models, experiments, datasets\n",
    "# write each generated post to json file indicating model, experiment, dataset\n",
    "\n",
    "def generate_post(prompt, model, tokenizer):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_SEQ_LEN,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    # Extract the generated text and parse the post format\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    \n",
    "    # Initialize the post object\n",
    "    post_obj = {\n",
    "        \"title\": \"\",\n",
    "        \"self_text\": \"\",\n",
    "        \"subreddit\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Split by lines and parse each field\n",
    "    lines = generated_text.strip().split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"title: \"):\n",
    "            post_obj[\"title\"] = line[7:]  # Remove \"title: \" prefix\n",
    "        elif line.startswith(\"self_text: \"):\n",
    "            post_obj[\"self_text\"] = line[11:]  # Remove \"self_text: \" prefix\n",
    "        elif line.startswith(\"subreddit: \"):\n",
    "            post_obj[\"subreddit\"] = line[11:]  # Remove \"subreddit: \" prefix\n",
    "    \n",
    "    return post_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41960fb",
   "metadata": {},
   "source": [
    "## Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ddc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge whether each generated post adheres to dataset category, heuristic based on word content and llm judge\n",
    "\n",
    "def judge_post_gpt5(post, dataset):\n",
    "    JUDGE_PROMPTS = {\n",
    "        \"nerdy\": \"Please judge whether the following post is nerdy. \\n\\npost: {post}\\n\\n\",\n",
    "        \"personal\": \"Please judge whether the following post is personal. \\n\\npost: {post}\\n\\n\",\n",
    "        \"alike 3\": \"Please judge whether the following post is similar to the 3 posts below. \\n\\npost: {post}\\n\\n\",\n",
    "        \"unalike 3\": \"Please judge whether the following post is not similar to the 3 posts below. \\n\\npost: {post}\\n\\n\",\n",
    "        \"format specific\": \"Please judge whether the following post is formatted correctly. \\n\\npost: {post}\\n\\n\",\n",
    "        \"college student\": \"Please judge whether the following post is college student. \\n\\npost: {post}\\n\\n\",\n",
    "        \"new mother\": \"Please judge whether the following post is new mother. \\n\\npost: {post}\\n\\n\",\n",
    "        \"creative gen alpha\": \"Please judge whether the following post is creative gen alpha. \\n\\npost: {post}\\n\\n\",\n",
    "    }\n",
    "    \n",
    "\n",
    "def judge_post_heuristic(post, dataset):\n",
    "    KEYWORDS = {\n",
    "        \"nerdy\": [],\n",
    "        \"personal\": [],\n",
    "        \"alike 3\": [],\n",
    "        \"unalike 3\": [],\n",
    "        \"format specific\": [],\n",
    "        \"college student\": [],\n",
    "        \"new mother\": [],\n",
    "        \"creative gen alpha\": [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad1c6b",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d853c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON output helpers\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "def ensure_parent_dir(path_str: str) -> Path:\n",
    "    path = Path(path_str)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def collect_posts(n: int, prompt: str, model, tokenizer) -> list:\n",
    "    posts = []\n",
    "    for i in tqdm(range(n), desc=\"Generating posts\"):\n",
    "        post = generate_post(prompt, model, tokenizer)\n",
    "        # print(f\"generated post {i}\")\n",
    "        posts.append(post)\n",
    "    return posts\n",
    "\n",
    "def write_posts_json(path_str: str, posts: list) -> None:\n",
    "    path = ensure_parent_dir(path_str)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdddbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"gpt-5\"\n",
    "]\n",
    "\n",
    "\n",
    "# Load dataset categories from JSON file\n",
    "DATASET1_NAMES = [\n",
    "    \"minecraft\",\n",
    "    \"ucla\",\n",
    "    \"nostupidquestions\",\n",
    "    \"copypasta\",\n",
    "    \"varietypack\",\n",
    "]\n",
    "DATASETS1 = [\n",
    "    {\n",
    "        \"minecraft\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"ucla\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"nostupidquestions\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"copypasta\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"nerdy\": 1,  \n",
    "        \"personal\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "        \"tech\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"animals\": 1, \n",
    "        \"boomerhumor\": 1,\n",
    "        \"copypasta\": 1,\n",
    "        \"creative\": 1,\n",
    "        \"food\": 1,\n",
    "        \"nba\": 1,\n",
    "        \"religion\": 1,\n",
    "        \"school\": 1,\n",
    "        \"ucla\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "DATASET2_NAMES = [\n",
    "    \"nerdy\",\n",
    "    \"personal\",\n",
    "    \"unalike\",\n",
    "    \"alike\",\n",
    "    \"formatspecific\",\n",
    "    \"college\",\n",
    "    \"newmother\",\n",
    "]\n",
    "DATASETS2 = [\n",
    "    {\n",
    "        \"nerdy\": 1,  \n",
    "    },\n",
    "    {\n",
    "        \"personal\": 1,  \n",
    "    },\n",
    "    { # unalike\n",
    "        \"pop\": 1,  \n",
    "        \"religion\": 1,\n",
    "        \"tech\": 1\n",
    "    },\n",
    "    { # alike\n",
    "        \"tech\": 1,\n",
    "        \"nerdy\": 1,\n",
    "        \"finance\": 1,\n",
    "    },\n",
    "    { # format specific\n",
    "        \"copypasta\": 1,\n",
    "        \"nostupidquestions\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "    },\n",
    "    { # college student\n",
    "        \"ucla\": 1,\n",
    "        \"nerdy\": 1,\n",
    "        \"okbuddy\": 1,\n",
    "        \"copypasta\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"food\": 1,\n",
    "        \"animals\": 1,\n",
    "    },\n",
    "    { # new mother\n",
    "        \"pregnancy\": 1,\n",
    "        \"parenting\": 1,\n",
    "        \"baby\": 1,\n",
    "        \"food\": 1,\n",
    "        \"amitheasshole\": 1,\n",
    "        \"pop\": 1,\n",
    "        \"boomerhumor\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "TRAIN_SIZES = [\n",
    "    10, 20, 50, 100, 250, 500, 1000,2000\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    \"self defined\",\n",
    "    \"summary\",\n",
    "    \"like history\",\n",
    "    \"fine tune\",\n",
    "    \"soft prompt\",\n",
    "]\n",
    "\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 0.2\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"models\"\n",
    "GENERATED_OUTPUT_DIR = \"generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6006704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"self defined\": {\n",
    "        # user defines their own interests. e.g. a bio of interests\n",
    "        \"nerdy\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"personal\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\", \n",
    "        \"alike 3\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"unalike 3\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"format specific\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"college student\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"new mother\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "        \"creative gen alpha\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "    },\n",
    "    \"fine tune\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "    \"soft prompt\": \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c396dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summarized_prompt(dataset):\n",
    "    return \"\"\n",
    "\n",
    "def generate_like_history_prompt(dataset):\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f9a65",
   "metadata": {},
   "source": [
    "### testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a9f9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # single test\n",
    "\n",
    "# model_name = MODELS[0]\n",
    "# dataset_name = DATASET1_NAMES[0]\n",
    "# dataset_dict = DATASETS1[0]\n",
    "# train_size = TRAIN_SIZES[3]\n",
    "# print(model_name, dataset_name, train_size)\n",
    "\n",
    "# # load data + model\n",
    "# login()\n",
    "# model, tokenizer = load_model(model_name)\n",
    "# examples = load_datasets_proportional(dataset_dict, train_size, PROMPTS[\"soft prompt\"]) \n",
    "# train_ds = preprocess_dataset(examples, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ef0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/soft_prompt_{model_name}_{dataset_name}_{train_size}\" \n",
    "\n",
    "# # initialize peft model\n",
    "# peft_model = init_peft_model(model, model_name)\n",
    "\n",
    "# # train soft prompt\n",
    "# train_soft_prompt(peft_model, tokenizer, train_ds, NUM_TRAIN_STEPS, soft_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4e54cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate posts\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/soft_prompt_{model_name}_{dataset_name}_{train_size}\" \n",
    "\n",
    "# # load peft model\n",
    "# peft_model = apply_peft_adapter(model, soft_output_dir)\n",
    "\n",
    "# out_path = Path(f\"{GENERATED_OUTPUT_DIR}/train_size_ablation/{model_name}_{dataset_name}_{train_size}.json\")\n",
    "# out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# posts = collect_posts(5, PROMPTS[\"soft prompt\"], peft_model, tokenizer)\n",
    "# write_posts_json(out_path, posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c7088",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffd32687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a499d133dbd46b48e4b5759d28e493d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b327b52e082248f1906395b18ddf60e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a83731e290423db0d9a39d4b349165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f0570bc32443b78fb7b7f1e84504a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1e5227855b4ceeb42ede7ddea72f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b9eab834f74bd8a7f5e8c75471ef69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/gemma-3-4b-it model and tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bbad57cf274f218f3c5657d4883ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f7932a9aa64995b91a3bce7d3dce36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/minecraft/10 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/minecraft/10.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/minecraft/20 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/minecraft/20.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/minecraft/50 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/minecraft/50.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/minecraft/100 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/minecraft/100.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/minecraft/250 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/minecraft/250.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/minecraft/500 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/minecraft/500.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/minecraft/1000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/minecraft/1000.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/minecraft/2000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/minecraft/2000.json already exists with posts, skipping generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14a905bed2e40fa9df4bd7f103bebe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/ucla/10 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/ucla/10.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/ucla/20 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/ucla/20.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/ucla/50 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/ucla/50.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/ucla/100 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/ucla/100.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/ucla/250 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/ucla/250.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/ucla/500 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/ucla/500.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/ucla/1000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/ucla/1000.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/ucla/2000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/ucla/2000.json already exists with posts, skipping generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a2c1a85c0b40078aaf2712a5659b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/nostupidquestions/10 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/nostupidquestions/10.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/nostupidquestions/20 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/nostupidquestions/20.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/nostupidquestions/50 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/nostupidquestions/50.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/nostupidquestions/100 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/nostupidquestions/100.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/nostupidquestions/250 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/nostupidquestions/250.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/nostupidquestions/500 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/nostupidquestions/500.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/nostupidquestions/1000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/nostupidquestions/1000.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/nostupidquestions/2000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/nostupidquestions/2000.json already exists with posts, skipping generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd4be8eca91426dab0e197d0e4c89a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/copypasta/10 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/copypasta/10.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/copypasta/20 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/copypasta/20.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/copypasta/50 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/copypasta/50.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/copypasta/100 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/copypasta/100.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/copypasta/250 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/copypasta/250.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/copypasta/500 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/copypasta/500.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/copypasta/1000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/copypasta/1000.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/copypasta/2000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/copypasta/2000.json already exists with posts, skipping generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b01a7994bde420ca0f3bae58e354f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/varietypack/10 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/varietypack/10.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/varietypack/20 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/varietypack/20.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/varietypack/50 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/varietypack/50.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/varietypack/100 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/varietypack/100.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/varietypack/250 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/varietypack/250.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/varietypack/500 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/varietypack/500.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/varietypack/1000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/varietypack/1000.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-4b-it/varietypack/2000 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-4b-it/varietypack/2000.json already exists with posts, skipping generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275e54311b824220b4b2b1170b4cc823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a54a44029149549fe3be874cdd3134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6f49532cd244b6bb8e7c2d703f48d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20218a89bec4cc78b15916ebfbed6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c96b36e2644028995942e3f0bec31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d173ae2d2db14310aeb782d63644107d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/972 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae9397a87e84acb8b5b958bc7916ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8a3d31064e467bab76a23fdd7b3c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed0e52be7ef47d3ade9ab815c397d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00012.safetensors:   0%|          | 0.00/4.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d47d8d714042dc8c65dea9ca295d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754d4e2ac2084350a65518af01098721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784365b94e0e45dfa36b724561c9092e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e480e866bcc94e04b219635879cb6982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d90ed9602794b74a91983aefebf45e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8c8b10bfa643dd8134ba5b9b526664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b711a09ef304f5cae296893add695c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8552266f5124527bced5ebfe074f16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556dbb69b9224af5a146a0b28f32de17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a7be9c17584035a053f228aef43e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00012.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5273b316e749ddabb77ae6314d62ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00012.safetensors:   0%|          | 0.00/462M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f810caefa2674b7cb368d31e1dbee91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0212b98d4841b98f420b80079a3a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/gemma-3-27b-it model and tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b63bec748bf4055afdd296f565b4100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06a0c748c714fdeab8618438ab52e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft prompt models/soft_prompts/google/gemma-3-27b-it/minecraft/10 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-27b-it/minecraft/10.json already exists with posts, skipping generation\n",
      "Soft prompt models/soft_prompts/google/gemma-3-27b-it/minecraft/20 exists, skipping training\n",
      "Output file generated/train_size_ablation/google/gemma-3-27b-it/minecraft/20.json already exists with posts, skipping generation\n",
      "Loading 50 posts from minecraft dataset (100.0%)\n",
      "Found 2000 valid posts in minecraft\n",
      "Loaded dataset {'minecraft': 1} with 50 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d0f00465634b7382eef032fb7573d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 5.8073\n",
      "step 10 loss 4.0967\n",
      "step 20 loss 4.0284\n",
      "step 30 loss 3.8912\n",
      "step 40 loss 3.7989\n",
      "step 50 loss 2.5073\n",
      "step 60 loss 4.2722\n",
      "step 70 loss 2.4161\n",
      "step 80 loss 2.3146\n",
      "step 90 loss 2.6515\n",
      "step 100 loss 2.2406\n",
      "step 110 loss 1.7969\n",
      "step 120 loss 2.2540\n",
      "step 130 loss 1.5700\n",
      "step 140 loss 2.1576\n",
      "step 150 loss 1.6297\n",
      "step 160 loss 2.4568\n",
      "step 170 loss 2.0251\n",
      "step 180 loss 2.2020\n",
      "step 190 loss 2.0891\n",
      "step 200 loss 2.7134\n",
      "step 210 loss 2.3844\n",
      "step 220 loss 2.0733\n",
      "step 230 loss 1.7907\n",
      "step 240 loss 2.3728\n",
      "step 250 loss 2.1218\n",
      "step 260 loss 2.3429\n",
      "step 270 loss 2.5956\n",
      "step 280 loss 2.0929\n",
      "step 290 loss 2.9024\n",
      "step 300 loss 1.9436\n",
      "step 310 loss 2.2734\n",
      "step 320 loss 1.6262\n",
      "step 330 loss 2.1235\n",
      "step 340 loss 2.2862\n",
      "step 350 loss 1.7943\n",
      "step 360 loss 2.0672\n",
      "step 370 loss 2.3323\n",
      "step 380 loss 2.0745\n",
      "step 390 loss 2.7223\n",
      "step 400 loss 2.0376\n",
      "step 410 loss 2.4616\n",
      "step 420 loss 2.3504\n",
      "step 430 loss 2.5040\n",
      "step 440 loss 2.0448\n",
      "step 450 loss 1.8398\n",
      "step 460 loss 1.6495\n",
      "step 470 loss 2.2869\n",
      "step 480 loss 2.6531\n",
      "step 490 loss 1.5617\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/minecraft/50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d5b954ced742d5a9b504c9b8458463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:2066: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 posts from minecraft dataset (100.0%)\n",
      "Found 2000 valid posts in minecraft\n",
      "Loaded dataset {'minecraft': 1} with 100 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617c64605acf40b0b5e4252d78a0d7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 4.2480\n",
      "step 10 loss 4.2077\n",
      "step 20 loss 4.9993\n",
      "step 30 loss 3.3485\n",
      "step 40 loss 3.8267\n",
      "step 50 loss 2.5198\n",
      "step 60 loss 3.3302\n",
      "step 70 loss 2.9033\n",
      "step 80 loss 2.4457\n",
      "step 90 loss 2.5789\n",
      "step 100 loss 2.4244\n",
      "step 110 loss 3.0907\n",
      "step 120 loss 2.6981\n",
      "step 130 loss 2.7288\n",
      "step 140 loss 2.3284\n",
      "step 150 loss 2.3728\n",
      "step 160 loss 1.9284\n",
      "step 170 loss 2.0379\n",
      "step 180 loss 2.7137\n",
      "step 190 loss 2.6386\n",
      "step 200 loss 1.9912\n",
      "step 210 loss 2.0485\n",
      "step 220 loss 2.0274\n",
      "step 230 loss 2.4109\n",
      "step 240 loss 2.4963\n",
      "step 250 loss 2.1186\n",
      "step 260 loss 2.0863\n",
      "step 270 loss 2.2535\n",
      "step 280 loss 1.9242\n",
      "step 290 loss 1.8787\n",
      "step 300 loss 2.0459\n",
      "step 310 loss 2.7635\n",
      "step 320 loss 1.7228\n",
      "step 330 loss 2.5685\n",
      "step 340 loss 2.3998\n",
      "step 350 loss 2.6054\n",
      "step 360 loss 1.9197\n",
      "step 370 loss 2.0941\n",
      "step 380 loss 2.3213\n",
      "step 390 loss 2.7049\n",
      "step 400 loss 2.8034\n",
      "step 410 loss 2.3428\n",
      "step 420 loss 2.1677\n",
      "step 430 loss 1.6796\n",
      "step 440 loss 3.0646\n",
      "step 450 loss 2.4841\n",
      "step 460 loss 2.2870\n",
      "step 470 loss 1.5805\n",
      "step 480 loss 3.2597\n",
      "step 490 loss 2.0376\n",
      "step 500 loss 1.9292\n",
      "step 510 loss 1.9387\n",
      "step 520 loss 2.6358\n",
      "step 530 loss 2.3796\n",
      "step 540 loss 1.8969\n",
      "step 550 loss 2.8878\n",
      "step 560 loss 1.9056\n",
      "step 570 loss 2.1555\n",
      "step 580 loss 2.1578\n",
      "step 590 loss 1.8140\n",
      "step 600 loss 1.7287\n",
      "step 610 loss 2.1309\n",
      "step 620 loss 2.2038\n",
      "step 630 loss 2.4918\n",
      "step 640 loss 2.4543\n",
      "step 650 loss 2.2502\n",
      "step 660 loss 2.1253\n",
      "step 670 loss 2.0684\n",
      "step 680 loss 2.6373\n",
      "step 690 loss 1.8641\n",
      "step 700 loss 1.9663\n",
      "step 710 loss 2.5277\n",
      "step 720 loss 2.5289\n",
      "step 730 loss 2.2460\n",
      "step 740 loss 2.2868\n",
      "step 750 loss 2.4400\n",
      "step 760 loss 1.8472\n",
      "step 770 loss 2.0120\n",
      "step 780 loss 2.5426\n",
      "step 790 loss 1.9945\n",
      "step 800 loss 1.8570\n",
      "step 810 loss 2.3320\n",
      "step 820 loss 2.2602\n",
      "step 830 loss 2.3828\n",
      "step 840 loss 2.0121\n",
      "step 850 loss 2.3624\n",
      "step 860 loss 1.8886\n",
      "step 870 loss 2.4697\n",
      "step 880 loss 2.0181\n",
      "step 890 loss 2.2832\n",
      "step 900 loss 2.2793\n",
      "step 910 loss 1.7997\n",
      "step 920 loss 1.7096\n",
      "step 930 loss 1.6688\n",
      "step 940 loss 2.6426\n",
      "step 950 loss 2.8804\n",
      "step 960 loss 1.7854\n",
      "step 970 loss 1.8657\n",
      "step 980 loss 1.8925\n",
      "step 990 loss 2.5053\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/minecraft/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437feae97b254ac8828d50e86c6de215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 250 posts from minecraft dataset (100.0%)\n",
      "Found 2000 valid posts in minecraft\n",
      "Loaded dataset {'minecraft': 1} with 250 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fe59245c8e44e9a5b66c76774977d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 8.9874\n",
      "step 10 loss 5.3874\n",
      "step 20 loss 3.8048\n",
      "step 30 loss 3.7419\n",
      "step 40 loss 3.0785\n",
      "step 50 loss 4.2218\n",
      "step 60 loss 2.9331\n",
      "step 70 loss 1.8676\n",
      "step 80 loss 2.2983\n",
      "step 90 loss 2.1802\n",
      "step 100 loss 2.0631\n",
      "step 110 loss 3.6209\n",
      "step 120 loss 2.0740\n",
      "step 130 loss 0.8353\n",
      "step 140 loss 1.9076\n",
      "step 150 loss 1.8886\n",
      "step 160 loss 2.2629\n",
      "step 170 loss 2.6546\n",
      "step 180 loss 2.0460\n",
      "step 190 loss 1.7712\n",
      "step 200 loss 2.2477\n",
      "step 210 loss 1.9614\n",
      "step 220 loss 2.4069\n",
      "step 230 loss 1.9478\n",
      "step 240 loss 2.6635\n",
      "step 250 loss 2.7251\n",
      "step 260 loss 1.8496\n",
      "step 270 loss 2.1652\n",
      "step 280 loss 2.3668\n",
      "step 290 loss 2.9251\n",
      "step 300 loss 1.8178\n",
      "step 310 loss 2.6987\n",
      "step 320 loss 1.1564\n",
      "step 330 loss 1.8206\n",
      "step 340 loss 2.4636\n",
      "step 350 loss 1.9434\n",
      "step 360 loss 2.7442\n",
      "step 370 loss 2.2423\n",
      "step 380 loss 1.6001\n",
      "step 390 loss 2.1979\n",
      "step 400 loss 2.2000\n",
      "step 410 loss 3.0480\n",
      "step 420 loss 2.5179\n",
      "step 430 loss 2.0166\n",
      "step 440 loss 2.3931\n",
      "step 450 loss 2.3489\n",
      "step 460 loss 2.5027\n",
      "step 470 loss 1.9824\n",
      "step 480 loss 1.8181\n",
      "step 490 loss 3.1223\n",
      "step 500 loss 1.8599\n",
      "step 510 loss 2.2548\n",
      "step 520 loss 1.9995\n",
      "step 530 loss 2.9795\n",
      "step 540 loss 2.6503\n",
      "step 550 loss 2.6916\n",
      "step 560 loss 2.6983\n",
      "step 570 loss 2.2850\n",
      "step 580 loss 2.3671\n",
      "step 590 loss 2.4542\n",
      "step 600 loss 1.7764\n",
      "step 610 loss 2.5635\n",
      "step 620 loss 1.6478\n",
      "step 630 loss 2.1224\n",
      "step 640 loss 2.7105\n",
      "step 650 loss 1.7616\n",
      "step 660 loss 2.4857\n",
      "step 670 loss 2.8852\n",
      "step 680 loss 1.4861\n",
      "step 690 loss 1.9500\n",
      "step 700 loss 1.5519\n",
      "step 710 loss 2.0129\n",
      "step 720 loss 1.9589\n",
      "step 730 loss 2.6376\n",
      "step 740 loss 2.1211\n",
      "step 750 loss 1.9591\n",
      "step 760 loss 2.0839\n",
      "step 770 loss 2.5499\n",
      "step 780 loss 2.2686\n",
      "step 790 loss 2.6587\n",
      "step 800 loss 2.4954\n",
      "step 810 loss 2.2699\n",
      "step 820 loss 2.2908\n",
      "step 830 loss 2.1096\n",
      "step 840 loss 2.6654\n",
      "step 850 loss 2.4158\n",
      "step 860 loss 2.5770\n",
      "step 870 loss 2.5419\n",
      "step 880 loss 2.5551\n",
      "step 890 loss 1.4569\n",
      "step 900 loss 2.0765\n",
      "step 910 loss 2.0742\n",
      "step 920 loss 2.2207\n",
      "step 930 loss 2.2044\n",
      "step 940 loss 1.6429\n",
      "step 950 loss 1.5200\n",
      "step 960 loss 2.4264\n",
      "step 970 loss 2.0049\n",
      "step 980 loss 2.2883\n",
      "step 990 loss 1.5486\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/minecraft/250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df3aa2418834cb4b47403041bcced14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 500 posts from minecraft dataset (100.0%)\n",
      "Found 2000 valid posts in minecraft\n",
      "Loaded dataset {'minecraft': 1} with 500 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b22f35091f44fefa40065fca7eb4953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 5.3883\n",
      "step 10 loss 4.3580\n",
      "step 20 loss 4.4126\n",
      "step 30 loss 3.2893\n",
      "step 40 loss 3.3688\n",
      "step 50 loss 2.3355\n",
      "step 60 loss 2.3559\n",
      "step 70 loss 2.5299\n",
      "step 80 loss 2.1042\n",
      "step 90 loss 2.7303\n",
      "step 100 loss 2.1284\n",
      "step 110 loss 2.8045\n",
      "step 120 loss 2.2151\n",
      "step 130 loss 2.1981\n",
      "step 140 loss 2.5482\n",
      "step 150 loss 2.7375\n",
      "step 160 loss 2.1290\n",
      "step 170 loss 2.6265\n",
      "step 180 loss 0.5753\n",
      "step 190 loss 2.6086\n",
      "step 200 loss 1.8925\n",
      "step 210 loss 4.1134\n",
      "step 220 loss 1.7122\n",
      "step 230 loss 1.6997\n",
      "step 240 loss 2.0531\n",
      "step 250 loss 2.7003\n",
      "step 260 loss 2.1587\n",
      "step 270 loss 2.7333\n",
      "step 280 loss 2.6918\n",
      "step 290 loss 2.3410\n",
      "step 300 loss 1.9014\n",
      "step 310 loss 1.9342\n",
      "step 320 loss 2.7364\n",
      "step 330 loss 2.7955\n",
      "step 340 loss 1.8276\n",
      "step 350 loss 2.7110\n",
      "step 360 loss 2.2367\n",
      "step 370 loss 2.4389\n",
      "step 380 loss 2.6982\n",
      "step 390 loss 2.1220\n",
      "step 400 loss 2.5671\n",
      "step 410 loss 2.4067\n",
      "step 420 loss 2.2171\n",
      "step 430 loss 1.8665\n",
      "step 440 loss 2.3796\n",
      "step 450 loss 1.9804\n",
      "step 460 loss 2.4379\n",
      "step 470 loss 3.0538\n",
      "step 480 loss 1.7644\n",
      "step 490 loss 2.2198\n",
      "step 500 loss 2.6851\n",
      "step 510 loss 2.1131\n",
      "step 520 loss 2.7694\n",
      "step 530 loss 2.1549\n",
      "step 540 loss 1.6333\n",
      "step 550 loss 1.7748\n",
      "step 560 loss 2.1178\n",
      "step 570 loss 2.6678\n",
      "step 580 loss 2.4724\n",
      "step 590 loss 2.0503\n",
      "step 600 loss 2.4998\n",
      "step 610 loss 2.8369\n",
      "step 620 loss 1.8358\n",
      "step 630 loss 2.4234\n",
      "step 640 loss 1.9524\n",
      "step 650 loss 2.1998\n",
      "step 660 loss 2.4387\n",
      "step 670 loss 3.0689\n",
      "step 680 loss 2.2775\n",
      "step 690 loss 1.2298\n",
      "step 700 loss 2.0518\n",
      "step 710 loss 2.7517\n",
      "step 720 loss 2.1152\n",
      "step 730 loss 2.5704\n",
      "step 740 loss 2.4156\n",
      "step 750 loss 2.2013\n",
      "step 760 loss 2.2974\n",
      "step 770 loss 3.9186\n",
      "step 780 loss 1.7150\n",
      "step 790 loss 2.3851\n",
      "step 800 loss 3.1842\n",
      "step 810 loss 2.0315\n",
      "step 820 loss 2.0604\n",
      "step 830 loss 1.9967\n",
      "step 840 loss 3.0106\n",
      "step 850 loss 2.2443\n",
      "step 860 loss 2.4971\n",
      "step 870 loss 1.9367\n",
      "step 880 loss 2.2747\n",
      "step 890 loss 1.8745\n",
      "step 900 loss 2.1015\n",
      "step 910 loss 2.7228\n",
      "step 920 loss 1.9112\n",
      "step 930 loss 1.9744\n",
      "step 940 loss 2.2362\n",
      "step 950 loss 2.3154\n",
      "step 960 loss 2.3781\n",
      "step 970 loss 2.1387\n",
      "step 980 loss 2.3084\n",
      "step 990 loss 2.0469\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/minecraft/500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495fd5625dac432bb1e0f6d1b8bc36c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1000 posts from minecraft dataset (100.0%)\n",
      "Found 2000 valid posts in minecraft\n",
      "Loaded dataset {'minecraft': 1} with 1000 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83d028b4a6e4cb891c45f140f5a8d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 5.4229\n",
      "step 10 loss 4.4254\n",
      "step 20 loss 3.0483\n",
      "step 30 loss 4.2299\n",
      "step 40 loss 3.4481\n",
      "step 50 loss 2.9043\n",
      "step 60 loss 2.1420\n",
      "step 70 loss 3.1112\n",
      "step 80 loss 2.3851\n",
      "step 90 loss 3.2437\n",
      "step 100 loss 2.1163\n",
      "step 110 loss 2.3263\n",
      "step 120 loss 1.7629\n",
      "step 130 loss 1.8152\n",
      "step 140 loss 2.0812\n",
      "step 150 loss 2.3899\n",
      "step 160 loss 2.4577\n",
      "step 170 loss 2.0584\n",
      "step 180 loss 2.9903\n",
      "step 190 loss 2.4983\n",
      "step 200 loss 2.5963\n",
      "step 210 loss 2.2104\n",
      "step 220 loss 2.6040\n",
      "step 230 loss 2.6219\n",
      "step 240 loss 3.3569\n",
      "step 250 loss 1.9424\n",
      "step 260 loss 2.3411\n",
      "step 270 loss 2.3262\n",
      "step 280 loss 1.5273\n",
      "step 290 loss 2.3958\n",
      "step 300 loss 1.8875\n",
      "step 310 loss 2.1842\n",
      "step 320 loss 1.8964\n",
      "step 330 loss 2.6166\n",
      "step 340 loss 2.3284\n",
      "step 350 loss 2.6732\n",
      "step 360 loss 1.7640\n",
      "step 370 loss 3.2729\n",
      "step 380 loss 2.4367\n",
      "step 390 loss 1.9536\n",
      "step 400 loss 1.7649\n",
      "step 410 loss 2.9520\n",
      "step 420 loss 1.9032\n",
      "step 430 loss 2.6303\n",
      "step 440 loss 2.3834\n",
      "step 450 loss 1.7222\n",
      "step 460 loss 1.9900\n",
      "step 470 loss 1.9173\n",
      "step 480 loss 1.9989\n",
      "step 490 loss 2.5924\n",
      "step 500 loss 1.8298\n",
      "step 510 loss 1.8410\n",
      "step 520 loss 2.3761\n",
      "step 530 loss 2.4809\n",
      "step 540 loss 2.2333\n",
      "step 550 loss 2.5171\n",
      "step 560 loss 2.6730\n",
      "step 570 loss 2.3155\n",
      "step 580 loss 2.2632\n",
      "step 590 loss 2.7215\n",
      "step 600 loss 2.5282\n",
      "step 610 loss 2.1837\n",
      "step 620 loss 2.6076\n",
      "step 630 loss 2.5995\n",
      "step 640 loss 2.7435\n",
      "step 650 loss 2.5180\n",
      "step 660 loss 2.8675\n",
      "step 670 loss 2.2117\n",
      "step 680 loss 2.4217\n",
      "step 690 loss 2.2521\n",
      "step 700 loss 2.5972\n",
      "step 710 loss 2.6793\n",
      "step 720 loss 2.5763\n",
      "step 730 loss 2.5884\n",
      "step 740 loss 2.3033\n",
      "step 750 loss 2.1254\n",
      "step 760 loss 1.9209\n",
      "step 770 loss 2.1707\n",
      "step 780 loss 2.6610\n",
      "step 790 loss 2.6747\n",
      "step 800 loss 2.1268\n",
      "step 810 loss 2.0087\n",
      "step 820 loss 2.2693\n",
      "step 830 loss 2.7338\n",
      "step 840 loss 3.0313\n",
      "step 850 loss 2.2371\n",
      "step 860 loss 1.4263\n",
      "step 870 loss 1.4798\n",
      "step 880 loss 1.9119\n",
      "step 890 loss 1.9143\n",
      "step 900 loss 2.8890\n",
      "step 910 loss 2.8154\n",
      "step 920 loss 1.5499\n",
      "step 930 loss 2.2199\n",
      "step 940 loss 2.0021\n",
      "step 950 loss 2.3251\n",
      "step 960 loss 2.1215\n",
      "step 970 loss 2.0681\n",
      "step 980 loss 1.6967\n",
      "step 990 loss 1.7328\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/minecraft/1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcdd6cd565a4de88ae1590640602164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2000 posts from minecraft dataset (100.0%)\n",
      "Found 2000 valid posts in minecraft\n",
      "Loaded dataset {'minecraft': 1} with 2000 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bc7008b09544eea92e6fbf8f412fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 5.9198\n",
      "step 10 loss 4.4162\n",
      "step 20 loss 3.8455\n",
      "step 30 loss 2.8268\n",
      "step 40 loss 3.2144\n",
      "step 50 loss 2.6945\n",
      "step 60 loss 2.1630\n",
      "step 70 loss 2.8229\n",
      "step 80 loss 2.1126\n",
      "step 90 loss 2.6046\n",
      "step 100 loss 2.1089\n",
      "step 110 loss 2.1190\n",
      "step 120 loss 1.9408\n",
      "step 130 loss 2.2363\n",
      "step 140 loss 1.8350\n",
      "step 150 loss 1.9701\n",
      "step 160 loss 2.5390\n",
      "step 170 loss 2.1567\n",
      "step 180 loss 1.9517\n",
      "step 190 loss 2.8258\n",
      "step 200 loss 1.5662\n",
      "step 210 loss 2.2369\n",
      "step 220 loss 2.0452\n",
      "step 230 loss 3.6362\n",
      "step 240 loss 1.9290\n",
      "step 250 loss 1.9707\n",
      "step 260 loss 1.9538\n",
      "step 270 loss 1.7985\n",
      "step 280 loss 3.7968\n",
      "step 290 loss 1.9790\n",
      "step 300 loss 2.8641\n",
      "step 310 loss 2.8618\n",
      "step 320 loss 2.4660\n",
      "step 330 loss 2.4197\n",
      "step 340 loss 2.2931\n",
      "step 350 loss 2.9035\n",
      "step 360 loss 1.7608\n",
      "step 370 loss 2.5821\n",
      "step 380 loss 2.7910\n",
      "step 390 loss 2.5779\n",
      "step 400 loss 2.9312\n",
      "step 410 loss 2.2055\n",
      "step 420 loss 2.2532\n",
      "step 430 loss 2.2520\n",
      "step 440 loss 1.5088\n",
      "step 450 loss 2.8963\n",
      "step 460 loss 2.2793\n",
      "step 470 loss 2.2079\n",
      "step 480 loss 2.7155\n",
      "step 490 loss 2.4722\n",
      "step 500 loss 3.1017\n",
      "step 510 loss 2.2946\n",
      "step 520 loss 1.7336\n",
      "step 530 loss 1.4974\n",
      "step 540 loss 2.3883\n",
      "step 550 loss 1.8067\n",
      "step 560 loss 1.5973\n",
      "step 570 loss 2.4254\n",
      "step 580 loss 2.1525\n",
      "step 590 loss 2.4341\n",
      "step 600 loss 2.4017\n",
      "step 610 loss 1.8327\n",
      "step 620 loss 2.1437\n",
      "step 630 loss 1.8294\n",
      "step 640 loss 2.1587\n",
      "step 650 loss 2.1831\n",
      "step 660 loss 2.1218\n",
      "step 670 loss 2.1416\n",
      "step 680 loss 2.6860\n",
      "step 690 loss 2.7941\n",
      "step 700 loss 2.6303\n",
      "step 710 loss 2.4626\n",
      "step 720 loss 2.1480\n",
      "step 730 loss 1.2668\n",
      "step 740 loss 2.4648\n",
      "step 750 loss 2.5728\n",
      "step 760 loss 1.7911\n",
      "step 770 loss 2.8277\n",
      "step 780 loss 2.2541\n",
      "step 790 loss 2.6565\n",
      "step 800 loss 2.0864\n",
      "step 810 loss 2.0219\n",
      "step 820 loss 1.6390\n",
      "step 830 loss 1.9728\n",
      "step 840 loss 2.1451\n",
      "step 850 loss 2.2658\n",
      "step 860 loss 2.6374\n",
      "step 870 loss 1.9359\n",
      "step 880 loss 2.3021\n",
      "step 890 loss 2.3259\n",
      "step 900 loss 2.4403\n",
      "step 910 loss 2.5239\n",
      "step 920 loss 2.3544\n",
      "step 930 loss 2.1607\n",
      "step 940 loss 2.0940\n",
      "step 950 loss 0.5317\n",
      "step 960 loss 1.5094\n",
      "step 970 loss 2.3355\n",
      "step 980 loss 1.8363\n",
      "step 990 loss 2.2158\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/minecraft/2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded774641d9d42fb9ce5ee0631ea9e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d77c2515124dfaba7ee7bbdec5a560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10 posts from ucla dataset (100.0%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loaded dataset {'ucla': 1} with 10 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c27c5825ed402991113049a158ac33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 6.3284\n",
      "step 10 loss 4.4542\n",
      "step 20 loss 4.2202\n",
      "step 30 loss 3.4734\n",
      "step 40 loss 3.3773\n",
      "step 50 loss 3.4470\n",
      "step 60 loss 2.3753\n",
      "step 70 loss 2.6218\n",
      "step 80 loss 3.0209\n",
      "step 90 loss 2.0639\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/ucla/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cc4132f5ed4f4a8fcaa5dc2c72c219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 posts from ucla dataset (100.0%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loaded dataset {'ucla': 1} with 20 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32860cc645dd4ebbb522f85373e4c0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 6.8984\n",
      "step 10 loss 5.7726\n",
      "step 20 loss 5.6115\n",
      "step 30 loss 4.4223\n",
      "step 40 loss 4.5487\n",
      "step 50 loss 3.1944\n",
      "step 60 loss 2.9197\n",
      "step 70 loss 2.4714\n",
      "step 80 loss 2.8289\n",
      "step 90 loss 1.7933\n",
      "step 100 loss 2.0078\n",
      "step 110 loss 2.6273\n",
      "step 120 loss 2.4194\n",
      "step 130 loss 2.2133\n",
      "step 140 loss 2.1409\n",
      "step 150 loss 2.0900\n",
      "step 160 loss 1.5073\n",
      "step 170 loss 1.9793\n",
      "step 180 loss 1.7427\n",
      "step 190 loss 2.7186\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/ucla/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2de12445274fb58402dc56196a07a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 50 posts from ucla dataset (100.0%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loaded dataset {'ucla': 1} with 50 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d3f7e493964d67b95c14927b4f69d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 6.8050\n",
      "step 10 loss 8.0148\n",
      "step 20 loss 4.5762\n",
      "step 30 loss 4.0029\n",
      "step 40 loss 3.9153\n",
      "step 50 loss 3.4510\n",
      "step 60 loss 3.4622\n",
      "step 70 loss 2.6573\n",
      "step 80 loss 2.4090\n",
      "step 90 loss 2.5488\n",
      "step 100 loss 2.6316\n",
      "step 110 loss 2.5420\n",
      "step 120 loss 2.5139\n",
      "step 130 loss 2.2948\n",
      "step 140 loss 3.2916\n",
      "step 150 loss 2.0568\n",
      "step 160 loss 2.3817\n",
      "step 170 loss 2.2800\n",
      "step 180 loss 2.3626\n",
      "step 190 loss 2.6806\n",
      "step 200 loss 2.3473\n",
      "step 210 loss 2.5116\n",
      "step 220 loss 2.2582\n",
      "step 230 loss 1.7680\n",
      "step 240 loss 2.0466\n",
      "step 250 loss 2.5021\n",
      "step 260 loss 2.2314\n",
      "step 270 loss 1.9931\n",
      "step 280 loss 2.4812\n",
      "step 290 loss 1.8567\n",
      "step 300 loss 2.7096\n",
      "step 310 loss 2.1981\n",
      "step 320 loss 2.7034\n",
      "step 330 loss 2.3687\n",
      "step 340 loss 2.5865\n",
      "step 350 loss 2.5323\n",
      "step 360 loss 1.7747\n",
      "step 370 loss 1.7327\n",
      "step 380 loss 2.1909\n",
      "step 390 loss 2.0995\n",
      "step 400 loss 1.6643\n",
      "step 410 loss 1.9751\n",
      "step 420 loss 2.8464\n",
      "step 430 loss 2.2235\n",
      "step 440 loss 1.9170\n",
      "step 450 loss 1.3684\n",
      "step 460 loss 1.6870\n",
      "step 470 loss 1.4404\n",
      "step 480 loss 2.3935\n",
      "step 490 loss 1.9784\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/ucla/50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6b926fcefd479b9b45863ad2bf3790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 posts from ucla dataset (100.0%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loaded dataset {'ucla': 1} with 100 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ad5e06adc44db9abfb141ae1de805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 6.3827\n",
      "step 10 loss 6.2817\n",
      "step 20 loss 4.6809\n",
      "step 30 loss 2.9835\n",
      "step 40 loss 3.0269\n",
      "step 50 loss 3.3770\n",
      "step 60 loss 2.8780\n",
      "step 70 loss 2.4940\n",
      "step 80 loss 2.3999\n",
      "step 90 loss 3.1208\n",
      "step 100 loss 2.1965\n",
      "step 110 loss 2.0323\n",
      "step 120 loss 2.6173\n",
      "step 130 loss 1.8992\n",
      "step 140 loss 2.0288\n",
      "step 150 loss 2.8957\n",
      "step 160 loss 2.3744\n",
      "step 170 loss 2.4591\n",
      "step 180 loss 2.5002\n",
      "step 190 loss 2.3924\n",
      "step 200 loss 2.1472\n",
      "step 210 loss 2.0493\n",
      "step 220 loss 2.4649\n",
      "step 230 loss 1.9794\n",
      "step 240 loss 2.3120\n",
      "step 250 loss 3.2114\n",
      "step 260 loss 2.1121\n",
      "step 270 loss 2.6105\n",
      "step 280 loss 2.6211\n",
      "step 290 loss 2.9507\n",
      "step 300 loss 2.4433\n",
      "step 310 loss 2.3514\n",
      "step 320 loss 1.9866\n",
      "step 330 loss 3.1880\n",
      "step 340 loss 1.7196\n",
      "step 350 loss 1.7859\n",
      "step 360 loss 1.7190\n",
      "step 370 loss 1.9942\n",
      "step 380 loss 2.1955\n",
      "step 390 loss 2.6111\n",
      "step 400 loss 2.7119\n",
      "step 410 loss 2.3993\n",
      "step 420 loss 2.4831\n",
      "step 430 loss 3.1759\n",
      "step 440 loss 2.5032\n",
      "step 450 loss 2.6827\n",
      "step 460 loss 1.9335\n",
      "step 470 loss 1.9229\n",
      "step 480 loss 3.1532\n",
      "step 490 loss 1.7214\n",
      "step 500 loss 1.5311\n",
      "step 510 loss 2.0837\n",
      "step 520 loss 1.7844\n",
      "step 530 loss 2.7132\n",
      "step 540 loss 2.1024\n",
      "step 550 loss 1.8910\n",
      "step 560 loss 2.1895\n",
      "step 570 loss 2.5755\n",
      "step 580 loss 2.4718\n",
      "step 590 loss 1.9852\n",
      "step 600 loss 2.0940\n",
      "step 610 loss 1.7687\n",
      "step 620 loss 1.9670\n",
      "step 630 loss 1.8638\n",
      "step 640 loss 2.1403\n",
      "step 650 loss 2.4891\n",
      "step 660 loss 2.0966\n",
      "step 670 loss 2.4274\n",
      "step 680 loss 3.1744\n",
      "step 690 loss 2.5859\n",
      "step 700 loss 2.0988\n",
      "step 710 loss 2.2063\n",
      "step 720 loss 2.1007\n",
      "step 730 loss 1.7805\n",
      "step 740 loss 2.4895\n",
      "step 750 loss 2.4607\n",
      "step 760 loss 2.1732\n",
      "step 770 loss 1.8522\n",
      "step 780 loss 1.3395\n",
      "step 790 loss 2.5842\n",
      "step 800 loss 1.9195\n",
      "step 810 loss 2.2385\n",
      "step 820 loss 1.9238\n",
      "step 830 loss 3.0587\n",
      "step 840 loss 2.0839\n",
      "step 850 loss 1.6823\n",
      "step 860 loss 1.8317\n",
      "step 870 loss 2.2590\n",
      "step 880 loss 2.2469\n",
      "step 890 loss 1.5113\n",
      "step 900 loss 2.0353\n",
      "step 910 loss 1.8641\n",
      "step 920 loss 2.1963\n",
      "step 930 loss 1.9730\n",
      "step 940 loss 2.2323\n",
      "step 950 loss 2.3809\n",
      "step 960 loss 2.0423\n",
      "step 970 loss 2.0272\n",
      "step 980 loss 2.0460\n",
      "step 990 loss 1.3446\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/ucla/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883c0e6e58b6441ea23bc1f7f0f42e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 250 posts from ucla dataset (100.0%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loaded dataset {'ucla': 1} with 250 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471b00789ce948349a3a1a55d963e85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 5.6088\n",
      "step 10 loss 5.0817\n",
      "step 20 loss 3.9709\n",
      "step 30 loss 3.6168\n",
      "step 40 loss 3.2097\n",
      "step 50 loss 3.4204\n",
      "step 60 loss 2.6389\n",
      "step 70 loss 2.7594\n",
      "step 80 loss 2.9824\n",
      "step 90 loss 2.5906\n",
      "step 100 loss 3.2240\n",
      "step 110 loss 1.4734\n",
      "step 120 loss 2.1828\n",
      "step 130 loss 2.7886\n",
      "step 140 loss 2.1599\n",
      "step 150 loss 2.2300\n",
      "step 160 loss 2.2268\n",
      "step 170 loss 2.7298\n",
      "step 180 loss 1.7014\n",
      "step 190 loss 3.3073\n",
      "step 200 loss 1.9983\n",
      "step 210 loss 2.2112\n",
      "step 220 loss 2.1542\n",
      "step 230 loss 2.7881\n",
      "step 240 loss 2.9401\n",
      "step 250 loss 3.0799\n",
      "step 260 loss 2.5538\n",
      "step 270 loss 2.2835\n",
      "step 280 loss 2.2101\n",
      "step 290 loss 2.6889\n",
      "step 300 loss 1.9209\n",
      "step 310 loss 1.7886\n",
      "step 320 loss 2.3135\n",
      "step 330 loss 1.9334\n",
      "step 340 loss 2.5830\n",
      "step 350 loss 1.5423\n",
      "step 360 loss 1.6510\n",
      "step 370 loss 1.6155\n",
      "step 380 loss 2.7780\n",
      "step 390 loss 1.4888\n",
      "step 400 loss 2.7175\n",
      "step 410 loss 2.3560\n",
      "step 420 loss 2.2542\n",
      "step 430 loss 2.5420\n",
      "step 440 loss 1.6617\n",
      "step 450 loss 2.1665\n",
      "step 460 loss 1.7288\n",
      "step 470 loss 1.9635\n",
      "step 480 loss 2.4053\n",
      "step 490 loss 2.4807\n",
      "step 500 loss 1.7879\n",
      "step 510 loss 2.5682\n",
      "step 520 loss 1.9785\n",
      "step 530 loss 2.3514\n",
      "step 540 loss 3.2457\n",
      "step 550 loss 1.5891\n",
      "step 560 loss 2.4079\n",
      "step 570 loss 2.2335\n",
      "step 580 loss 2.2120\n",
      "step 590 loss 2.2552\n",
      "step 600 loss 2.5988\n",
      "step 610 loss 2.9373\n",
      "step 620 loss 3.0378\n",
      "step 630 loss 2.9783\n",
      "step 640 loss 2.6055\n",
      "step 650 loss 2.6243\n",
      "step 660 loss 1.7834\n",
      "step 670 loss 2.0874\n",
      "step 680 loss 2.4480\n",
      "step 690 loss 2.3267\n",
      "step 700 loss 1.6368\n",
      "step 710 loss 2.7218\n",
      "step 720 loss 2.2723\n",
      "step 730 loss 2.5774\n",
      "step 740 loss 1.6978\n",
      "step 750 loss 2.1856\n",
      "step 760 loss 2.1068\n",
      "step 770 loss 1.6585\n",
      "step 780 loss 1.7684\n",
      "step 790 loss 2.4246\n",
      "step 800 loss 2.9334\n",
      "step 810 loss 1.5178\n",
      "step 820 loss 2.0852\n",
      "step 830 loss 1.6478\n",
      "step 840 loss 2.6924\n",
      "step 850 loss 2.4545\n",
      "step 860 loss 2.8167\n",
      "step 870 loss 2.5135\n",
      "step 880 loss 1.9322\n",
      "step 890 loss 2.4266\n",
      "step 900 loss 1.9627\n",
      "step 910 loss 1.9112\n",
      "step 920 loss 2.6656\n",
      "step 930 loss 2.0726\n",
      "step 940 loss 2.8968\n",
      "step 950 loss 2.1385\n",
      "step 960 loss 3.1288\n",
      "step 970 loss 1.7063\n",
      "step 980 loss 2.4264\n",
      "step 990 loss 2.3711\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/ucla/250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbfffe54bf74a5b937f7dc7f2d11af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 500 posts from ucla dataset (100.0%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loaded dataset {'ucla': 1} with 500 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c56e21938e8413bbdb4769885bdcec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 8.8773\n",
      "step 10 loss 3.5993\n",
      "step 20 loss 7.0780\n",
      "step 30 loss 5.5693\n",
      "step 40 loss 4.4166\n",
      "step 50 loss 3.7301\n",
      "step 60 loss 2.8223\n",
      "step 70 loss 2.5603\n",
      "step 80 loss 2.5490\n",
      "step 90 loss 2.3152\n",
      "step 100 loss 2.1485\n",
      "step 110 loss 2.4741\n",
      "step 120 loss 1.8251\n",
      "step 130 loss 2.3934\n",
      "step 140 loss 1.7384\n",
      "step 150 loss 1.9061\n",
      "step 160 loss 2.5741\n",
      "step 170 loss 1.7352\n",
      "step 180 loss 1.9586\n",
      "step 190 loss 2.5596\n",
      "step 200 loss 1.5170\n",
      "step 210 loss 1.9126\n",
      "step 220 loss 1.9534\n",
      "step 230 loss 2.6949\n",
      "step 240 loss 2.2807\n",
      "step 250 loss 2.1485\n",
      "step 260 loss 2.2542\n",
      "step 270 loss 2.6882\n",
      "step 280 loss 2.5390\n",
      "step 290 loss 2.9530\n",
      "step 300 loss 2.6566\n",
      "step 310 loss 2.1144\n",
      "step 320 loss 1.8975\n",
      "step 330 loss 2.0894\n",
      "step 340 loss 2.4792\n",
      "step 350 loss 2.6329\n",
      "step 360 loss 2.0041\n",
      "step 370 loss 2.1027\n",
      "step 380 loss 2.0597\n",
      "step 390 loss 2.3275\n",
      "step 400 loss 2.2487\n",
      "step 410 loss 2.2951\n",
      "step 420 loss 2.9117\n",
      "step 430 loss 1.8111\n",
      "step 440 loss 2.0725\n",
      "step 450 loss 3.4249\n",
      "step 460 loss 2.9330\n",
      "step 470 loss 2.4428\n",
      "step 480 loss 2.3646\n",
      "step 490 loss 2.0930\n",
      "step 500 loss 2.8396\n",
      "step 510 loss 2.2358\n",
      "step 520 loss 2.1794\n",
      "step 530 loss 1.9430\n",
      "step 540 loss 2.2772\n",
      "step 550 loss 2.0793\n",
      "step 560 loss 2.4440\n",
      "step 570 loss 2.1954\n",
      "step 580 loss 2.5378\n",
      "step 590 loss 1.8723\n",
      "step 600 loss 2.3103\n",
      "step 610 loss 1.5905\n",
      "step 620 loss 1.7038\n",
      "step 630 loss 2.2704\n",
      "step 640 loss 2.9026\n",
      "step 650 loss 3.1251\n",
      "step 660 loss 2.0669\n",
      "step 670 loss 1.7840\n",
      "step 680 loss 2.5161\n",
      "step 690 loss 2.0925\n",
      "step 700 loss 1.6351\n",
      "step 710 loss 2.1038\n",
      "step 720 loss 2.0340\n",
      "step 730 loss 2.4361\n",
      "step 740 loss 2.9653\n",
      "step 750 loss 2.0467\n",
      "step 760 loss 2.0859\n",
      "step 770 loss 1.6047\n",
      "step 780 loss 2.0559\n",
      "step 790 loss 2.3242\n",
      "step 800 loss 2.2105\n",
      "step 810 loss 2.3705\n",
      "step 820 loss 1.6015\n",
      "step 830 loss 2.1295\n",
      "step 840 loss 1.9229\n",
      "step 850 loss 2.5093\n",
      "step 860 loss 2.0231\n",
      "step 870 loss 2.2529\n",
      "step 880 loss 1.7696\n",
      "step 890 loss 1.8978\n",
      "step 900 loss 2.4908\n",
      "step 910 loss 2.9964\n",
      "step 920 loss 2.3923\n",
      "step 930 loss 2.7896\n",
      "step 940 loss 2.2573\n",
      "step 950 loss 2.0002\n",
      "step 960 loss 2.7705\n",
      "step 970 loss 1.9702\n",
      "step 980 loss 2.6138\n",
      "step 990 loss 2.6611\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/ucla/500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa14408432a45d8aa5c65c6c6442b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1000 posts from ucla dataset (100.0%)\n",
      "Found 1516 valid posts in ucla\n",
      "Loaded dataset {'ucla': 1} with 1000 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903ffb3dcc3b4d2b85ee742d7e7e71b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 7.7602\n",
      "step 10 loss 7.6175\n",
      "step 20 loss 6.3875\n",
      "step 30 loss 3.8353\n",
      "step 40 loss 2.4495\n",
      "step 50 loss 3.4705\n",
      "step 60 loss 2.9824\n",
      "step 70 loss 3.1570\n",
      "step 80 loss 2.3581\n",
      "step 90 loss 2.8599\n",
      "step 100 loss 1.6387\n",
      "step 110 loss 2.6015\n",
      "step 120 loss 1.9091\n",
      "step 130 loss 1.9243\n",
      "step 140 loss 2.0900\n",
      "step 150 loss 1.5000\n",
      "step 160 loss 2.4536\n",
      "step 170 loss 2.3662\n",
      "step 180 loss 2.3063\n",
      "step 190 loss 2.8089\n",
      "step 200 loss 2.6831\n",
      "step 210 loss 1.7887\n",
      "step 220 loss 2.5160\n",
      "step 230 loss 1.4391\n",
      "step 240 loss 1.9189\n",
      "step 250 loss 2.7050\n",
      "step 260 loss 1.5018\n",
      "step 270 loss 2.0230\n",
      "step 280 loss 2.1536\n",
      "step 290 loss 2.2700\n",
      "step 300 loss 2.4064\n",
      "step 310 loss 2.2376\n",
      "step 320 loss 2.4472\n",
      "step 330 loss 2.6608\n",
      "step 340 loss 2.2081\n",
      "step 350 loss 1.8358\n",
      "step 360 loss 2.0176\n",
      "step 370 loss 3.1237\n",
      "step 380 loss 1.7176\n",
      "step 390 loss 2.5074\n",
      "step 400 loss 1.9756\n",
      "step 410 loss 1.5832\n",
      "step 420 loss 2.8290\n",
      "step 430 loss 2.7996\n",
      "step 440 loss 2.2503\n",
      "step 450 loss 3.0951\n",
      "step 460 loss 1.8384\n",
      "step 470 loss 2.2517\n",
      "step 480 loss 2.2995\n",
      "step 490 loss 1.7433\n",
      "step 500 loss 2.1785\n",
      "step 510 loss 2.1835\n",
      "step 520 loss 1.5731\n",
      "step 530 loss 1.3419\n",
      "step 540 loss 1.4876\n",
      "step 550 loss 1.9555\n",
      "step 560 loss 2.2245\n",
      "step 570 loss 2.0902\n",
      "step 580 loss 1.8360\n",
      "step 590 loss 1.8849\n",
      "step 600 loss 2.3809\n",
      "step 610 loss 1.7091\n",
      "step 620 loss 1.5004\n",
      "step 630 loss 1.6465\n",
      "step 640 loss 2.4617\n",
      "step 650 loss 1.6075\n",
      "step 660 loss 1.6058\n",
      "step 670 loss 2.3627\n",
      "step 680 loss 1.8908\n",
      "step 690 loss 2.3076\n",
      "step 700 loss 1.9368\n",
      "step 710 loss 2.1627\n",
      "step 720 loss 2.1109\n",
      "step 730 loss 2.5652\n",
      "step 740 loss 2.9826\n",
      "step 750 loss 1.8045\n",
      "step 760 loss 2.0807\n",
      "step 770 loss 3.4185\n",
      "step 780 loss 2.3516\n",
      "step 790 loss 1.9667\n",
      "step 800 loss 2.2375\n",
      "step 810 loss 1.8618\n",
      "step 820 loss 1.2125\n",
      "step 830 loss 1.4883\n",
      "step 840 loss 2.6541\n",
      "step 850 loss 2.7521\n",
      "step 860 loss 1.9001\n",
      "step 870 loss 2.6522\n",
      "step 880 loss 3.5391\n",
      "step 890 loss 1.6874\n",
      "step 900 loss 2.6025\n",
      "step 910 loss 3.7388\n",
      "step 920 loss 1.8274\n",
      "step 930 loss 1.9992\n",
      "step 940 loss 2.4138\n",
      "step 950 loss 2.0786\n",
      "step 960 loss 2.7785\n",
      "step 970 loss 2.0731\n",
      "step 980 loss 2.5857\n",
      "step 990 loss 1.6186\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/ucla/1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7b3af76865490fa10c7379902563c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2000 posts from ucla dataset (100.0%)\n",
      "Found 1516 valid posts in ucla\n",
      "Warning: Only 1516 posts available, using all\n",
      "Loaded dataset {'ucla': 1} with 2000 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a3b0cefe2d4d17bd0f52bf98932c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1516 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 7.3847\n",
      "step 10 loss 8.0401\n",
      "step 20 loss 7.0952\n",
      "step 30 loss 3.7820\n",
      "step 40 loss 4.4172\n",
      "step 50 loss 3.1832\n",
      "step 60 loss 3.8824\n",
      "step 70 loss 2.0960\n",
      "step 80 loss 2.8739\n",
      "step 90 loss 2.2988\n",
      "step 100 loss 2.4422\n",
      "step 110 loss 2.6634\n",
      "step 120 loss 2.4409\n",
      "step 130 loss 2.2462\n",
      "step 140 loss 2.0266\n",
      "step 150 loss 2.4585\n",
      "step 160 loss 1.5218\n",
      "step 170 loss 2.3163\n",
      "step 180 loss 2.3684\n",
      "step 190 loss 2.4759\n",
      "step 200 loss 1.5946\n",
      "step 210 loss 2.0984\n",
      "step 220 loss 2.6432\n",
      "step 230 loss 3.1245\n",
      "step 240 loss 1.9242\n",
      "step 250 loss 2.0532\n",
      "step 260 loss 2.7115\n",
      "step 270 loss 2.2664\n",
      "step 280 loss 2.5603\n",
      "step 290 loss 2.8168\n",
      "step 300 loss 2.2952\n",
      "step 310 loss 1.9055\n",
      "step 320 loss 1.8181\n",
      "step 330 loss 2.1942\n",
      "step 340 loss 1.8052\n",
      "step 350 loss 3.0929\n",
      "step 360 loss 1.9940\n",
      "step 370 loss 1.9617\n",
      "step 380 loss 2.1265\n",
      "step 390 loss 2.4058\n",
      "step 400 loss 2.4134\n",
      "step 410 loss 2.6591\n",
      "step 420 loss 2.5023\n",
      "step 430 loss 1.8958\n",
      "step 440 loss 1.9160\n",
      "step 450 loss 2.0424\n",
      "step 460 loss 2.5947\n",
      "step 470 loss 2.4870\n",
      "step 480 loss 1.5526\n",
      "step 490 loss 2.0017\n",
      "step 500 loss 2.5837\n",
      "step 510 loss 2.3252\n",
      "step 520 loss 3.1472\n",
      "step 530 loss 2.5145\n",
      "step 540 loss 3.5618\n",
      "step 550 loss 1.9654\n",
      "step 560 loss 2.0543\n",
      "step 570 loss 2.3499\n",
      "step 580 loss 2.4219\n",
      "step 590 loss 2.9131\n",
      "step 600 loss 2.0627\n",
      "step 610 loss 2.7416\n",
      "step 620 loss 2.3364\n",
      "step 630 loss 2.1589\n",
      "step 640 loss 2.1968\n",
      "step 650 loss 1.7164\n",
      "step 660 loss 2.0942\n",
      "step 670 loss 2.5021\n",
      "step 680 loss 1.9764\n",
      "step 690 loss 1.9843\n",
      "step 700 loss 2.0360\n",
      "step 710 loss 2.1485\n",
      "step 720 loss 2.1126\n",
      "step 730 loss 2.6799\n",
      "step 740 loss 2.3762\n",
      "step 750 loss 1.7491\n",
      "step 760 loss 2.3632\n",
      "step 770 loss 2.1216\n",
      "step 780 loss 1.7805\n",
      "step 790 loss 1.8232\n",
      "step 800 loss 2.6183\n",
      "step 810 loss 2.2054\n",
      "step 820 loss 2.0210\n",
      "step 830 loss 2.1818\n",
      "step 840 loss 1.7631\n",
      "step 850 loss 1.4092\n",
      "step 860 loss 1.4742\n",
      "step 870 loss 2.5776\n",
      "step 880 loss 2.0979\n",
      "step 890 loss 2.6592\n",
      "step 900 loss 2.2170\n",
      "step 910 loss 2.1778\n",
      "step 920 loss 2.2540\n",
      "step 930 loss 1.9892\n",
      "step 940 loss 2.0081\n",
      "step 950 loss 1.5750\n",
      "step 960 loss 2.3177\n",
      "step 970 loss 2.6043\n",
      "step 980 loss 2.5634\n",
      "step 990 loss 1.1961\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/ucla/2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82c2fc87f6e4bb7991f349511019e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0f4080a7c44bb983208a931879db6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10 posts from nostupidquestions dataset (100.0%)\n",
      "Found 2000 valid posts in nostupidquestions\n",
      "Loaded dataset {'nostupidquestions': 1} with 10 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf458f1e7c74f48b2fd00cb21347bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 7.9151\n",
      "step 10 loss 3.6468\n",
      "step 20 loss 4.8702\n",
      "step 30 loss 4.2047\n",
      "step 40 loss 2.2517\n",
      "step 50 loss 2.7754\n",
      "step 60 loss 2.1571\n",
      "step 70 loss 2.1832\n",
      "step 80 loss 3.6761\n",
      "step 90 loss 1.3983\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/nostupidquestions/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a26017ce82c45f997aaa950ab7ef026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 posts from nostupidquestions dataset (100.0%)\n",
      "Found 2000 valid posts in nostupidquestions\n",
      "Loaded dataset {'nostupidquestions': 1} with 20 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cd2e8cce024b49a6d902947e63ffae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 5.7724\n",
      "step 10 loss 6.6342\n",
      "step 20 loss 5.0668\n",
      "step 30 loss 3.8720\n",
      "step 40 loss 4.0133\n",
      "step 50 loss 2.5765\n",
      "step 60 loss 1.3336\n",
      "step 70 loss 2.0728\n",
      "step 80 loss 2.1254\n",
      "step 90 loss 2.8193\n",
      "step 100 loss 2.3732\n",
      "step 110 loss 2.4172\n",
      "step 120 loss 2.2439\n",
      "step 130 loss 2.7838\n",
      "step 140 loss 1.9997\n",
      "step 150 loss 2.2181\n",
      "step 160 loss 2.7349\n",
      "step 170 loss 1.4897\n",
      "step 180 loss 2.1799\n",
      "step 190 loss 1.7490\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/nostupidquestions/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d4b51c8b3e45f5829246d78e95313c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 50 posts from nostupidquestions dataset (100.0%)\n",
      "Found 2000 valid posts in nostupidquestions\n",
      "Loaded dataset {'nostupidquestions': 1} with 50 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20bdddae4a44e18a96c33e47338a22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 8.0128\n",
      "step 10 loss 5.4999\n",
      "step 20 loss 4.4184\n",
      "step 30 loss 4.1618\n",
      "step 40 loss 3.6235\n",
      "step 50 loss 3.0167\n",
      "step 60 loss 3.2615\n",
      "step 70 loss 2.2927\n",
      "step 80 loss 3.5337\n",
      "step 90 loss 1.8666\n",
      "step 100 loss 2.8918\n",
      "step 110 loss 2.3984\n",
      "step 120 loss 2.8494\n",
      "step 130 loss 2.3710\n",
      "step 140 loss 2.7398\n",
      "step 150 loss 2.5398\n",
      "step 160 loss 2.9435\n",
      "step 170 loss 1.9332\n",
      "step 180 loss 2.5083\n",
      "step 190 loss 2.3975\n",
      "step 200 loss 1.7346\n",
      "step 210 loss 2.8295\n",
      "step 220 loss 1.8011\n",
      "step 230 loss 2.4035\n",
      "step 240 loss 1.1497\n",
      "step 250 loss 2.0926\n",
      "step 260 loss 2.6063\n",
      "step 270 loss 2.4476\n",
      "step 280 loss 2.2504\n",
      "step 290 loss 1.8201\n",
      "step 300 loss 1.4653\n",
      "step 310 loss 3.1969\n",
      "step 320 loss 2.7860\n",
      "step 330 loss 1.9307\n",
      "step 340 loss 2.0849\n",
      "step 350 loss 1.8306\n",
      "step 360 loss 2.3536\n",
      "step 370 loss 2.8738\n",
      "step 380 loss 2.4373\n",
      "step 390 loss 2.5353\n",
      "step 400 loss 1.8804\n",
      "step 410 loss 2.7115\n",
      "step 420 loss 2.5466\n",
      "step 430 loss 2.1835\n",
      "step 440 loss 2.6312\n",
      "step 450 loss 0.9426\n",
      "step 460 loss 2.8495\n",
      "step 470 loss 1.8765\n",
      "step 480 loss 2.6951\n",
      "step 490 loss 2.0864\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/nostupidquestions/50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a1f92deaff497d804dc67175b7fcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 posts from nostupidquestions dataset (100.0%)\n",
      "Found 2000 valid posts in nostupidquestions\n",
      "Loaded dataset {'nostupidquestions': 1} with 100 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccb8bd1c77041259d7e7ff43db9998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 4.9072\n",
      "step 10 loss 4.2330\n",
      "step 20 loss 5.7853\n",
      "step 30 loss 5.0320\n",
      "step 40 loss 3.6424\n",
      "step 50 loss 3.7930\n",
      "step 60 loss 2.7013\n",
      "step 70 loss 2.5146\n",
      "step 80 loss 2.3096\n",
      "step 90 loss 2.0404\n",
      "step 100 loss 2.5515\n",
      "step 110 loss 2.1396\n",
      "step 120 loss 2.4089\n",
      "step 130 loss 2.3047\n",
      "step 140 loss 2.9213\n",
      "step 150 loss 1.7193\n",
      "step 160 loss 1.9874\n",
      "step 170 loss 2.5613\n",
      "step 180 loss 1.9156\n",
      "step 190 loss 2.0534\n",
      "step 200 loss 2.3844\n",
      "step 210 loss 2.2340\n",
      "step 220 loss 1.4732\n",
      "step 230 loss 3.6227\n",
      "step 240 loss 2.7518\n",
      "step 250 loss 1.9477\n",
      "step 260 loss 1.9137\n",
      "step 270 loss 2.0871\n",
      "step 280 loss 2.1201\n",
      "step 290 loss 3.4911\n",
      "step 300 loss 2.3241\n",
      "step 310 loss 2.1745\n",
      "step 320 loss 1.9340\n",
      "step 330 loss 1.4510\n",
      "step 340 loss 2.6341\n",
      "step 350 loss 2.5056\n",
      "step 360 loss 1.8115\n",
      "step 370 loss 2.1564\n",
      "step 380 loss 1.9688\n",
      "step 390 loss 1.7413\n",
      "step 400 loss 2.2317\n",
      "step 410 loss 2.3960\n",
      "step 420 loss 1.9914\n",
      "step 430 loss 2.3157\n",
      "step 440 loss 1.7115\n",
      "step 450 loss 2.0300\n",
      "step 460 loss 2.6354\n",
      "step 470 loss 2.5438\n",
      "step 480 loss 1.7054\n",
      "step 490 loss 2.2107\n",
      "step 500 loss 1.9353\n",
      "step 510 loss 3.0092\n",
      "step 520 loss 2.2841\n",
      "step 530 loss 2.2205\n",
      "step 540 loss 1.7420\n",
      "step 550 loss 2.8508\n",
      "step 560 loss 2.0140\n",
      "step 570 loss 3.3350\n",
      "step 580 loss 2.3308\n",
      "step 590 loss 1.8815\n",
      "step 600 loss 1.6963\n",
      "step 610 loss 2.3438\n",
      "step 620 loss 3.4520\n",
      "step 630 loss 2.0673\n",
      "step 640 loss 2.3675\n",
      "step 650 loss 1.6813\n",
      "step 660 loss 2.7943\n",
      "step 670 loss 2.0244\n",
      "step 680 loss 2.4018\n",
      "step 690 loss 1.9898\n",
      "step 700 loss 2.0027\n",
      "step 710 loss 1.7894\n",
      "step 720 loss 1.7898\n",
      "step 730 loss 1.8484\n",
      "step 740 loss 1.2945\n",
      "step 750 loss 2.3204\n",
      "step 760 loss 3.2123\n",
      "step 770 loss 2.2111\n",
      "step 780 loss 1.5942\n",
      "step 790 loss 1.6727\n",
      "step 800 loss 1.9199\n",
      "step 810 loss 2.0276\n",
      "step 820 loss 2.2644\n",
      "step 830 loss 2.4011\n",
      "step 840 loss 1.4817\n",
      "step 850 loss 2.3785\n",
      "step 860 loss 2.6817\n",
      "step 870 loss 1.6533\n",
      "step 880 loss 1.8755\n",
      "step 890 loss 2.3415\n",
      "step 900 loss 2.9314\n",
      "step 910 loss 3.1967\n",
      "step 920 loss 2.5421\n",
      "step 930 loss 2.1045\n",
      "step 940 loss 2.2840\n",
      "step 950 loss 1.8927\n",
      "step 960 loss 1.6793\n",
      "step 970 loss 2.2764\n",
      "step 980 loss 2.2533\n",
      "step 990 loss 1.7205\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/nostupidquestions/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f11c95700c9435ca682fdfd22f7d295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 250 posts from nostupidquestions dataset (100.0%)\n",
      "Found 2000 valid posts in nostupidquestions\n",
      "Loaded dataset {'nostupidquestions': 1} with 250 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b312ab37a27e4a4c8a8ce6319fd620e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 3.5467\n",
      "step 10 loss 7.7952\n",
      "step 20 loss 4.8722\n",
      "step 30 loss 3.4524\n",
      "step 40 loss 2.8702\n",
      "step 50 loss 2.5693\n",
      "step 60 loss 2.8286\n",
      "step 70 loss 2.1986\n",
      "step 80 loss 2.4780\n",
      "step 90 loss 2.0104\n",
      "step 100 loss 2.3073\n",
      "step 110 loss 1.6715\n",
      "step 120 loss 1.5996\n",
      "step 130 loss 1.9800\n",
      "step 140 loss 2.7099\n",
      "step 150 loss 2.3612\n",
      "step 160 loss 1.9690\n",
      "step 170 loss 1.9790\n",
      "step 180 loss 2.5327\n",
      "step 190 loss 2.7815\n",
      "step 200 loss 2.4422\n",
      "step 210 loss 2.3768\n",
      "step 220 loss 1.7372\n",
      "step 230 loss 2.6508\n",
      "step 240 loss 2.4390\n",
      "step 250 loss 2.1860\n",
      "step 260 loss 2.2392\n",
      "step 270 loss 2.2560\n",
      "step 280 loss 2.0490\n",
      "step 290 loss 2.4323\n",
      "step 300 loss 2.7283\n",
      "step 310 loss 1.9428\n",
      "step 320 loss 2.6524\n",
      "step 330 loss 2.1058\n",
      "step 340 loss 2.0866\n",
      "step 350 loss 1.6083\n",
      "step 360 loss 2.5270\n",
      "step 370 loss 1.9967\n",
      "step 380 loss 2.3054\n",
      "step 390 loss 2.3876\n",
      "step 400 loss 2.0182\n",
      "step 410 loss 2.1947\n",
      "step 420 loss 2.9658\n",
      "step 430 loss 2.3839\n",
      "step 440 loss 1.9671\n",
      "step 450 loss 1.7568\n",
      "step 460 loss 2.0932\n",
      "step 470 loss 2.2539\n",
      "step 480 loss 1.7423\n",
      "step 490 loss 2.0109\n",
      "step 500 loss 1.9771\n",
      "step 510 loss 1.8997\n",
      "step 520 loss 2.7669\n",
      "step 530 loss 2.4058\n",
      "step 540 loss 2.7014\n",
      "step 550 loss 2.0095\n",
      "step 560 loss 2.3759\n",
      "step 570 loss 2.1537\n",
      "step 580 loss 2.7474\n",
      "step 590 loss 2.6068\n",
      "step 600 loss 2.3215\n",
      "step 610 loss 1.2584\n",
      "step 620 loss 2.0622\n",
      "step 630 loss 2.4779\n",
      "step 640 loss 2.1759\n",
      "step 650 loss 1.9529\n",
      "step 660 loss 1.6362\n",
      "step 670 loss 2.9183\n",
      "step 680 loss 2.4076\n",
      "step 690 loss 1.8530\n",
      "step 700 loss 2.2348\n",
      "step 710 loss 2.7009\n",
      "step 720 loss 1.1550\n",
      "step 730 loss 2.2155\n",
      "step 740 loss 2.3106\n",
      "step 750 loss 2.1861\n",
      "step 760 loss 1.9357\n",
      "step 770 loss 2.1234\n",
      "step 780 loss 1.7597\n",
      "step 790 loss 2.8898\n",
      "step 800 loss 1.7393\n",
      "step 810 loss 1.8807\n",
      "step 820 loss 2.3931\n",
      "step 830 loss 2.6159\n",
      "step 840 loss 2.1661\n",
      "step 850 loss 2.0299\n",
      "step 860 loss 2.2681\n",
      "step 870 loss 2.1497\n",
      "step 880 loss 2.2022\n",
      "step 890 loss 2.2264\n",
      "step 900 loss 1.8543\n",
      "step 910 loss 1.8217\n",
      "step 920 loss 2.2821\n",
      "step 930 loss 1.8616\n",
      "step 940 loss 1.8252\n",
      "step 950 loss 1.8266\n",
      "step 960 loss 2.4896\n",
      "step 970 loss 2.0206\n",
      "step 980 loss 2.1930\n",
      "step 990 loss 1.9550\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/nostupidquestions/250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a211921bd70d433db2da118a7c50b0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 500 posts from nostupidquestions dataset (100.0%)\n",
      "Found 2000 valid posts in nostupidquestions\n",
      "Loaded dataset {'nostupidquestions': 1} with 500 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bd72d5c8724a83913bd7e0beec0b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 6.2973\n",
      "step 10 loss 3.2555\n",
      "step 20 loss 3.5144\n",
      "step 30 loss 4.9920\n",
      "step 40 loss 3.7571\n",
      "step 50 loss 1.8892\n",
      "step 60 loss 2.4598\n",
      "step 70 loss 2.7903\n",
      "step 80 loss 2.2910\n",
      "step 90 loss 1.8680\n",
      "step 100 loss 2.5391\n",
      "step 110 loss 1.7840\n",
      "step 120 loss 2.2376\n",
      "step 130 loss 2.2962\n",
      "step 140 loss 2.2547\n",
      "step 150 loss 2.2395\n",
      "step 160 loss 1.6916\n",
      "step 170 loss 2.0888\n",
      "step 180 loss 1.9304\n",
      "step 190 loss 1.7932\n",
      "step 200 loss 2.3416\n",
      "step 210 loss 1.6210\n",
      "step 220 loss 2.1042\n",
      "step 230 loss 2.5765\n",
      "step 240 loss 1.3326\n",
      "step 250 loss 2.7416\n",
      "step 260 loss 2.1911\n",
      "step 270 loss 2.9077\n",
      "step 280 loss 2.6689\n",
      "step 290 loss 2.3533\n",
      "step 300 loss 2.3509\n",
      "step 310 loss 2.0563\n",
      "step 320 loss 2.1799\n",
      "step 330 loss 1.2543\n",
      "step 340 loss 2.3480\n",
      "step 350 loss 2.3046\n",
      "step 360 loss 2.0960\n",
      "step 370 loss 1.9746\n",
      "step 380 loss 1.8570\n",
      "step 390 loss 1.9072\n",
      "step 400 loss 2.2504\n",
      "step 410 loss 2.1223\n",
      "step 420 loss 2.5667\n",
      "step 430 loss 2.5758\n",
      "step 440 loss 2.2152\n",
      "step 450 loss 2.1113\n",
      "step 460 loss 2.5017\n",
      "step 470 loss 1.8674\n",
      "step 480 loss 2.3008\n",
      "step 490 loss 1.8341\n",
      "step 500 loss 2.3236\n",
      "step 510 loss 1.8146\n",
      "step 520 loss 2.2488\n",
      "step 530 loss 2.4339\n",
      "step 540 loss 2.0092\n",
      "step 550 loss 2.3765\n",
      "step 560 loss 2.0579\n",
      "step 570 loss 2.6872\n",
      "step 580 loss 2.7561\n",
      "step 590 loss 2.0660\n",
      "step 600 loss 1.7587\n",
      "step 610 loss 1.7640\n",
      "step 620 loss 2.2404\n",
      "step 630 loss 2.0717\n",
      "step 640 loss 1.9551\n",
      "step 650 loss 1.8841\n",
      "step 660 loss 2.2178\n",
      "step 670 loss 2.0160\n",
      "step 680 loss 2.0436\n",
      "step 690 loss 1.2767\n",
      "step 700 loss 2.1764\n",
      "step 710 loss 1.7429\n",
      "step 720 loss 2.6790\n",
      "step 730 loss 1.5286\n",
      "step 740 loss 1.9704\n",
      "step 750 loss 2.2781\n",
      "step 760 loss 2.0887\n",
      "step 770 loss 2.2482\n",
      "step 780 loss 1.7235\n",
      "step 790 loss 1.9651\n",
      "step 800 loss 1.9946\n",
      "step 810 loss 1.9718\n",
      "step 820 loss 2.0783\n",
      "step 830 loss 1.9038\n",
      "step 840 loss 1.1848\n",
      "step 850 loss 2.2896\n",
      "step 860 loss 2.5684\n",
      "step 870 loss 2.3659\n",
      "step 880 loss 1.6090\n",
      "step 890 loss 1.9210\n",
      "step 900 loss 2.1910\n",
      "step 910 loss 1.8888\n",
      "step 920 loss 2.1706\n",
      "step 930 loss 3.8112\n",
      "step 940 loss 2.2671\n",
      "step 950 loss 2.1769\n",
      "step 960 loss 1.8281\n",
      "step 970 loss 2.0957\n",
      "step 980 loss 1.6548\n",
      "step 990 loss 2.6219\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/nostupidquestions/500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26805ad900594895b116d96ea813a093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1000 posts from nostupidquestions dataset (100.0%)\n",
      "Found 2000 valid posts in nostupidquestions\n",
      "Loaded dataset {'nostupidquestions': 1} with 1000 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9174d12c1f6f47d5a4ed907c608c8aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 5.8191\n",
      "step 10 loss 5.6174\n",
      "step 20 loss 4.3457\n",
      "step 30 loss 3.6723\n",
      "step 40 loss 3.0602\n",
      "step 50 loss 2.5285\n",
      "step 60 loss 2.2842\n",
      "step 70 loss 2.9732\n",
      "step 80 loss 2.5144\n",
      "step 90 loss 2.9286\n",
      "step 100 loss 2.0965\n",
      "step 110 loss 2.6175\n",
      "step 120 loss 2.1971\n",
      "step 130 loss 2.2220\n",
      "step 140 loss 1.8648\n",
      "step 150 loss 2.8463\n",
      "step 160 loss 2.3864\n",
      "step 170 loss 2.5889\n",
      "step 180 loss 3.1097\n",
      "step 190 loss 2.8888\n",
      "step 200 loss 2.2743\n",
      "step 210 loss 2.3924\n",
      "step 220 loss 1.6887\n",
      "step 230 loss 1.9744\n",
      "step 240 loss 2.2583\n",
      "step 250 loss 2.7013\n",
      "step 260 loss 1.7205\n",
      "step 270 loss 2.0441\n",
      "step 280 loss 1.8637\n",
      "step 290 loss 2.1139\n",
      "step 300 loss 1.5739\n",
      "step 310 loss 1.7900\n",
      "step 320 loss 2.2568\n",
      "step 330 loss 1.6742\n",
      "step 340 loss 2.1266\n",
      "step 350 loss 2.4728\n",
      "step 360 loss 2.1875\n",
      "step 370 loss 2.2074\n",
      "step 380 loss 2.1712\n",
      "step 390 loss 2.2490\n",
      "step 400 loss 2.6735\n",
      "step 410 loss 1.9026\n",
      "step 420 loss 2.6049\n",
      "step 430 loss 2.4754\n",
      "step 440 loss 2.7904\n",
      "step 450 loss 2.5088\n",
      "step 460 loss 2.7488\n",
      "step 470 loss 2.9497\n",
      "step 480 loss 1.9674\n",
      "step 490 loss 2.0234\n",
      "step 500 loss 2.4364\n",
      "step 510 loss 3.6080\n",
      "step 520 loss 2.3097\n",
      "step 530 loss 2.5245\n",
      "step 540 loss 1.6741\n",
      "step 550 loss 2.3496\n",
      "step 560 loss 2.1206\n",
      "step 570 loss 2.6214\n",
      "step 580 loss 1.4124\n",
      "step 590 loss 2.1268\n",
      "step 600 loss 1.8694\n",
      "step 610 loss 2.6179\n",
      "step 620 loss 1.9062\n",
      "step 630 loss 2.3572\n",
      "step 640 loss 2.0321\n",
      "step 650 loss 2.2967\n",
      "step 660 loss 2.3275\n",
      "step 670 loss 2.4187\n",
      "step 680 loss 2.0555\n",
      "step 690 loss 2.5687\n",
      "step 700 loss 1.7359\n",
      "step 710 loss 2.4680\n",
      "step 720 loss 2.0770\n",
      "step 730 loss 2.3135\n",
      "step 740 loss 2.5822\n",
      "step 750 loss 2.5068\n",
      "step 760 loss 2.1843\n",
      "step 770 loss 2.7071\n",
      "step 780 loss 2.5053\n",
      "step 790 loss 2.5398\n",
      "step 800 loss 2.3123\n",
      "step 810 loss 1.9515\n",
      "step 820 loss 2.0710\n",
      "step 830 loss 1.5640\n",
      "step 840 loss 1.4717\n",
      "step 850 loss 2.6476\n",
      "step 860 loss 2.3005\n",
      "step 870 loss 2.5376\n",
      "step 880 loss 1.5527\n",
      "step 890 loss 1.7681\n",
      "step 900 loss 1.9666\n",
      "step 910 loss 2.3556\n",
      "step 920 loss 1.7529\n",
      "step 930 loss 2.3037\n",
      "step 940 loss 2.2869\n",
      "step 950 loss 2.0451\n",
      "step 960 loss 2.4980\n",
      "step 970 loss 3.0390\n",
      "step 980 loss 2.2829\n",
      "step 990 loss 2.0882\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/nostupidquestions/1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c033ad5636184ac9933337376cfd6242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2000 posts from nostupidquestions dataset (100.0%)\n",
      "Found 2000 valid posts in nostupidquestions\n",
      "Loaded dataset {'nostupidquestions': 1} with 2000 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2e7baedf464023921c673b0f92c72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 5.0470\n",
      "step 10 loss 5.5157\n",
      "step 20 loss 4.4989\n",
      "step 30 loss 4.1899\n",
      "step 40 loss 3.2494\n",
      "step 50 loss 2.8119\n",
      "step 60 loss 2.9556\n",
      "step 70 loss 2.2135\n",
      "step 80 loss 1.9947\n",
      "step 90 loss 1.6207\n",
      "step 100 loss 2.1958\n",
      "step 110 loss 2.5259\n",
      "step 120 loss 2.3184\n",
      "step 130 loss 2.0182\n",
      "step 140 loss 1.8270\n",
      "step 150 loss 2.1318\n",
      "step 160 loss 2.3625\n",
      "step 170 loss 1.8436\n",
      "step 180 loss 1.9621\n",
      "step 190 loss 2.6570\n",
      "step 200 loss 1.5772\n",
      "step 210 loss 2.3780\n",
      "step 220 loss 2.2781\n",
      "step 230 loss 1.9508\n",
      "step 240 loss 2.4718\n",
      "step 250 loss 2.3513\n",
      "step 260 loss 1.7086\n",
      "step 270 loss 1.9756\n",
      "step 280 loss 2.7172\n",
      "step 290 loss 1.7821\n",
      "step 300 loss 2.4019\n",
      "step 310 loss 1.9688\n",
      "step 320 loss 2.6918\n",
      "step 330 loss 1.9202\n",
      "step 340 loss 1.3691\n",
      "step 350 loss 2.2740\n",
      "step 360 loss 2.4064\n",
      "step 370 loss 1.9099\n",
      "step 380 loss 1.4347\n",
      "step 390 loss 2.5885\n",
      "step 400 loss 2.2348\n",
      "step 410 loss 2.0769\n",
      "step 420 loss 1.6511\n",
      "step 430 loss 2.7055\n",
      "step 440 loss 2.0692\n",
      "step 450 loss 1.7161\n",
      "step 460 loss 1.7208\n",
      "step 470 loss 2.6010\n",
      "step 480 loss 1.8465\n",
      "step 490 loss 1.8487\n",
      "step 500 loss 2.0947\n",
      "step 510 loss 2.1085\n",
      "step 520 loss 1.8397\n",
      "step 530 loss 2.2976\n",
      "step 540 loss 1.6161\n",
      "step 550 loss 3.2142\n",
      "step 560 loss 2.7626\n",
      "step 570 loss 2.3267\n",
      "step 580 loss 2.1414\n",
      "step 590 loss 1.8770\n",
      "step 600 loss 1.9721\n",
      "step 610 loss 1.2389\n",
      "step 620 loss 2.1520\n",
      "step 630 loss 1.7630\n",
      "step 640 loss 1.6477\n",
      "step 650 loss 1.4543\n",
      "step 660 loss 2.5183\n",
      "step 670 loss 2.4993\n",
      "step 680 loss 3.1286\n",
      "step 690 loss 1.7730\n",
      "step 700 loss 2.6366\n",
      "step 710 loss 2.3066\n",
      "step 720 loss 2.7249\n",
      "step 730 loss 1.8488\n",
      "step 740 loss 2.5817\n",
      "step 750 loss 2.1775\n",
      "step 760 loss 2.2111\n",
      "step 770 loss 2.1107\n",
      "step 780 loss 2.2264\n",
      "step 790 loss 2.6204\n",
      "step 800 loss 2.1760\n",
      "step 810 loss 2.0089\n",
      "step 820 loss 2.2532\n",
      "step 830 loss 1.9143\n",
      "step 840 loss 1.3820\n",
      "step 850 loss 1.9784\n",
      "step 860 loss 2.5977\n",
      "step 870 loss 1.8855\n",
      "step 880 loss 2.2819\n",
      "step 890 loss 1.8378\n",
      "step 900 loss 2.2261\n",
      "step 910 loss 1.9638\n",
      "step 920 loss 1.7902\n",
      "step 930 loss 2.0935\n",
      "step 940 loss 2.2000\n",
      "step 950 loss 1.8970\n",
      "step 960 loss 2.1540\n",
      "step 970 loss 2.4710\n",
      "step 980 loss 1.7314\n",
      "step 990 loss 2.1834\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/nostupidquestions/2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86abc415fb604791984c93f657c6224f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7724db9875e1459ca4fbcee8a381287a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train sizes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10 posts from copypasta dataset (100.0%)\n",
      "Found 2000 valid posts in copypasta\n",
      "Loaded dataset {'copypasta': 1} with 10 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96cb89d698a466096331df25ac5cf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 3.8862\n",
      "step 10 loss 3.8647\n",
      "step 20 loss 3.3094\n",
      "step 30 loss 3.9612\n",
      "step 40 loss 2.5637\n",
      "step 50 loss 2.5224\n",
      "step 60 loss 2.4953\n",
      "step 70 loss 2.9582\n",
      "step 80 loss 3.5746\n",
      "step 90 loss 2.7935\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/copypasta/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb87c5867654f18894f4edb8eba943d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 posts from copypasta dataset (100.0%)\n",
      "Found 2000 valid posts in copypasta\n",
      "Loaded dataset {'copypasta': 1} with 20 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c685a7269be4413b821d0855d1ea8287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 0.7476\n",
      "step 10 loss 2.9676\n",
      "step 20 loss 4.1379\n",
      "step 30 loss 5.4495\n",
      "step 40 loss 2.6812\n",
      "step 50 loss 2.0732\n",
      "step 60 loss 2.7317\n",
      "step 70 loss 2.5951\n",
      "step 80 loss 4.3091\n",
      "step 90 loss 1.9684\n",
      "step 100 loss 2.7599\n",
      "step 110 loss 2.4441\n",
      "step 120 loss 2.3817\n",
      "step 130 loss 1.9671\n",
      "step 140 loss 2.5008\n",
      "step 150 loss 2.5158\n",
      "step 160 loss 2.3484\n",
      "step 170 loss 2.2968\n",
      "step 180 loss 3.1016\n",
      "step 190 loss 2.2351\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/copypasta/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf5a8a33fb84e73982603a417a4d160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 50 posts from copypasta dataset (100.0%)\n",
      "Found 2000 valid posts in copypasta\n",
      "Loaded dataset {'copypasta': 1} with 50 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeddb6b2b6ae431eafb2233de747928e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset...\n",
      "Training on device: cuda:0\n",
      "step 0 loss 6.4060\n",
      "step 10 loss 4.6519\n",
      "step 20 loss 5.2767\n",
      "step 30 loss 3.5214\n",
      "step 40 loss 1.9853\n",
      "step 50 loss 3.6628\n",
      "step 60 loss 3.6781\n",
      "step 70 loss 1.8015\n",
      "step 80 loss 3.7032\n",
      "step 90 loss 2.0246\n",
      "step 100 loss 2.3385\n",
      "step 110 loss 2.5372\n",
      "step 120 loss 3.1740\n",
      "step 130 loss 4.0036\n",
      "step 140 loss 3.0430\n",
      "step 150 loss 2.2919\n",
      "step 160 loss 3.3811\n",
      "step 170 loss 2.5402\n",
      "step 180 loss 1.8270\n",
      "step 190 loss 2.2648\n",
      "step 200 loss 2.0897\n",
      "step 210 loss 3.4899\n",
      "step 220 loss 3.4568\n",
      "step 230 loss 2.8160\n",
      "step 240 loss 4.7090\n",
      "step 250 loss 2.2507\n",
      "step 260 loss 1.9644\n",
      "step 270 loss 2.9088\n",
      "step 280 loss 2.5174\n",
      "step 290 loss 2.9413\n",
      "step 300 loss 3.5359\n",
      "step 310 loss 2.8395\n",
      "step 320 loss 2.8091\n",
      "step 330 loss 3.2925\n",
      "step 340 loss 0.7139\n",
      "step 350 loss 2.8050\n",
      "step 360 loss 0.8510\n",
      "step 370 loss 2.2055\n",
      "step 380 loss 1.9346\n",
      "step 390 loss 3.1983\n",
      "step 400 loss 1.8103\n",
      "step 410 loss 2.7991\n",
      "step 420 loss 2.4921\n",
      "step 430 loss 1.9583\n",
      "step 440 loss 2.0430\n",
      "step 450 loss 2.1866\n",
      "step 460 loss 3.3256\n",
      "step 470 loss 1.3808\n",
      "step 480 loss 2.6028\n",
      "step 490 loss 2.8064\n",
      "Saved prompt adapter to: models/soft_prompts/google/gemma-3-27b-it/copypasta/50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100cf7c72c5f4f97aa52c68011411bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating posts:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m (json.JSONDecodeError, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m     44\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not read existing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, regenerating\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m         posts = \u001b[43mcollect_posts\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROMPTS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msoft prompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m         write_posts_json(out_path_obj, posts)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# unload model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_posts\u001b[39m\u001b[34m(n, prompt, model, tokenizer)\u001b[39m\n\u001b[32m     12\u001b[39m posts = []\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n), desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating posts\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     post = \u001b[43mgenerate_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# print(f\"generated post {i}\")\u001b[39;00m\n\u001b[32m     16\u001b[39m     posts.append(post)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mgenerate_post\u001b[39m\u001b[34m(prompt, model, tokenizer)\u001b[39m\n\u001b[32m     12\u001b[39m inputs = tokenizer.apply_chat_template(\n\u001b[32m     13\u001b[39m     messages,\n\u001b[32m     14\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     18\u001b[39m ).to(model.device)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Extract the generated text and parse the post format\u001b[39;00m\n\u001b[32m     31\u001b[39m generated_text = tokenizer.decode(outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1975\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1973\u001b[39m             outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(*args, **kwargs)\n\u001b[32m   1974\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1975\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1977\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2870\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2868\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2869\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2870\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2872\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2873\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2874\u001b[39m     outputs,\n\u001b[32m   2875\u001b[39m     model_kwargs,\n\u001b[32m   2876\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2877\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py:1077\u001b[39m, in \u001b[36mGemma3ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[39m\n\u001b[32m   1072\u001b[39m output_hidden_states = (\n\u001b[32m   1073\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1074\u001b[39m )\n\u001b[32m   1075\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py:937\u001b[39m, in \u001b[36mGemma3Model.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **lm_kwargs)\u001b[39m\n\u001b[32m    931\u001b[39m     \u001b[38;5;66;03m# Create the masks\u001b[39;00m\n\u001b[32m    932\u001b[39m     causal_mask_mapping = {\n\u001b[32m    933\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfull_attention\u001b[39m\u001b[33m\"\u001b[39m: create_causal_mask(**mask_kwargs),\n\u001b[32m    934\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msliding_attention\u001b[39m\u001b[33m\"\u001b[39m: create_sliding_window_causal_mask(**mask_kwargs),\n\u001b[32m    935\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3ModelOutputWithPast(\n\u001b[32m    951\u001b[39m     last_hidden_state=outputs.last_hidden_state,\n\u001b[32m    952\u001b[39m     past_key_values=outputs.past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    955\u001b[39m     image_hidden_states=image_features \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    956\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1062\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py:555\u001b[39m, in \u001b[36mGemma3TextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    553\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py:404\u001b[39m, in \u001b[36mGemma3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    403\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_feedforward_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    406\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_feedforward_layernorm(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py:143\u001b[39m, in \u001b[36mGemma3RMSNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# Llama does x.to(float16) * w whilst Gemma3 is (x * w).to(float16)\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n\u001b[32m    146\u001b[39m     output = output * (\u001b[32m1.0\u001b[39m + \u001b[38;5;28mself\u001b[39m.weight.float())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py:140\u001b[39m, in \u001b[36mGemma3RMSNorm._norm\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "login()\n",
    "\n",
    "# training size ablation\n",
    "for model_name in tqdm(MODELS, desc=\"Models\"):\n",
    "    if model_name == \"gpt-5\":\n",
    "        continue\n",
    "\n",
    "    model, tokenizer = load_model(model_name)\n",
    "\n",
    "    for dataset_name, dataset_dict in tqdm(zip(DATASET1_NAMES, DATASETS1), total=len(DATASETS1), desc=\"Datasets1\"):\n",
    "        for train_size in tqdm(TRAIN_SIZES, desc=\"Train sizes\"):\n",
    "            soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/{model_name}/{dataset_name}/{train_size}\"\n",
    "            \n",
    "            # check if soft prompt exists\n",
    "            if not os.path.exists(soft_output_dir):\n",
    "                examples = load_datasets_proportional(dataset_dict, train_size, PROMPTS[\"soft prompt\"])\n",
    "                train_ds = preprocess_dataset(examples, tokenizer)\n",
    "                \n",
    "                peft_model = init_peft_model(model, model_name)\n",
    "                train_soft_prompt(peft_model, tokenizer, train_ds, NUM_TRAIN_STEPS, soft_output_dir)\n",
    "            else:\n",
    "                print(f\"Soft prompt {soft_output_dir} exists, skipping training\")\n",
    "\n",
    "            # apply soft prompt to model\n",
    "            peft_model = apply_peft_adapter(model, soft_output_dir)\n",
    "            \n",
    "            # generate posts\n",
    "            out_path = f\"{GENERATED_OUTPUT_DIR}/train_size_ablation/{model_name}/{dataset_name}/{train_size}.json\"\n",
    "            \n",
    "            # Check if output file already exists and has posts\n",
    "            out_path_obj = Path(out_path)\n",
    "            out_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if out_path_obj.exists():\n",
    "                try:\n",
    "                    with open(out_path_obj, 'r') as f:\n",
    "                        existing_posts = json.load(f)\n",
    "                    if existing_posts:  # Skip if file has posts\n",
    "                        print(f\"Output file {out_path} already exists with posts, skipping generation\")\n",
    "                        continue\n",
    "                except (json.JSONDecodeError, Exception):\n",
    "                    print(f\"Could not read existing file {out_path}, regenerating\")\n",
    "            \n",
    "            posts = collect_posts(100, PROMPTS[\"soft prompt\"], peft_model, tokenizer)\n",
    "            write_posts_json(out_path_obj, posts)\n",
    "    \n",
    "    # unload model\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "login()\n",
    "\n",
    "# experiment ablation\n",
    "for model_name in tqdm(MODELS, desc=\"Models\"):\n",
    "    if model_name == \"gpt-5\":\n",
    "        continue\n",
    "    model, tokenizer = load_model(model_name)\n",
    "    for dataset_name, dataset_dict in tqdm(zip(DATASET2_NAMES, DATASETS2), total=len(DATASETS2), desc=\"Datasets2\"):\n",
    "        for experiment in tqdm(EXPERIMENTS, desc=\"Experiments\"):\n",
    "            prompt = PROMPTS[experiment]\n",
    "            examples = load_datasets_proportional(dataset_dict, 100, prompt)\n",
    "            train_ds = preprocess_dataset(examples, tokenizer)\n",
    "\n",
    "            if experiment == \"soft prompt\":\n",
    "                soft_output_dir = f\"{MODEL_OUTPUT_DIR}/soft_prompts/soft_prompt_{model_name}_{dataset_name}_exp\"\n",
    "                \n",
    "                # check if soft prompt exists\n",
    "                if not os.path.exists(soft_output_dir):\n",
    "                    peft_model = init_peft_model(model, model_name)\n",
    "                    train_soft_prompt(peft_model, tokenizer, train_ds, NUM_TRAIN_STEPS, soft_output_dir)\n",
    "                else:\n",
    "                    print(f\"Soft prompt {soft_output_dir} exists, skipping training\")\n",
    "                model = apply_peft_adapter(model, soft_output_dir)\n",
    "            elif experiment == \"fine tune\":\n",
    "                break  # skip fine tuning\n",
    "            elif experiment == \"summary\":\n",
    "                prompt = PROMPTS[1]\n",
    "            elif experiment == \"like history\":\n",
    "                prompt = PROMPTS[2]\n",
    "            elif experiment == \"self defined\":\n",
    "                prompt = PROMPTS[3]\n",
    "\n",
    "            out_path = f\"{GENERATED_OUTPUT_DIR}/experiment_ablation/{model_name}_{dataset_name}_{experiment}.json\"\n",
    "            with open(out_path, \"w\") as f:\n",
    "                for _ in tqdm(range(100), desc=f\"Generate {dataset_name} {experiment}\"):\n",
    "                    post = generate_post(prompt, model, tokenizer)\n",
    "                    f.write(post + \"\\n\")\n",
    "                    \n",
    "    # unload model\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# judging \n",
    "for model_name in tqdm(MODELS, desc=\"Models\"):\n",
    "    for dataset in tqdm(DATASETS1, desc=\"Datasets1\"):\n",
    "        for train_size in tqdm(TRAIN_SIZES, desc=\"Train sizes\"):\n",
    "            with open(f\"{GENERATED_OUTPUT_DIR}/train_size_ablation/{model_name}_{dataset}_{train_size}.json\", \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i in tqdm(range(0, len(lines), 3), desc=\"Posts\"):\n",
    "                    post_lines = lines[i:i+3]\n",
    "                    post = ''.join(line.strip() for line in post_lines)\n",
    "                    if post:\n",
    "                        gpt_judgement = judge_post_gpt5(post, dataset)\n",
    "                        heuristic_judgement = judge_post_heuristic(post, dataset)\n",
    "                        print(post)\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# judging \n",
    "for model_name in tqdm(MODELS, desc=\"Models\"):   \n",
    "    for dataset in tqdm(DATASETS2, desc=\"Datasets2\"):\n",
    "        for experiment in tqdm(EXPERIMENTS, desc=\"Experiments\"):\n",
    "            with open(f\"{GENERATED_OUTPUT_DIR}/experiment_ablation/{model_name}_{dataset}_{experiment}.json\", \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i in tqdm(range(0, len(lines), 3), desc=\"Posts\"):\n",
    "                    post_lines = lines[i:i+3]\n",
    "                    post = ''.join(line.strip() for line in post_lines)\n",
    "                    if post:\n",
    "                        gpt_judgement = judge_post_gpt5(post, dataset)\n",
    "                        heuristic_judgement = judge_post_heuristic(post, dataset)\n",
    "                        print(post)\n",
    "                        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
