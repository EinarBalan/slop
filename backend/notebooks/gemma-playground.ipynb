{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207820f",
   "metadata": {},
   "source": [
    "### Output-only style tuning with soft prompts (self-contained)\n",
    "\n",
    "This notebook fine-tunes style by training ONLY on assistant outputs (no instructions), using PEFT Prompt Tuning on an instruct model.\n",
    "\n",
    "It will:\n",
    "- Install dependencies in-notebook\n",
    "- Load a chat checkpoint via `transformers`\n",
    "- Configure Prompt Tuning (learn virtual tokens only)\n",
    "- Train on an outputs-only dataset to steer style\n",
    "- Run inference on a normal user prompt\n",
    "\n",
    "Notes:\n",
    "- Adjust `MODEL_ID` to a model you can pull.\n",
    "- Large models need significant VRAM; pick a smaller one if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be2b63",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140c0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers peft accelerate datasets trl einops sentencepiece bitsandbytes jinja2>=3.1.0 dotenv\n",
    "# %pip install -U git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf2cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\", # 0 # very slow, pretty much same quality as 8b on a100\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",  # 1 # good quality, pretty fast\n",
    "    \"openai/gpt-oss-20b\",                   # 2 # good quality, decently quick, but have to deal with thinking \n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",          # 3 # tends to generate the same post over and over (without tuning)\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\",     # 4 # pretty slow > 1 min per post on a100, good quality\n",
    "    \"google/gemma-3-4b-it\",                 # 5 # multimodal, can't get working\n",
    "    \"google/gemma-3-27b-it\",                # 6 # multimodal, can't get working\n",
    "    \"mistralai/Mistral-7B-v0.1\",            # 7 WAITING FOR ACCESS\n",
    "    \"microsoft/phi-4\",                      # 8 14B, pretty slow, low-decent quality, generates same post over and over\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" #9 very fast, have to deal with thinking, decent quality\n",
    "    ]\n",
    "\n",
    "MODEL_ID = models[5] \n",
    "OUTPUT_DIR = \"./softprompt-style-outputs\"\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 0.2\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "PROMPT = \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5fa89",
   "metadata": {},
   "source": [
    "## Load data \n",
    "(make sure to run sampleposts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5a6cdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  91\n",
      "{'instruction': 'Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n', 'output': 'title: What are my exit opportunities in Tax Compliance? (U.K.)\\nself_text: Hi all,  I work for a top 10 firm in the tax compliance department. I have 1 year audit experience, and will have 2.5 years tax compliance experience once I am ACA qualified.   I’m not sure tax is where I want to stay after, but I’m not sure how limited my current role makes my exit opportunities so I’m looking for some general advice on post ACA jobs that I’d be able to go for, e.g management accountant, corporate finance etc.   Thank you.\\nsubreddit: Accounting'}\n"
     ]
    }
   ],
   "source": [
    "# Config and instruction/output dataset\n",
    "from typing import List\n",
    "import json\n",
    "import re\n",
    "\n",
    "datasets = [\n",
    "    \"finance\",\n",
    "    \"interesting\",\n",
    "]\n",
    "\n",
    "examples: List[dict] = []\n",
    "for dataset in datasets:\n",
    "    # Load sampled Reddit posts from JSON created by sample-posts.py\n",
    "    # Each item is a dict with keys: title, subreddit, self_text\n",
    "    with open(f\"../../datasets/{dataset}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        reddit_posts: List[dict] = json.load(f)\n",
    "\n",
    "    # Build dataset in the format: {\"instruction\": PROMPT, \"output\": post}\n",
    "    for p in reddit_posts:\n",
    "        title = p.get(\"title\", \"\")\n",
    "        self_text = p.get(\"self_text\", \"\")\n",
    "        image_url = p.get(\"image_url\", \"\")\n",
    "        \n",
    "        if not self_text or image_url: continue\n",
    "        \n",
    "        subreddit = p.get(\"subreddit\", \"\")\n",
    "        subreddit = re.sub(r\"\\s*(/)?r/\", \"r/\", subreddit)\n",
    "        post = f\"title: {title}\\nself_text: {self_text}\\nsubreddit: {subreddit}\"\n",
    "        examples.append({\"instruction\": PROMPT, \"output\": post})\n",
    "\n",
    "print(\"number of examples: \", len(examples))\n",
    "print(examples[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6ac99",
   "metadata": {},
   "source": [
    "# Load model\n",
    "\n",
    "Make sure to set HUGGING_FACE_HUB_TOKEN environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19dc1945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import dotenv, os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "login(token=os.getenv(\"HUGGING_FACE_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d090ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using single device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73405baa6c8d456da3c56f1e781f2bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47aa924135a249fda4c953d2e4f2aaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4a4c6bccad47d5b6a19b628d21211b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842bf4b03d684aa0ae614f350e4cdf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b15598f86134dcd9c543c9f318d3585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf1cfe997714d87b8cb5452a1aa4618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e869563a86df448db2fa0837a4066cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28803f64332a4d38abf0a0d79432c29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1cfdc7935d4c708f7f3c7f49473bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d282204c9645a1970fb0eac10fa7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e423f352d3a49328bb739ba352d106a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6b3b8d0f704ccd907dbfa8aae4a021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: google/gemma-3-4b-it\n",
      "Model device configuration: {'': device(type='cuda', index=0)}\n",
      "Available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model with proper device management\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "# Device configuration - choose single GPU or multi-GPU\n",
    "USE_MULTI_GPU = True  # Set to True for multi-GPU training\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "    device = torch.device(\"cuda:0\")  # Primary device\n",
    "    device_map = \"auto\"  # Let transformers handle multi-GPU distribution\n",
    "else:\n",
    "    # Single GPU configuration - explicitly set device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device_map = {\"\": device}  # Force all parameters to single device\n",
    "    print(f\"Using single device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "print(\"Loaded:\", MODEL_ID)\n",
    "print(f\"Model device configuration: {device_map}\")\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d41bc2",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f27a82",
   "metadata": {},
   "source": [
    "### Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a generated Reddit post:\n",
      "\n",
      "title: I accidentally became a cat lady and I'm not mad about it\n",
      "self_text: \"I've always thought of myself as a dog person, but after a series of unfortunate events (i.e. my roommate moving out and leaving her cat behind), I found myself solo-parenting a sassy feline named Mr. Whiskers. At first, I was hesitant - I didn't know the first thing about cat care and I was worried I'd be stuck with a furry little dictator. But fast forward 6 months and I'm now the proud owner of 5 (yes, FIVE) cat trees, a catio, and a subscription to CatLadyBox. Mr. Whiskers has taken over my apartment and my heart, and I couldn't be happier. Has anyone else out there accidentally fallen into cat lady-dom? Share your stories!\"\n",
      "subreddit: r/cats<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# prompt = \"Please generate one reddit post (and nothing else). Make sure to stick to the format below exactly. Don't include any extraneous characters like asterisks or other symbols. \\n\\n title: {title} \\n self_text: {self_text} \\n subreddit: {subreddit} \\n Here's an example of the format: \\n\\ntitle: This is the title of the post! \\nself_text: Here's where the content of the post goes. \\nsubreddit: This is the subreddit, or the name of the community the post belongs to.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You generate reddit posts in the given format.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": PROMPT},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "streamer = TextStreamer(tokenizer, \n",
    "                        skip_special_tokens=False,\n",
    "                        skip_prompt=True)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "\tmax_new_tokens=MAX_SEQ_LEN,\n",
    "\ttemperature=0.7,\n",
    "\ttop_p=0.95,\n",
    "\tdo_sample=True,\n",
    "\tstreamer=streamer,\n",
    ")\n",
    "# print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e0408",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1ecd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 163,840 || all params: 4,300,243,312 || trainable%: 0.0038\n"
     ]
    }
   ],
   "source": [
    "# Configure PEFT Prompt Tuning\n",
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=PROMPT_TOKENS,\n",
    "    prompt_tuning_init_text=\"Generate a reddit post.\",\n",
    "    tokenizer_name_or_path=MODEL_ID,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e9acb179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad7a784c7f546bea7976de9c41220a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 91\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess instruction/output dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Build HF dataset from examples [{\"instruction\", \"output\"}]\n",
    "dataset = Dataset.from_list(examples)\n",
    "\n",
    "# Tokenize instruction with chat template, and supervise only the output tokens\n",
    "def tokenize_io(sample):\n",
    "    # Build chat prompt prefix for the user instruction\n",
    "    messages = [{\"role\": \"user\", \"content\": sample[\"instruction\"]}]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"]\n",
    "    output_ids = tokenizer(sample[\"output\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = prompt_ids + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "    labels = ([-100] * len(prompt_ids)) + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Truncate from the left if too long, keeping alignment between inputs and labels\n",
    "    if len(input_ids) > MAX_SEQ_LEN:\n",
    "        input_ids = input_ids[-MAX_SEQ_LEN:]\n",
    "        labels = labels[-MAX_SEQ_LEN:]\n",
    "        attention_mask = attention_mask[-MAX_SEQ_LEN:]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "train_ds = dataset.map(tokenize_io, remove_columns=dataset.column_names)\n",
    "train_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8564581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda:0\n",
      "step 0 loss 6.1731\n",
      "step 10 loss 5.3709\n",
      "step 20 loss 4.0201\n",
      "step 30 loss 3.8768\n",
      "step 40 loss 2.9995\n",
      "step 50 loss 2.3487\n",
      "step 60 loss 2.7495\n",
      "step 70 loss 2.3072\n",
      "step 80 loss 2.6456\n",
      "step 90 loss 2.3731\n",
      "step 100 loss 1.9763\n",
      "step 110 loss 2.7469\n",
      "step 120 loss 2.3299\n",
      "step 130 loss 2.5570\n",
      "step 140 loss 2.6228\n",
      "step 150 loss 3.4514\n",
      "step 160 loss 2.5105\n",
      "step 170 loss 2.3284\n",
      "step 180 loss 2.2292\n",
      "step 190 loss 2.4957\n",
      "step 200 loss 3.0886\n",
      "step 210 loss 2.6961\n",
      "step 220 loss 3.4625\n",
      "step 230 loss 1.9726\n",
      "step 240 loss 2.6976\n",
      "step 250 loss 2.4095\n",
      "step 260 loss 2.4377\n",
      "step 270 loss 2.9709\n",
      "step 280 loss 3.2205\n",
      "step 290 loss 2.6579\n",
      "step 300 loss 2.2539\n",
      "step 310 loss 2.6942\n",
      "step 320 loss 1.9169\n",
      "step 330 loss 2.8303\n",
      "step 340 loss 3.2853\n",
      "step 350 loss 1.8399\n",
      "step 360 loss 2.6713\n",
      "step 370 loss 2.2258\n",
      "step 380 loss 2.4331\n",
      "step 390 loss 3.2679\n",
      "step 400 loss 2.4539\n",
      "step 410 loss 2.6118\n",
      "step 420 loss 2.9980\n",
      "step 430 loss 2.6986\n",
      "step 440 loss 2.0943\n",
      "step 450 loss 2.7482\n",
      "step 460 loss 1.7808\n",
      "step 470 loss 2.1268\n",
      "step 480 loss 2.5185\n",
      "step 490 loss 2.2434\n",
      "step 500 loss 1.8869\n",
      "step 510 loss 2.6184\n",
      "step 520 loss 2.6422\n",
      "step 530 loss 2.9694\n",
      "step 540 loss 2.1653\n",
      "step 550 loss 2.7753\n",
      "step 560 loss 2.2351\n",
      "step 570 loss 2.7988\n",
      "step 580 loss 2.4684\n",
      "step 590 loss 2.5217\n",
      "step 600 loss 3.2722\n",
      "step 610 loss 2.6227\n",
      "step 620 loss 2.4270\n",
      "step 630 loss 2.1759\n",
      "step 640 loss 3.1554\n",
      "step 650 loss 1.8404\n",
      "step 660 loss 2.2219\n",
      "step 670 loss 3.1988\n",
      "step 680 loss 2.4417\n",
      "step 690 loss 2.1807\n",
      "step 700 loss 1.8346\n",
      "step 710 loss 3.0412\n",
      "step 720 loss 1.8474\n",
      "step 730 loss 2.5271\n",
      "step 740 loss 2.7376\n",
      "step 750 loss 2.8817\n",
      "step 760 loss 2.2503\n",
      "step 770 loss 2.5903\n",
      "step 780 loss 2.0380\n",
      "step 790 loss 2.1349\n",
      "step 800 loss 1.7851\n",
      "step 810 loss 2.3543\n",
      "step 820 loss 2.2100\n",
      "step 830 loss 2.4325\n",
      "step 840 loss 2.6035\n",
      "step 850 loss 2.2911\n",
      "step 860 loss 2.8938\n",
      "step 870 loss 2.3854\n",
      "step 880 loss 2.7821\n",
      "step 890 loss 2.4379\n",
      "step 900 loss 2.6535\n",
      "Saved prompt adapter to: ./softprompt-style-outputs\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup and brief training\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "def collate_fn(features):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    batch_size = len(features)\n",
    "    seq_lens = [len(f[\"input_ids\"]) for f in features]\n",
    "    max_len = max(seq_lens)\n",
    "\n",
    "    input_ids = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "    labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "\n",
    "    for i, f in enumerate(features):\n",
    "        ids = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
    "        attn = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
    "        labs = torch.tensor(f[\"labels\"], dtype=torch.long)\n",
    "        L = ids.size(0)\n",
    "        input_ids[i, :L] = ids\n",
    "        attention_mask[i, :L] = attn\n",
    "        labels[i, :L] = labs\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=MICRO_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# Total optimizer steps we intend to take\n",
    "total_optim_steps = NUM_TRAIN_STEPS\n",
    "num_warmup_steps = max(1, int(0.1 * total_optim_steps))\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_optim_steps,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Ensure model is on the correct device(s)\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1 and device_map == \"auto\":\n",
    "    # For multi-GPU with device_map=\"auto\", model is already distributed\n",
    "    # Get the device of the first parameter for data placement\n",
    "    model_device = next(model.parameters()).device\n",
    "else:\n",
    "    # For single GPU, ensure model is on the specified device\n",
    "    model = model.to(device)\n",
    "    model_device = device\n",
    "\n",
    "print(f\"Training on device: {model_device}\")\n",
    "\n",
    "optimizer.zero_grad()\n",
    "optim_step = 0\n",
    "accumulated = 0\n",
    "running_loss = 0.0\n",
    "for epoch in range(10):  # repeat over dataset until reaching desired steps\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(model_device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        (loss / GRAD_ACCUM_STEPS).backward()\n",
    "        running_loss += loss.item()\n",
    "        accumulated += 1\n",
    "        if accumulated % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if optim_step % 10 == 0:\n",
    "                print(f\"step {optim_step} loss {running_loss / GRAD_ACCUM_STEPS:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            optim_step += 1\n",
    "            if optim_step >= total_optim_steps:\n",
    "                break\n",
    "    if optim_step >= total_optim_steps:\n",
    "        break\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved prompt adapter to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5aa429bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5dc28ce8e14ac892a7d5b23fec8683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import TextStreamer, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Use the same device configuration as training\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1:\n",
    "    device = torch.device(\"cuda:0\")  # Primary device\n",
    "    device_map = \"auto\"  # Let transformers handle multi-GPU distribution\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device_map = {\"\": device}  # Force all parameters to single device\n",
    "\n",
    "# Reload base + adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "base.eval()\n",
    "\n",
    "# For inference, get the correct device\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1 and device_map == \"auto\":\n",
    "    inference_device = next(base.parameters()).device\n",
    "else:\n",
    "    inference_device = device\n",
    "    \n",
    "print(f\"Inference device: {inference_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7dd30cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: I'm in a long-term relationship with someone who is obsessed with their health and fitness, and it's starting to affect me.\n",
      "self_text: Hi everyone,  I’ve been dating my girlfriend for about two years now, and she's incredibly passionate about her health and fitness. She goes to the gym almost every day, eats very healthy (mostly just salads and grilled chicken), and constantly monitors her macros. At first, I was really supportive of her goals and admired her dedication. But lately, it's started to feel like it's affecting our relationship negatively.  She often makes comments about how \"unhealthy\" I am because I enjoy eating pizza or having a beer with friends. She’ll sometimes guilt trip me into joining her on workouts when I don't want to, and she gets upset if we order takeout instead of cooking something “clean.”  It's gotten to the point where I find myself avoiding certain activities with her because I know it will lead to these conversations.  I love her, but I'm starting to feel like I'm being judged and pressured all the time.  I'm not trying to be unhealthy, but I also don't want to feel like I need to change everything I do to fit her standards.  How can I address this without hurting her feelings?  Thanks for your advice.\n",
      "subreddit: relationships\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, \n",
    "                        skip_special_tokens=True,\n",
    "                        skip_prompt=True\n",
    "                        )\n",
    "\n",
    "# Build chat-formatted inputs via the model's chat template\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": PROMPT},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(base.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_SEQ_LEN,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        streamer=streamer,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multi-GPU Training Setup and Utils\n",
    "\n",
    "# If you want to enable multi-GPU training, run this cell first:\n",
    "\n",
    "def setup_multi_gpu_training():\n",
    "    \"\"\"\n",
    "    Setup for proper multi-GPU training with PyTorch.\n",
    "    This provides several strategies for multi-GPU training.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.nn.parallel import DataParallel, DistributedDataParallel\n",
    "    import os\n",
    "    \n",
    "    print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    if torch.cuda.device_count() < 2:\n",
    "        print(\"Warning: Less than 2 GPUs available. Multi-GPU training not possible.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def enable_multi_gpu_mode():\n",
    "    \"\"\"\n",
    "    Call this to switch to multi-GPU mode.\n",
    "    You'll need to restart the kernel and re-run cells after changing this.\n",
    "    \"\"\"\n",
    "    global USE_MULTI_GPU\n",
    "    USE_MULTI_GPU = True\n",
    "    print(\"Multi-GPU mode enabled. Please restart kernel and re-run all cells.\")\n",
    "    print(\"Alternative approaches for multi-GPU training:\")\n",
    "    print(\"1. Use device_map='auto' (current approach)\")\n",
    "    print(\"2. Use torch.nn.DataParallel (simpler but less efficient)\")\n",
    "    print(\"3. Use torch.nn.DistributedDataParallel (most efficient)\")\n",
    "\n",
    "# Check GPU setup\n",
    "setup_multi_gpu_training()\n",
    "\n",
    "# Uncomment the next line to enable multi-GPU training:\n",
    "# enable_multi_gpu_mode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e230bce",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
