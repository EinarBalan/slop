{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82be2b63",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140c0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers peft accelerate datasets trl einops sentencepiece bitsandbytes jinja2>=3.1.0 dotenv\n",
    "# %pip install -U git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\", # 0 # very slow, pretty much same quality as 8b on a100\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",  # 1 # good quality, pretty fast\n",
    "    \"openai/gpt-oss-20b\",                   # 2 # good quality, decently quick, but have to deal with thinking \n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",          # 3 # tends to generate the same post over and over (without tuning)\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\",     # 4 # pretty slow > 1 min per post on a100, good quality\n",
    "    \"google/gemma-3-4b-it\",                 # 5 # multimodal, good quality\n",
    "    \"google/gemma-3-27b-it\",                # 6 # multimodal, good quality\n",
    "    \"mistralai/Mistral-7B-v0.1\",            # 7 WAITING FOR ACCESS\n",
    "    \"microsoft/phi-4\",                      # 8 14B, pretty slow, low-decent quality, generates same post over and over\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" #9 very fast, have to deal with thinking, decent quality\n",
    "    ]\n",
    "\n",
    "MODEL_ID = models[5] \n",
    "OUTPUT_DIR = \"./lora-style-outputs\"\n",
    "PROMPT_TOKENS = 64\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_TRAIN_STEPS = 1000  \n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "PROMPT = \"Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5fa89",
   "metadata": {},
   "source": [
    "## Load data \n",
    "(make sure to run sampleposts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5a6cdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 50 posts from ucla dataset (50.0%)\n",
      "Found 500 valid posts in ucla\n",
      "Loading 50 posts from minecraft dataset (50.0%)\n",
      "Found 500 valid posts in minecraft\n",
      "Total number of examples loaded: 100\n",
      "Sample example:\n",
      "{'instruction': 'Please generate one reddit post. Use this format. \\n\\ntitle: {title}\\n self_text: {self_text}\\n subreddit: {subreddit}\\n', 'output': 'title: How to spread Mycelium over very large areas?\\nself_text: I want to turn my steampunk village into a land impoverished by massive industrial use, and I found Mycelium has got the right look to replace all the grass in my village.  The problem is, by eyeball I would need to replace somewhere between 20k and 30k grass blocks if I include the surrounding mountains. The prospect is tedious.  Is there any way to make Mycelium spread to blocks that are already covered in grass? Or is there any way to shortcut through the process or manually replacing the blocks (except with cheats/creative mode)? Is there an efficient way to do this at all?\\nsubreddit: Minecraft'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "def load_datasets_proportional(datasets_dict: Dict[str, float], total_posts: int) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load datasets with proportional sampling.\n",
    "    \n",
    "    Args:\n",
    "        datasets_dict: Dictionary mapping dataset names to their proportions (0-1)\n",
    "        total_posts: Total number of posts desired across all datasets\n",
    "    \n",
    "    Returns:\n",
    "        List of examples in the format: {\"instruction\": PROMPT, \"output\": post}\n",
    "    \"\"\"\n",
    "    # Validate proportions sum to approximately 1\n",
    "    total_proportion = sum(datasets_dict.values())\n",
    "    if not (0.99 <= total_proportion <= 1.01):\n",
    "        print(f\"Warning: Proportions sum to {total_proportion:.3f}, not 1.0\")\n",
    "    \n",
    "    examples: List[dict] = []\n",
    "    \n",
    "    for dataset_name, proportion in datasets_dict.items():\n",
    "        # Calculate number of posts for this dataset\n",
    "        target_count = int(total_posts * proportion)\n",
    "        print(f\"Loading {target_count} posts from {dataset_name} dataset ({proportion*100:.1f}%)\")\n",
    "        \n",
    "        # Load sampled Reddit posts from JSON created by sample-posts.py\n",
    "        # Each item is a dict with keys: title, subreddit, self_text\n",
    "        try:\n",
    "            with open(f\"../../datasets/{dataset_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                reddit_posts: List[dict] = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find dataset file for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter valid posts (must have self_text and no image_url)\n",
    "        valid_posts = []\n",
    "        for p in reddit_posts:\n",
    "            title = p.get(\"title\", \"\")\n",
    "            self_text = p.get(\"self_text\", \"\")\n",
    "            image_url = p.get(\"image_url\", \"\")\n",
    "            \n",
    "            if self_text and not image_url:\n",
    "                subreddit = p.get(\"subreddit\", \"\")\n",
    "                subreddit = re.sub(r\"\\s*(/)?r/\", \"r/\", subreddit)\n",
    "                post = f\"title: {title}\\nself_text: {self_text}\\nsubreddit: {subreddit}\"\n",
    "                valid_posts.append({\"instruction\": PROMPT, \"output\": post})\n",
    "        \n",
    "        print(f\"Found {len(valid_posts)} valid posts in {dataset_name}\")\n",
    "        \n",
    "        # Sample the target number of posts\n",
    "        if len(valid_posts) >= target_count:\n",
    "            # Randomly sample target_count posts\n",
    "            sampled_posts = random.sample(valid_posts, target_count)\n",
    "        else:\n",
    "            # Use all available posts if we don't have enough\n",
    "            print(f\"Warning: Only {len(valid_posts)} posts available, using all\")\n",
    "            sampled_posts = valid_posts\n",
    "        \n",
    "        examples.extend(sampled_posts)\n",
    "    \n",
    "    # Shuffle the final dataset to mix posts from different datasets\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Example usage - modify these values as needed\n",
    "datasets_dict = {\n",
    "    \"ucla\": 0.5,  # 100% minecraft posts\n",
    "    \"minecraft\": 0.5,  \n",
    "}\n",
    "total_posts = 100  # Total number of posts desired\n",
    "\n",
    "examples = load_datasets_proportional(datasets_dict, total_posts)\n",
    "\n",
    "print(f\"Total number of examples loaded: {len(examples)}\")\n",
    "if examples:\n",
    "    print(\"Sample example:\")\n",
    "    print(examples[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6ac99",
   "metadata": {},
   "source": [
    "# Load model\n",
    "\n",
    "Make sure to set HUGGING_FACE_HUB_TOKEN environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19dc1945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import dotenv, os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "login(token=os.getenv(\"HUGGING_FACE_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d090ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using single device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a5ccebb0df4c1b976da215d8abc6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: google/gemma-3-4b-it\n",
      "Model device configuration: {'': device(type='cuda', index=0)}\n",
      "Available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model with proper device management\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "# Device configuration - choose single GPU or multi-GPU\n",
    "USE_MULTI_GPU = True  # Set to True for multi-GPU training\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "    device = torch.device(\"cuda:0\")  # Primary device\n",
    "    device_map = \"auto\"  # Let transformers handle multi-GPU distribution\n",
    "else:\n",
    "    # Single GPU configuration - explicitly set device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device_map = {\"\": device}  # Force all parameters to single device\n",
    "    print(f\"Using single device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "print(\"Loaded:\", MODEL_ID)\n",
    "print(f\"Model device configuration: {device_map}\")\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d41bc2",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f27a82",
   "metadata": {},
   "source": [
    "### Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f8444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, please provide me with the details for the Reddit post you want me to generate! I need the following information to fill in the template:\n",
      "\n",
      "*   **title:** (The title of the post)\n",
      "*   **self_text:** (The main body of the post - the actual text that will be posted)\n",
      "*   **subreddit:** (The subreddit where you want to post it)\n",
      "\n",
      "Once you give me those three things, Iâ€™ll format it exactly as you requested. ðŸ˜Š<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# prompt = \"Please generate one reddit post (and nothing else). Make sure to stick to the format below exactly. Don't include any extraneous characters like asterisks or other symbols. \\n\\n title: {title} \\n self_text: {self_text} \\n subreddit: {subreddit} \\n Here's an example of the format: \\n\\ntitle: This is the title of the post! \\nself_text: Here's where the content of the post goes. \\nsubreddit: This is the subreddit, or the name of the community the post belongs to.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You generate reddit posts in the given format.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": PROMPT},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "streamer = TextStreamer(tokenizer, \n",
    "                        skip_special_tokens=False,\n",
    "                        skip_prompt=True)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "\tmax_new_tokens=MAX_SEQ_LEN,\n",
    "\ttemperature=0.7,\n",
    "\ttop_p=0.95,\n",
    "\tdo_sample=True,\n",
    "\tstreamer=streamer,\n",
    ")\n",
    "# print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e0408",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 163,840 || all params: 4,300,243,312 || trainable%: 0.0038\n"
     ]
    }
   ],
   "source": [
    "# Configure PEFT LoRA\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e9acb179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934dee01e1b44f14b9abc38d240cfd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess instruction/output dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Build HF dataset from examples [{\"instruction\", \"output\"}]\n",
    "dataset = Dataset.from_list(examples)\n",
    "\n",
    "# Tokenize instruction with chat template, and supervise only the output tokens\n",
    "def tokenize_io(sample):\n",
    "    # Build chat prompt prefix for the user instruction\n",
    "    messages = [{\"role\": \"user\", \"content\": sample[\"instruction\"]}]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"]\n",
    "    output_ids = tokenizer(sample[\"output\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = prompt_ids + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "    labels = ([-100] * len(prompt_ids)) + output_ids + ([eos_id] if eos_id is not None else [])\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Truncate from the left if too long, keeping alignment between inputs and labels\n",
    "    if len(input_ids) > MAX_SEQ_LEN:\n",
    "        input_ids = input_ids[-MAX_SEQ_LEN:]\n",
    "        labels = labels[-MAX_SEQ_LEN:]\n",
    "        attention_mask = attention_mask[-MAX_SEQ_LEN:]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "train_ds = dataset.map(tokenize_io, remove_columns=dataset.column_names)\n",
    "train_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8564581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda:0\n",
      "step 0 loss 10.9245\n",
      "step 10 loss 10.3352\n",
      "step 20 loss 10.5659\n",
      "step 30 loss 6.9277\n",
      "step 40 loss 5.0669\n",
      "step 50 loss 5.4028\n",
      "step 60 loss 3.4549\n",
      "step 70 loss 2.2578\n",
      "step 80 loss 3.1206\n",
      "step 90 loss 2.3812\n",
      "step 100 loss 3.5285\n",
      "step 110 loss 1.9654\n",
      "step 120 loss 2.5969\n",
      "step 130 loss 2.5752\n",
      "step 140 loss 3.5270\n",
      "step 150 loss 2.2049\n",
      "step 160 loss 3.4621\n",
      "step 170 loss 2.8819\n",
      "step 180 loss 2.3326\n",
      "step 190 loss 2.9377\n",
      "step 200 loss 2.5657\n",
      "step 210 loss 1.9290\n",
      "step 220 loss 2.5735\n",
      "step 230 loss 2.4257\n",
      "step 240 loss 2.2963\n",
      "step 250 loss 3.1765\n",
      "step 260 loss 1.7853\n",
      "step 270 loss 2.2927\n",
      "step 280 loss 2.5812\n",
      "step 290 loss 2.2205\n",
      "step 300 loss 2.4991\n",
      "step 310 loss 2.1616\n",
      "step 320 loss 2.1505\n",
      "step 330 loss 2.7876\n",
      "step 340 loss 2.4344\n",
      "step 350 loss 3.2336\n",
      "step 360 loss 1.9318\n",
      "step 370 loss 2.7044\n",
      "step 380 loss 3.1333\n",
      "step 390 loss 2.8938\n",
      "step 400 loss 2.7853\n",
      "step 410 loss 2.4263\n",
      "step 420 loss 2.4813\n",
      "step 430 loss 2.0607\n",
      "step 440 loss 2.6551\n",
      "step 450 loss 2.3264\n",
      "step 460 loss 2.0593\n",
      "step 470 loss 2.9672\n",
      "step 480 loss 2.3063\n",
      "step 490 loss 2.0332\n",
      "step 500 loss 2.0879\n",
      "step 510 loss 1.6325\n",
      "step 520 loss 1.5197\n",
      "step 530 loss 2.1017\n",
      "step 540 loss 2.0771\n",
      "step 550 loss 2.1870\n",
      "step 560 loss 2.0122\n",
      "step 570 loss 2.2925\n",
      "step 580 loss 2.1319\n",
      "step 590 loss 2.9616\n",
      "step 600 loss 2.7371\n",
      "step 610 loss 1.7063\n",
      "step 620 loss 2.6956\n",
      "step 630 loss 2.6832\n",
      "step 640 loss 2.8332\n",
      "step 650 loss 3.1061\n",
      "step 660 loss 2.2547\n",
      "step 670 loss 2.2228\n",
      "step 680 loss 2.2741\n",
      "step 690 loss 3.2256\n",
      "step 700 loss 1.6243\n",
      "step 710 loss 2.6593\n",
      "step 720 loss 2.3106\n",
      "step 730 loss 2.4355\n",
      "step 740 loss 2.3988\n",
      "step 750 loss 1.7057\n",
      "step 760 loss 2.3215\n",
      "step 770 loss 2.4364\n",
      "step 780 loss 2.3863\n",
      "step 790 loss 2.7955\n",
      "step 800 loss 2.6116\n",
      "step 810 loss 1.8710\n",
      "step 820 loss 2.2115\n",
      "step 830 loss 1.4762\n",
      "step 840 loss 2.8602\n",
      "step 850 loss 2.5013\n",
      "step 860 loss 2.5380\n",
      "step 870 loss 1.8274\n",
      "step 880 loss 2.4044\n",
      "step 890 loss 1.8107\n",
      "step 900 loss 2.3915\n",
      "step 910 loss 2.1821\n",
      "step 920 loss 2.3358\n",
      "step 930 loss 2.6895\n",
      "step 940 loss 1.5467\n",
      "step 950 loss 1.8449\n",
      "step 960 loss 2.5516\n",
      "step 970 loss 1.5995\n",
      "step 980 loss 1.9677\n",
      "step 990 loss 2.4376\n",
      "Saved prompt adapter to: ./softprompt-style-outputs\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup and brief training\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "def collate_fn(features):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    batch_size = len(features)\n",
    "    seq_lens = [len(f[\"input_ids\"]) for f in features]\n",
    "    max_len = max(seq_lens)\n",
    "\n",
    "    input_ids = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "    labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "\n",
    "    for i, f in enumerate(features):\n",
    "        ids = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
    "        attn = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
    "        labs = torch.tensor(f[\"labels\"], dtype=torch.long)\n",
    "        L = ids.size(0)\n",
    "        input_ids[i, :L] = ids\n",
    "        attention_mask[i, :L] = attn\n",
    "        labels[i, :L] = labs\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=MICRO_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# Total optimizer steps we intend to take\n",
    "total_optim_steps = NUM_TRAIN_STEPS\n",
    "num_warmup_steps = max(1, int(0.1 * total_optim_steps))\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_optim_steps,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Ensure model is on the correct device(s)\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1 and device_map == \"auto\":\n",
    "    # For multi-GPU with device_map=\"auto\", model is already distributed\n",
    "    # Get the device of the first parameter for data placement\n",
    "    model_device = next(model.parameters()).device\n",
    "else:\n",
    "    # For single GPU, ensure model is on the specified device\n",
    "    model = model.to(device)\n",
    "    model_device = device\n",
    "\n",
    "print(f\"Training on device: {model_device}\")\n",
    "\n",
    "optimizer.zero_grad()\n",
    "optim_step = 0\n",
    "accumulated = 0\n",
    "running_loss = 0.0\n",
    "for epoch in range(10):  # repeat over dataset until reaching desired steps\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(model_device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        (loss / GRAD_ACCUM_STEPS).backward()\n",
    "        running_loss += loss.item()\n",
    "        accumulated += 1\n",
    "        if accumulated % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if optim_step % 10 == 0:\n",
    "                print(f\"step {optim_step} loss {running_loss / GRAD_ACCUM_STEPS:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            optim_step += 1\n",
    "            if optim_step >= total_optim_steps:\n",
    "                break\n",
    "    if optim_step >= total_optim_steps:\n",
    "        break\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved LoRA adapter to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa429bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a485da3c5f49fe8ea443ce1db5bce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import TextStreamer, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Use the same device configuration as training\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1:\n",
    "    device = torch.device(\"cuda:0\")  # Primary device\n",
    "    device_map = \"auto\"  # Let transformers handle multi-GPU distribution\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device_map = {\"\": device}  # Force all parameters to single device\n",
    "\n",
    "# Reload base + adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "base.eval()\n",
    "\n",
    "# For inference, get the correct device\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1 and device_map == \"auto\":\n",
    "    inference_device = next(base.parameters()).device\n",
    "else:\n",
    "    inference_device = device\n",
    "    \n",
    "print(f\"Inference device: {inference_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7dd30cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: How many classes can I take in a semester?\n",
      "self_text: I'm considering taking 15 credit hours this fall, but I'm not sure if that's even possible. Some people seem to be able to take more than that! Can anyone tell me the maximum number of classes I can take in a single semester?\n",
      "subreddit: ucla\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, \n",
    "                        skip_special_tokens=True,\n",
    "                        skip_prompt=True\n",
    "                        )\n",
    "\n",
    "# Build chat-formatted inputs via the model's chat template\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": PROMPT},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(base.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_SEQ_LEN,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        streamer=streamer,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d68598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "GPU 0: NVIDIA H200\n",
      "Warning: Less than 2 GPUs available. Multi-GPU training not possible.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multi-GPU Training Setup and Utils\n",
    "\n",
    "# If you want to enable multi-GPU training, run this cell first:\n",
    "\n",
    "def setup_multi_gpu_training():\n",
    "    \"\"\"\n",
    "    Setup for proper multi-GPU training with PyTorch.\n",
    "    This provides several strategies for multi-GPU training.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.nn.parallel import DataParallel, DistributedDataParallel\n",
    "    import os\n",
    "    \n",
    "    print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    if torch.cuda.device_count() < 2:\n",
    "        print(\"Warning: Less than 2 GPUs available. Multi-GPU training not possible.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def enable_multi_gpu_mode():\n",
    "    \"\"\"\n",
    "    Call this to switch to multi-GPU mode.\n",
    "    You'll need to restart the kernel and re-run cells after changing this.\n",
    "    \"\"\"\n",
    "    global USE_MULTI_GPU\n",
    "    USE_MULTI_GPU = True\n",
    "    print(\"Multi-GPU mode enabled. Please restart kernel and re-run all cells.\")\n",
    "    print(\"Alternative approaches for multi-GPU training:\")\n",
    "    print(\"1. Use device_map='auto' (current approach)\")\n",
    "    print(\"2. Use torch.nn.DataParallel (simpler but less efficient)\")\n",
    "    print(\"3. Use torch.nn.DistributedDataParallel (most efficient)\")\n",
    "\n",
    "# Check GPU setup\n",
    "setup_multi_gpu_training()\n",
    "\n",
    "# Uncomment the next line to enable multi-GPU training:\n",
    "# enable_multi_gpu_mode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e230bce",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
