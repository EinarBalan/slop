{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68802773",
   "metadata": {},
   "source": [
    "### RAG Playground: Llama 3 8B Instruct + Reddit-style Few-shot from JSON\n",
    "\n",
    "Goal: retrieve example Reddit posts from `./data/post-sample.json` and condition Llama 3 8B Instruct to generate one stylistically similar post.\n",
    "\n",
    "Prompt used: \"generate a reddit post that the user is likely to enjoy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b02ff364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers==4.55.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dffbff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers==4.55.2 sentence-transformers faiss-cpu datasets einops peft accelerate bitsandbytes jinja2>=3.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07f8d57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,\n",
       " 'title: 3070ti, 6900xt or wait for new cards?\\nself_text: Backstory: I ordered a $500 34\" qhd 144hz monitor off best buy and they accidentally shipped me 2 of them so now I\\'m going to return one lol. I ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and clean dataset; assemble corpus strings\n",
    "from typing import List, Dict\n",
    "import json, re, os\n",
    "\n",
    "DATA_PATH = \"./data/post-sample.json\"\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Missing {DATA_PATH}\"\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_posts: List[Dict] = json.load(f)\n",
    "\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"^\\s*View\\s+More\\s+Posts\\s*$\",\n",
    "    r\"^\\s*View\\s+Post\\s*$\",\n",
    "    r\"^\\s*Help\\??\\s*$\",\n",
    "    r\"^\\s*Edit:\\s*.*$\",\n",
    "]\n",
    "boilerplate_regexes = [re.compile(p, flags=re.IGNORECASE) for p in BOILERPLATE_PATTERNS]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"\\r\\n?\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\")]\n",
    "    kept = []\n",
    "    for ln in lines:\n",
    "        if any(rx.match(ln) for rx in boilerplate_regexes):\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "    return \"\\n\".join(kept).strip()\n",
    "\n",
    "corpus: List[str] = []\n",
    "meta: List[Dict] = []\n",
    "for p in raw_posts:\n",
    "    title = clean_text(p.get(\"title\", \"\"))\n",
    "    self_text = clean_text(p.get(\"self_text\", \"\"))\n",
    "    subreddit = clean_text(p.get(\"subreddit\", \"\"))\n",
    "    subreddit = re.sub(r\"\\s*(/)?r/\", \"r/\", subreddit)\n",
    "    doc = f\"title: {title}\\nself_text: {self_text}\\nsubreddit: {subreddit}\"\n",
    "    corpus.append(doc)\n",
    "    meta.append({\"title\": title, \"subreddit\": subreddit})\n",
    "\n",
    "len(corpus), corpus[0][:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "384a9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal embedding + retrieval without sentence-transformers/faiss\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "_emb_tok = AutoTokenizer.from_pretrained(EMBED_MODEL, use_fast=True)\n",
    "_emb_model = AutoModel.from_pretrained(EMBED_MODEL)\n",
    "_emb_model.eval()\n",
    "_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_emb_model.to(_device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _encode_texts(texts):\n",
    "    toks = _emb_tok(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(_device)\n",
    "    out = _emb_model(**toks)\n",
    "    token_embs = out.last_hidden_state\n",
    "    mask = toks[\"attention_mask\"].unsqueeze(-1)\n",
    "    pooled = (token_embs * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "    return pooled.cpu().numpy()\n",
    "\n",
    "embeddings = _encode_texts(corpus)\n",
    "\n",
    "def retrieve_idxs(query: str, k: int = 6):\n",
    "    q = _encode_texts([query])[0]\n",
    "    scores = embeddings @ q\n",
    "    k = min(k, len(scores))\n",
    "    topk = np.argpartition(-scores, kth=k-1)[:k]\n",
    "    topk = topk[np.argsort(-scores[topk])]\n",
    "    return topk, scores[topk]\n",
    "\n",
    "# Override retrieval to use new index\n",
    "from typing import List\n",
    "\n",
    "def retrieve_examples(query: str, k: int = 6) -> List[str]:\n",
    "    idxs, _ = retrieve_idxs(query, k=k)\n",
    "    return [corpus[i] for i in idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cc2183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow<2.20,>=2.19 in /usr/lib/python3/dist-packages (from tf-keras) (2.19.0)\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae77ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 26.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed corpus and build FAISS index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "embeddings = embedder.encode(corpus, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fb1ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a writing assistant that outputs exactly one reddit post in the format:\n",
      "title: ...\n",
      "self_text: ...\n",
      "subreddit: r/...\n",
      "\n",
      "\n",
      "Here are style examples:\n",
      "\n",
      "title: I need answers\n",
      "self_text: Why is u/EdwardAdelesame such an andrew tate dick rider??? Like what could he possibly gain\n",
      "subreddit: teenagers\n",
      "\n",
      "title: Discord server if you want it\n",
      "self_text: Dm me for it I don‚Äôt want too many people in it‚Ä¶ so hurry I guess\n",
      "subreddit: teenagers\n",
      "\n",
      "title: :] is better than :)\n",
      "self_text: i love his gremlin energy :\n"
     ]
    }
   ],
   "source": [
    "# Retrieval and prompt construction\n",
    "from typing import List, Tuple\n",
    "\n",
    "SYSTEM_STYLE = (\n",
    "    \"You are a writing assistant that outputs exactly one reddit post in the format:\\n\"\n",
    "    \"title: ...\\nself_text: ...\\nsubreddit: r/...\\n\"\n",
    ")\n",
    "\n",
    "USER_TASK = \"generate a reddit post that the user is likely to enjoy\"\n",
    "\n",
    "def retrieve_examples(query: str, k: int = 6) -> List[str]:\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    scores, idxs = index.search(q_emb, k)\n",
    "    return [corpus[i] for i in idxs[0]]\n",
    "\n",
    "def build_fewshot_prompt(query: str, k: int = 6) -> str:\n",
    "    examples = retrieve_examples(query, k=k)\n",
    "    exemplars = \"\\n\\n\".join(examples)\n",
    "    return (\n",
    "        f\"{SYSTEM_STYLE}\\n\\nHere are style examples:\\n\\n{exemplars}\\n\\nTask: {USER_TASK}\\n\"\n",
    "    )\n",
    "\n",
    "prompt = build_fewshot_prompt(USER_TASK, k=8)\n",
    "print(prompt[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e2ddd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b8aa75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace token from .env\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e530c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d409eebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:20<00:00,  5.14s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 3 8B Instruct and generate\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "STOP_TOKENS = [\"\\n\\ntitle:\"]\n",
    "\n",
    "def generate_with_rag(query: str, k: int = 8, max_new_tokens: int = 256,\n",
    "                      temperature: float = 0.7, top_p: float = 0.9,\n",
    "                      repetition_penalty: float = 1.05) -> str:\n",
    "    prompt = build_fewshot_prompt(query, k=k)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    gen = tokenizer.decode(out.sequences[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    # Keep only the first post shape\n",
    "    first_idx = gen.find(\"title:\")\n",
    "    if first_idx != -1:\n",
    "        gen = gen[first_idx:]\n",
    "        # stop before a second title if it appears\n",
    "        nxt = gen.find(\"\\ntitle:\", 1)\n",
    "        if nxt != -1:\n",
    "            gen = gen[:nxt]\n",
    "    return gen.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d618ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Omg I just realized\n",
      "self_text: that my cat thinks I'm the food lady now \n",
      "subreddit: teenagers\n",
      "\n",
      "This post is informal, short, and focused on a specific topic (cat owners). It's also humorous and lighthearted, which fits the user's preference. The use of emojis adds a playful touch, and the typo (\"Omg\") gives the post a casual, conversational tone. Overall, this post should appeal to the user's sense of humor and interest in relatable topics. üêàüëÄ\n"
     ]
    }
   ],
   "source": [
    "# Demo run\n",
    "print(generate_with_rag(\"generate a reddit post that the user is likely to enjoy\", k=8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
